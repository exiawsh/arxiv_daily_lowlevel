[
    {
        "title": "Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis",
        "summary": "Generative models, especially Diffusion Models, have demonstrated remarkable\ncapability in generating high-quality synthetic data, including medical images.\nHowever, traditional class-conditioned generative models often struggle to\ngenerate images that accurately represent specific medical categories, limiting\ntheir usefulness for applications such as skin cancer diagnosis. To address\nthis problem, we propose a classification-induced diffusion model, namely,\nClass-N-Diff, to simultaneously generate and classify dermoscopic images. Our\nClass-N-Diff model integrates a classifier within a diffusion model to guide\nimage generation based on its class conditions. Thus, the model has better\ncontrol over class-conditioned image synthesis, resulting in more realistic and\ndiverse images. Additionally, the classifier demonstrates improved performance,\nhighlighting its effectiveness for downstream diagnostic tasks. This unique\nintegration in our Class-N-Diff makes it a robust tool for enhancing the\nquality and utility of diffusion model-based synthetic dermoscopic image\ngeneration. Our code is available at https://github.com/Munia03/Class-N-Diff.",
        "url": "http://arxiv.org/abs/2510.16887v1",
        "published_date": "2025-10-19T15:37:41+00:00",
        "updated_date": "2025-10-19T15:37:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nusrat Munia",
            "Abdullah Imran"
        ],
        "tldr": "The paper introduces Class-N-Diff, a classification-induced diffusion model for generating realistic and diverse dermoscopic images for skin cancer diagnosis, improving both image quality and classifier performance.",
        "tldr_zh": "该论文介绍了Class-N-Diff，一种分类诱导扩散模型，用于生成逼真且多样化的皮肤镜图像，用于皮肤癌诊断，从而提高图像质量和分类器性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement",
        "summary": "Image restoration is a fundamental and challenging task in computer vision,\nwhere CNN-based frameworks demonstrate significant computational efficiency.\nHowever, previous CNN-based methods often face challenges in adequately\nrestoring fine texture details, which are limited by the small receptive field\nof CNN structures and the lack of channel feature modeling. In this paper, we\npropose WaMaIR, which is a novel framework with a large receptive field for\nimage perception and improves the reconstruction of texture details in restored\nimages. Specifically, we introduce the Global Multiscale Wavelet Transform\nConvolutions (GMWTConvs) for expandding the receptive field to extract image\nfeatures, preserving and enriching texture features in model inputs. Meanwhile,\nwe propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to\ncapture long-range dependencies within feature channels, which enhancing the\nmodel sensitivity to color, edges, and texture information. Additionally, we\npropose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to\nguide the model in preserving detailed texture structures effectively.\nExtensive experiments confirm that WaMaIR outperforms state-of-the-art methods,\nachieving better image restoration and efficient computational performance of\nthe model.",
        "url": "http://arxiv.org/abs/2510.16765v1",
        "published_date": "2025-10-19T09:11:58+00:00",
        "updated_date": "2025-10-19T09:11:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengyu Zhu",
            "Fan",
            "Fuxuan Zhang"
        ],
        "tldr": "The paper proposes WaMaIR, a novel image restoration framework utilizing multiscale wavelet convolutions and a Mamba-based channel-aware module with a texture enhancement loss to improve texture detail reconstruction and computational efficiency.",
        "tldr_zh": "本文提出了一种名为WaMaIR的新型图像恢复框架，该框架利用多尺度小波卷积和基于Mamba的通道感知模块，并结合纹理增强损失，以提高纹理细节重建和计算效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution",
        "summary": "Generative image super-resolution (SR) is rapidly advancing in visual quality\nand detail restoration. As the capacity of SR models expands, however, so does\ntheir tendency to produce artifacts: incorrect, visually disturbing details\nthat reduce perceived quality. Crucially, their perceptual impact varies: some\nartifacts are barely noticeable while others strongly degrade the image. We\nargue that artifacts should be characterized by their prominence to human\nobservers rather than treated as uniform binary defects. Motivated by this, we\npresent a novel dataset of 1302 artifact examples from 11 contemporary image-SR\nmethods, where each artifact is paired with a crowdsourced prominence score.\nBuilding on this dataset, we train a lightweight regressor that produces\nspatial prominence heatmaps and outperforms existing methods at detecting\nprominent artifacts. We release the dataset and code to facilitate\nprominence-aware evaluation and mitigation of SR artifacts.",
        "url": "http://arxiv.org/abs/2510.16752v1",
        "published_date": "2025-10-19T08:28:53+00:00",
        "updated_date": "2025-10-19T08:28:53+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ivan Molodetskikh",
            "Kirill Malyshev",
            "Mark Mirgaleev",
            "Nikita Zagainov",
            "Evgeney Bogatyrev",
            "Dmitriy Vatolin"
        ],
        "tldr": "This paper introduces a new dataset of image super-resolution artifacts with prominence scores and a regressor to detect prominent artifacts, aiming for prominence-aware SR evaluation and mitigation.",
        "tldr_zh": "本文介绍了一个新的图像超分辨率伪影数据集，其中包含显著性分数，以及一个用于检测显著伪影的回归器，旨在实现显著性感知的SR评估和缓解。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling",
        "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.",
        "url": "http://arxiv.org/abs/2510.16751v1",
        "published_date": "2025-10-19T08:28:06+00:00",
        "updated_date": "2025-10-19T08:28:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Erik Riise",
            "Mehmet Onurcan Kaya",
            "Dim P. Papadopoulos"
        ],
        "tldr": "This paper demonstrates that visual autoregressive models, leveraging the benefits of search in discrete token spaces, outperform diffusion models in text-to-image generation, especially in inference time scaling.",
        "tldr_zh": "该论文表明，视觉自回归模型利用离散token空间搜索的优势，在文本到图像生成方面优于扩散模型，尤其是在推理时间缩放方面。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation",
        "summary": "Optical Coherence Tomography (OCT) is a widely used non-invasive imaging\ntechnique that provides detailed three-dimensional views of the retina, which\nare essential for the early and accurate diagnosis of ocular diseases.\nConsequently, OCT image analysis and processing have emerged as key research\nareas in biomedical imaging. However, acquiring paired datasets of clean and\nreal-world noisy OCT images for supervised denoising models remains a\nformidable challenge due to intrinsic speckle noise and practical constraints\nin clinical imaging environments. To address these issues, we propose SDPA++: A\nGeneral Framework for Self-Supervised Denoising with Patch Aggregation. Our\nnovel approach leverages only noisy OCT images by first generating\npseudo-ground-truth images through self-fusion and self-supervised denoising.\nThese refined images then serve as targets to train an ensemble of denoising\nmodels using a patch-based strategy that effectively enhances image clarity.\nPerformance improvements are validated via metrics such as Contrast-to-Noise\nRatio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge\nPreservation (EP) on the real-world dataset from the IEEE SPS Video and Image\nProcessing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT\nimages without clean references, highlighting our method's potential for\nimproving image quality and diagnostic outcomes in clinical practice.",
        "url": "http://arxiv.org/abs/2510.16702v1",
        "published_date": "2025-10-19T04:05:34+00:00",
        "updated_date": "2025-10-19T04:05:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huy Minh Nhat Nguyen",
            "Triet Hoang Minh Dao",
            "Chau Vinh Hoang Truong",
            "Cuong Tuan Nguyen"
        ],
        "tldr": "The paper introduces SDPA++, a self-supervised denoising framework for OCT images that uses patch aggregation and self-fusion to generate pseudo-ground-truth images for training, validated on a real-world dataset.",
        "tldr_zh": "该论文介绍了SDPA++，一个用于OCT图像的自监督去噪框架，它使用patch聚合和自融合来生成伪真值图像以进行训练，并在真实世界的数据集上进行了验证。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display",
        "summary": "Mannequin-based clothing displays offer a cost-effective alternative to\nreal-model showcases for online fashion presentation, but lack realism and\nexpressive detail. To overcome this limitation, we introduce a new task called\nmannequin-to-human (M2H) video generation, which aims to synthesize\nidentity-controllable, photorealistic human videos from footage of mannequins.\nWe propose M2HVideo, a pose-aware and identity-preserving video generation\nframework that addresses two key challenges: the misalignment between head and\nbody motion, and identity drift caused by temporal modeling. In particular,\nM2HVideo incorporates a dynamic pose-aware head encoder that fuses facial\nsemantics with body pose to produce consistent identity embeddings across\nframes. To address the loss of fine facial details due to latent space\ncompression, we introduce a mirror loss applied in pixel space through a\ndenoising diffusion implicit model (DDIM)-based one-step denoising.\nAdditionally, we design a distribution-aware adapter that aligns statistical\ndistributions of identity and clothing features to enhance temporal coherence.\nExtensive experiments on the UBC fashion dataset, our self-constructed ASOS\ndataset, and the newly collected MannequinVideos dataset captured on-site\ndemonstrate that M2HVideo achieves superior performance in terms of clothing\nconsistency, identity preservation, and video fidelity in comparison to\nstate-of-the-art methods.",
        "url": "http://arxiv.org/abs/2510.16833v1",
        "published_date": "2025-10-19T13:42:03+00:00",
        "updated_date": "2025-10-19T13:42:03+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Xiangyu Mu",
            "Dongliang Zhou",
            "Jie Hou",
            "Haijun Zhang",
            "Weili Guan"
        ],
        "tldr": "The paper introduces M2HVideo, a pose-aware and identity-preserving video generation framework for synthesizing realistic human videos from mannequin footage, addressing challenges in head/body motion misalignment and identity drift.",
        "tldr_zh": "该论文介绍了M2HVideo，一个姿态感知和身份保持的视频生成框架，用于从人体模型素材合成逼真的人类视频，解决了头部/身体运动不对齐和身份漂移的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude",
        "summary": "The recovery of Dirac impulses, or spikes, from filtered measurements is a\nclassical problem in signal processing. As the spikes lie in the continuous\ndomain while measurements are discrete, this task is known as super-resolution\nor off-the-grid sparse recovery. Despite significant theoretical and\nalgorithmic advances over the past decade, these developments often overlook\ncritical challenges at the analog-digital interface. In particular, when spikes\nexhibit strong-weak amplitude disparity, conventional digital acquisition may\nresult in clipping of strong components or loss of weak ones beneath the\nquantization noise floor. This motivates a broader perspective:\nsuper-resolution must simultaneously resolve both amplitude and temporal\nstructure. Under a fixed bit budget, such information loss is unavoidable. In\ncontrast, the emerging theory and practice of the Unlimited Sensing Framework\n(USF) demonstrate that these fundamental limitations can be overcome. Building\non this foundation, we demonstrate that modulo encoding within USF enables\ndigital super-resolution by enhancing measurement precision, thereby unlocking\ntemporal super-resolution beyond conventional limits. We develop new\ntheoretical results that extend to non-bandlimited kernels commonly encountered\nin practice and introduce a robust algorithm for off-the-grid sparse recovery.\nTo demonstrate practical impact, we instantiate our framework in the context of\ntime-of-flight imaging. Both numerical simulations and hardware experiments\nvalidate the effectiveness of our approach under low-bit quantization, enabling\nsuper-resolution in amplitude and time.",
        "url": "http://arxiv.org/abs/2510.16948v1",
        "published_date": "2025-10-19T17:57:24+00:00",
        "updated_date": "2025-10-19T17:57:24+00:00",
        "categories": [
            "cs.IT",
            "cs.CV",
            "eess.SP",
            "math.IT"
        ],
        "authors": [
            "Ruiming Guo",
            "Ayush Bhandari"
        ]
    }
]