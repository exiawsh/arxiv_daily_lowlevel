[
    {
        "title": "D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy",
        "summary": "The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \\textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \\textbf{12$\\times$}",
        "url": "http://arxiv.org/abs/2602.08395v1",
        "published_date": "2026-02-09T08:52:51+00:00",
        "updated_date": "2026-02-09T08:52:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianfeng Liang",
            "Shaocheng Shen",
            "Botao Xu",
            "Qiang Hu",
            "Xiaoyun Zhang"
        ],
        "tldr": "D$^2$-VR is a single-image diffusion-based video restoration framework that addresses the limitations of diffusion-based methods, such as high latency and temporal instability, by using degradation-robust flow alignment, adversarial distillation, and synergistic optimization, achieving SOTA performance with a 12x speedup.",
        "tldr_zh": "D$^2$-VR是一个基于单图像扩散的视频修复框架，通过使用抗降解流对齐、对抗蒸馏和协同优化，解决了基于扩散的方法的局限性，如高延迟和时间不稳定，实现了最先进的性能，并加速了12倍。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Improving Reconstruction of Representation Autoencoder",
        "summary": "Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.",
        "url": "http://arxiv.org/abs/2602.08620v1",
        "published_date": "2026-02-09T13:12:35+00:00",
        "updated_date": "2026-02-09T13:12:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyu Liu",
            "Chujie Qin",
            "Hubery Yin",
            "Qixin Yan",
            "Zheng-Peng Duan",
            "Chen Li",
            "Jing Lyu",
            "Chun-Le Guo",
            "Chongyi Li"
        ],
        "tldr": "The paper introduces LV-RAE, a representation autoencoder that enhances latent diffusion models by augmenting semantic features with low-level information for improved reconstruction fidelity and generation quality, addressing a bottleneck in scaling LDMs.",
        "tldr_zh": "该论文介绍了一种名为LV-RAE的表征自编码器，通过增强语义特征的低级信息来改进潜在扩散模型，从而提高重建保真度和生成质量，解决了LDM扩展的一个瓶颈。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "A Unified Framework for Multimodal Image Reconstruction and Synthesis using Denoising Diffusion Models",
        "summary": "Image reconstruction and image synthesis are important for handling incomplete multimodal imaging data, but existing methods require various task-specific models, complicating training and deployment workflows. We introduce Any2all, a unified framework that addresses this limitation by formulating these disparate tasks as a single virtual inpainting problem. We train a single, unconditional diffusion model on the complete multimodal data stack. This model is then adapted at inference time to ``inpaint'' all target modalities from any combination of inputs of available clean images or noisy measurements. We validated Any2all on a PET/MR/CT brain dataset. Our results show that Any2all can achieve excellent performance on both multimodal reconstruction and synthesis tasks, consistently yielding images with competitive distortion-based performance and superior perceptual quality over specialized methods.",
        "url": "http://arxiv.org/abs/2602.08249v1",
        "published_date": "2026-02-09T03:54:24+00:00",
        "updated_date": "2026-02-09T03:54:24+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Weijie Gan",
            "Xucheng Wang",
            "Tongyao Wang",
            "Wenshang Wang",
            "Chunwei Ying",
            "Yuyang Hu",
            "Yasheng Chen",
            "Hongyu An",
            "Ulugbek S. Kamilov"
        ],
        "tldr": "The paper introduces Any2all, a unified diffusion model-based framework for multimodal image reconstruction and synthesis, treating all tasks as virtual inpainting, achieving competitive performance and superior perceptual quality compared to task-specific methods.",
        "tldr_zh": "该论文介绍了Any2all，一个基于扩散模型的统一框架，用于多模态图像重建和合成，将所有任务视为虚拟修复，与特定任务的方法相比，实现了有竞争力的性能和卓越的感知质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework",
        "summary": "Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.",
        "url": "http://arxiv.org/abs/2602.08727v1",
        "published_date": "2026-02-09T14:36:05+00:00",
        "updated_date": "2026-02-09T14:36:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Johannes Thalhammer",
            "Tina Dorosti",
            "Sebastian Peterhansl",
            "Daniela Pfeiffer",
            "Franz Pfeiffer",
            "Florian Schaff"
        ],
        "tldr": "This paper proposes a hybrid 2D-3D CNN framework for artifact reduction in undersampled 3D cone-beam CT volumes, balancing computational efficiency and volumetric consistency.",
        "tldr_zh": "本文提出了一种混合2D-3D CNN框架，用于减少欠采样3D锥束CT体积中的伪影，从而平衡了计算效率和体积一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration",
        "summary": "While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.",
        "url": "http://arxiv.org/abs/2602.08615v1",
        "published_date": "2026-02-09T13:00:16+00:00",
        "updated_date": "2026-02-09T13:00:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kfir Goldberg",
            "Elad Richardson",
            "Yael Vinker"
        ],
        "tldr": "The paper introduces \"Inspiration Seeds,\" a generative framework that creates diverse image compositions from two input images, revealing latent relationships without text prompts, aimed at supporting early-stage visual ideation.",
        "tldr_zh": "该论文介绍了“灵感种子”，一个生成框架，它从两张输入图像创建多样化的图像组合，揭示潜在的关系，无需文本提示，旨在支持早期阶段的视觉构思。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers",
        "summary": "Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.",
        "url": "http://arxiv.org/abs/2602.08388v1",
        "published_date": "2026-02-09T08:39:47+00:00",
        "updated_date": "2026-02-09T08:39:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuo Zhang",
            "Wenzhuo Wu",
            "Huayu Zhang",
            "Jiarong Cheng",
            "Xianghao Zang",
            "Chao Ban",
            "Hao Sun",
            "Zhongjiang He",
            "Tianwei Cao",
            "Kongming Liang",
            "Zhanyu Ma"
        ],
        "tldr": "GeoEdit introduces a diffusion transformer-based framework with effects-sensitive attention for geometrically accurate and realistic image editing, accompanied by a new large-scale dataset, RS-Objects.",
        "tldr_zh": "GeoEdit 提出了一个基于扩散变换器的框架，具有效果敏感的注意力机制，用于实现几何精确和逼真的图像编辑，并伴随一个新的大规模数据集 RS-Objects。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ViT-5: Vision Transformers for The Mid-2020s",
        "summary": "This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.",
        "url": "http://arxiv.org/abs/2602.08071v1",
        "published_date": "2026-02-08T18:03:44+00:00",
        "updated_date": "2026-02-08T18:03:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Wang",
            "Sucheng Ren",
            "Tiezheng Zhang",
            "Predrag Neskovic",
            "Anand Bhattad",
            "Cihang Xie",
            "Alan Yuille"
        ],
        "tldr": "The paper introduces ViT-5, a modernized Vision Transformer architecture incorporating recent advancements in normalization, activations, positional encoding, gating, and learnable tokens. ViT-5 demonstrates improved performance on image classification and generation tasks compared to vanilla ViTs.",
        "tldr_zh": "该论文介绍了ViT-5，一种现代化的视觉Transformer架构，它结合了最新的归一化、激活函数、位置编码、门控机制和可学习标记方面的进展。ViT-5在图像分类和生成任务上表现出比原始ViT更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]