[
    {
        "title": "Test-Time Iterative Error Correction for Efficient Diffusion Models",
        "summary": "With the growing demand for high-quality image generation on\nresource-constrained devices, efficient diffusion models have received\nincreasing attention. However, such models suffer from approximation errors\nintroduced by efficiency techniques, which significantly degrade generation\nquality. Once deployed, these errors are difficult to correct, as modifying the\nmodel is typically infeasible in deployment environments. Through an analysis\nof error propagation across diffusion timesteps, we reveal that these\napproximation errors can accumulate exponentially, severely impairing output\nquality. Motivated by this insight, we propose Iterative Error Correction\n(IEC), a novel test-time method that mitigates inference-time errors by\niteratively refining the model's output. IEC is theoretically proven to reduce\nerror propagation from exponential to linear growth, without requiring any\nretraining or architectural changes. IEC can seamlessly integrate into the\ninference process of existing diffusion models, enabling a flexible trade-off\nbetween performance and efficiency. Extensive experiments show that IEC\nconsistently improves generation quality across various datasets, efficiency\ntechniques, and model architectures, establishing it as a practical and\ngeneralizable solution for test-time enhancement of efficient diffusion models.",
        "url": "http://arxiv.org/abs/2511.06250v1",
        "published_date": "2025-11-09T06:29:22+00:00",
        "updated_date": "2025-11-09T06:29:22+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yunshan Zhong",
            "Yanwei Qi",
            "Yuxin Zhang"
        ],
        "tldr": "This paper introduces Iterative Error Correction (IEC), a test-time method to improve the generation quality of efficient diffusion models by mitigating inference-time errors without retraining or architectural changes.",
        "tldr_zh": "本文提出了一种名为迭代误差校正（IEC）的测试时方法，通过减少推理时误差来提高高效扩散模型的生成质量，而无需重新训练或更改架构。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Physics-Informed Image Restoration via Progressive PDE Integration",
        "summary": "Motion blur, caused by relative movement between camera and scene during\nexposure, significantly degrades image quality and impairs downstream computer\nvision tasks such as object detection, tracking, and recognition in dynamic\nenvironments. While deep learning-based motion deblurring methods have achieved\nremarkable progress, existing approaches face fundamental challenges in\ncapturing the long-range spatial dependencies inherent in motion blur patterns.\nTraditional convolutional methods rely on limited receptive fields and require\nextremely deep networks to model global spatial relationships. These\nlimitations motivate the need for alternative approaches that incorporate\nphysical priors to guide feature evolution during restoration. In this paper,\nwe propose a progressive training framework that integrates physics-informed\nPDE dynamics into state-of-the-art restoration architectures. By leveraging\nadvection-diffusion equations to model feature evolution, our approach\nnaturally captures the directional flow characteristics of motion blur while\nenabling principled global spatial modeling. Our PDE-enhanced deblurring models\nachieve superior restoration quality with minimal overhead, adding only\napproximately 1\\% to inference GMACs while providing consistent improvements in\nperceptual quality across multiple state-of-the-art architectures.\nComprehensive experiments on standard motion deblurring benchmarks demonstrate\nthat our physics-informed approach improves PSNR and SSIM significantly across\nfour diverse architectures, including FFTformer, NAFNet, Restormer, and\nStripformer. These results validate that incorporating mathematical physics\nprinciples through PDE-based global layers can enhance deep learning-based\nimage restoration, establishing a promising direction for physics-informed\nneural network design in computer vision applications.",
        "url": "http://arxiv.org/abs/2511.06244v1",
        "published_date": "2025-11-09T06:10:20+00:00",
        "updated_date": "2025-11-09T06:10:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shamika Likhite",
            "Santiago López-Tapia",
            "Aggelos K. Katsaggelos"
        ],
        "tldr": "This paper introduces a physics-informed deep learning approach for motion deblurring, leveraging PDE-based feature evolution to capture long-range spatial dependencies and improve restoration quality with minimal computational overhead.",
        "tldr_zh": "本文提出了一种物理信息驱动的深度学习方法，用于运动去模糊，通过基于PDE的特征演化来捕捉长程空间依赖关系，并以最小的计算开销提高恢复质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MambaOVSR: Multiscale Fusion with Global Motion Modeling for Chinese Opera Video Super-Resolution",
        "summary": "Chinese opera is celebrated for preserving classical art. However, early\nfilming equipment limitations have degraded videos of last-century performances\nby renowned artists (e.g., low frame rates and resolution), hindering archival\nefforts. Although space-time video super-resolution (STVSR) has advanced\nsignificantly, applying it directly to opera videos remains challenging. The\nscarcity of datasets impedes the recovery of high frequency details, and\nexisting STVSR methods lack global modeling capabilities, compromising visual\nquality when handling opera's characteristic large motions. To address these\nchallenges, we pioneer a large scale Chinese Opera Video Clip (COVC) dataset\nand propose the Mamba-based multiscale fusion network for space-time Opera\nVideo Super-Resolution (MambaOVSR). Specifically, MambaOVSR involves three\nnovel components: the Global Fusion Module (GFM) for motion modeling through a\nmultiscale alternating scanning mechanism, and the Multiscale Synergistic Mamba\nModule (MSMM) for alignment across different sequence lengths. Additionally,\nour MambaVR block resolves feature artifacts and positional information loss\nduring alignment. Experimental results on the COVC dataset show that MambaOVSR\nsignificantly outperforms the SOTA STVSR method by an average of 1.86 dB in\nterms of PSNR. Dataset and Code will be publicly released.",
        "url": "http://arxiv.org/abs/2511.06172v1",
        "published_date": "2025-11-09T00:53:58+00:00",
        "updated_date": "2025-11-09T00:53:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hua Chang",
            "Xin Xu",
            "Wei Liu",
            "Wei Wang",
            "Xin Yuan",
            "Kui Jiang"
        ],
        "tldr": "This paper introduces MambaOVSR, a Mamba-based video super-resolution method tailored for Chinese opera videos, along with a new large-scale dataset (COVC). It addresses challenges related to low-resolution footage and large motions typical in opera performances, achieving superior results compared to state-of-the-art methods.",
        "tldr_zh": "本文介绍了 MambaOVSR，一种基于 Mamba 的视频超分辨率方法，专为中国戏曲视频设计，并发布了一个新的大规模数据集 (COVC)。它解决了戏曲表演中常见的低分辨率素材和大动作相关的挑战，与最先进的方法相比，取得了优异的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Refinement via Flow Matching for Training-free Linear Inverse Problem Solving",
        "summary": "Recent advances in inverse problem solving have increasingly adopted flow\npriors over diffusion models due to their ability to construct straight\nprobability paths from noise to data, thereby enhancing efficiency in both\ntraining and inference. However, current flow-based inverse solvers face two\nprimary limitations: (i) they operate directly in pixel space, which demands\nheavy computational resources for training and restricts scalability to\nhigh-resolution images, and (ii) they employ guidance strategies with\nprior-agnostic posterior covariances, which can weaken alignment with the\ngenerative trajectory and degrade posterior coverage. In this paper, we propose\nLFlow (Latent Refinement via Flows), a training-free framework for solving\nlinear inverse problems via pretrained latent flow priors. LFlow leverages the\nefficiency of flow matching to perform ODE sampling in latent space along an\noptimal path. This latent formulation further allows us to introduce a\ntheoretically grounded posterior covariance, derived from the optimal vector\nfield, enabling effective flow guidance. Experimental results demonstrate that\nour proposed method outperforms state-of-the-art latent diffusion solvers in\nreconstruction quality across most tasks. The code will be publicly available\nat https://github.com/hosseinaskari-cs/LFlow .",
        "url": "http://arxiv.org/abs/2511.06138v1",
        "published_date": "2025-11-08T21:20:59+00:00",
        "updated_date": "2025-11-08T21:20:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hossein Askari",
            "Yadan Luo",
            "Hongfu Sun",
            "Fred Roosta"
        ],
        "tldr": "The paper introduces LFlow, a training-free framework for solving linear inverse problems using pretrained latent flow priors and an optimized posterior covariance, outperforming state-of-the-art latent diffusion solvers in reconstruction quality.",
        "tldr_zh": "该论文介绍了 LFlow，一个无需训练的框架，用于使用预训练的潜在流先验和优化的后验协方差来解决线性逆问题，在重建质量方面优于最先进的潜在扩散求解器。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation",
        "summary": "Centerline graphs, crucial for path planning in autonomous driving, are\ntraditionally learned using deterministic methods. However, these methods often\nlack spatial reasoning and struggle with occluded or invisible centerlines.\nGenerative approaches, despite their potential, remain underexplored in this\ndomain. We introduce LaneDiffusion, a novel generative paradigm for centerline\ngraph learning. LaneDiffusion innovatively employs diffusion models to generate\nlane centerline priors at the Bird's Eye View (BEV) feature level, instead of\ndirectly predicting vectorized centerlines. Our method integrates a Lane Prior\nInjection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively\nconstruct diffusion targets and manage the diffusion process. Furthermore,\nvectorized centerlines and topologies are then decoded from these\nprior-injected BEV features. Extensive evaluations on the nuScenes and\nArgoverse2 datasets demonstrate that LaneDiffusion significantly outperforms\nexisting methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on\nfine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and\n2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and\nTOP_ll). These results establish state-of-the-art performance in centerline\ngraph learning, offering new insights into generative models for this task.",
        "url": "http://arxiv.org/abs/2511.06272v1",
        "published_date": "2025-11-09T08:15:58+00:00",
        "updated_date": "2025-11-09T08:15:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zijie Wang",
            "Weiming Zhang",
            "Wei Zhang",
            "Xiao Tan",
            "Hongxing Liu",
            "Yaowei Wang",
            "Guanbin Li"
        ],
        "tldr": "LaneDiffusion is a novel generative method using diffusion models to generate lane centerline priors at the BEV feature level, achieving state-of-the-art performance on centerline graph learning tasks.",
        "tldr_zh": "LaneDiffusion 是一种新的生成方法，使用扩散模型在鸟瞰视图（BEV）特征级别生成车道中心线先验，在中心线图学习任务上实现了最先进的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]