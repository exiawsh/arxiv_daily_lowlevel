[
    {
        "title": "SAViL-Det: Semantic-Aware Vision-Language Model for Multi-Script Text Detection",
        "summary": "Detecting text in natural scenes remains challenging, particularly for\ndiverse scripts and arbitrarily shaped instances where visual cues alone are\noften insufficient. Existing methods do not fully leverage semantic context.\nThis paper introduces SAViL-Det, a novel semantic-aware vision-language model\nthat enhances multi-script text detection by effectively integrating textual\nprompts with visual features. SAViL-Det utilizes a pre-trained CLIP model\ncombined with an Asymptotic Feature Pyramid Network (AFPN) for multi-scale\nvisual feature fusion. The core of the proposed framework is a novel\nlanguage-vision decoder that adaptively propagates fine-grained semantic\ninformation from text prompts to visual features via cross-modal attention.\nFurthermore, a text-to-pixel contrastive learning mechanism explicitly aligns\ntextual and corresponding visual pixel features. Extensive experiments on\nchallenging benchmarks demonstrate the effectiveness of the proposed approach,\nachieving state-of-the-art performance with F-scores of 84.8% on the benchmark\nmulti-lingual MLT-2019 dataset and 90.2% on the curved-text CTW1500 dataset.",
        "url": "http://arxiv.org/abs/2507.20188v1",
        "published_date": "2025-07-27T09:16:39+00:00",
        "updated_date": "2025-07-27T09:16:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohammed-En-Nadhir Zighem",
            "Abdenour Hadid"
        ],
        "tldr": "SAViL-Det is a new semantic-aware vision-language model for multi-script text detection that uses CLIP, AFPN, and a novel language-vision decoder to integrate textual prompts with visual features, achieving state-of-the-art results on MLT-2019 and CTW1500 datasets.",
        "tldr_zh": "SAViL-Det是一种新型的语义感知视觉-语言模型，用于多脚本文本检测。它使用CLIP，AFPN和一个新的语言-视觉解码器，将文本提示与视觉特征集成，并在MLT-2019和CTW1500数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks",
        "summary": "Real-world applications, such as autonomous driving and humanoid robot\nmanipulation, require precise spatial perception. However, it remains\nunderexplored how Vision-Language Models (VLMs) recognize spatial relationships\nand perceive spatial movement. In this work, we introduce a spatial evaluation\npipeline and construct a corresponding benchmark. Specifically, we categorize\nspatial understanding into two main types: absolute spatial understanding,\nwhich involves querying the absolute spatial position (e.g., left, right) of an\nobject within an image, and 3D spatial understanding, which includes movement\nand rotation. Notably, our dataset is entirely synthetic, enabling the\ngeneration of test samples at a low cost while also preventing dataset\ncontamination. We conduct experiments on multiple state-of-the-art VLMs and\nobserve that there is significant room for improvement in their spatial\nunderstanding abilities. Explicitly, in our experiments, humans achieve\nnear-perfect performance on all tasks, whereas current VLMs attain human-level\nperformance only on the two simplest tasks. For the remaining tasks, the\nperformance of VLMs is distinctly lower than that of humans. In fact, the\nbest-performing Vision-Language Models even achieve near-zero scores on\nmultiple tasks. The dataset and code are available on\nhttps://github.com/kong13661/LRR-Bench.",
        "url": "http://arxiv.org/abs/2507.20174v1",
        "published_date": "2025-07-27T08:31:24+00:00",
        "updated_date": "2025-07-27T08:31:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fei Kong",
            "Jinhao Duan",
            "Kaidi Xu",
            "Zhenhua Guo",
            "Xiaofeng Zhu",
            "Xiaoshuang Shi"
        ],
        "tldr": "The paper introduces LRR-Bench, a synthetic benchmark to evaluate spatial understanding capabilities of Vision-Language Models (VLMs). Experiments show that VLMs struggle with spatial reasoning tasks compared to humans.",
        "tldr_zh": "该论文介绍了 LRR-Bench，一个合成的基准测试，用于评估视觉语言模型（VLM）的空间理解能力。实验表明，与人类相比，VLM 在空间推理任务中表现不佳。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality",
        "summary": "Vision-language models (VLMs) extend the conventional large language models\nby integrating visual data, enabling richer multimodal reasoning and\nsignificantly broadens the practical applications of AI. However, including\nvisual inputs also brings new challenges in maintaining data quality. Empirical\nevidence consistently shows that carefully curated and representative training\nexamples often yield superior results compared to simply increasing the\nquantity of data. Inspired by this observation, we introduce a streamlined data\nfiltration framework that employs a compact VLM, fine-tuned on a high-quality\nimage-caption annotated dataset. This model effectively evaluates and filters\npotential training samples based on caption and image quality and alignment.\nUnlike previous approaches, which typically add auxiliary filtration modules on\ntop of existing full-scale VLMs, our method exclusively utilizes the inherent\nevaluative capability of a purpose-built small VLM. This strategy eliminates\nthe need for extra modules and reduces training overhead. Our lightweight model\nefficiently filters out inaccurate, noisy web data, improving image-text\nalignment and caption linguistic fluency. Experimental results show that\ndatasets underwent high-precision filtration using our compact VLM perform on\npar with, or even surpass, larger and noisier datasets gathered through\nhigh-volume web crawling. Thus, our method provides a lightweight yet robust\nsolution for building high-quality vision-language training corpora. \\\\\n\\textbf{Availability and implementation:} Our compact VLM filtration model,\ntraining data, utility scripts, and Supplementary data (Appendices) are freely\navailable at https://github.com/daulettoibazar/Compact_VLM_Filter.",
        "url": "http://arxiv.org/abs/2507.20156v1",
        "published_date": "2025-07-27T07:20:25+00:00",
        "updated_date": "2025-07-27T07:20:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Daulet Toibazar",
            "Kesen Wang",
            "Sherif Mohamed",
            "Abdulaziz Al-Badawi",
            "Abdulrahman Alfulayt",
            "Pedro J. Moreno"
        ],
        "tldr": "This paper introduces a compact VLM fine-tuned for filtering noisy image-text data, improving data quality and model performance while reducing training overhead compared to using larger VLMs with auxiliary modules.",
        "tldr_zh": "本文介绍了一种紧凑型 VLM，通过微调来过滤噪声图像文本数据，从而提高数据质量和模型性能，同时与使用带有辅助模块的大型 VLM 相比，降低了训练开销。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding",
        "summary": "Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large\nLanguage Models (MLLMs) have significantly advanced 3D scene perception towards\nlanguage-driven cognition. However, existing 3D language models struggle with\nsparse, large-scale point clouds due to slow feature extraction and limited\nrepresentation accuracy. To address these challenges, we propose NeuroVoxel-LM,\na novel framework that integrates Neural Radiance Fields (NeRF) with dynamic\nresolution voxelization and lightweight meta-embedding. Specifically, we\nintroduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that\nadaptively adjusts voxel granularity based on geometric and structural\ncomplexity, reducing computational cost while preserving reconstruction\nfidelity. In addition, we propose the Token-level Adaptive Pooling for\nLightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic\nrepresentation through attention-based weighting and residual fusion.\nExperimental results demonstrate that DR-MSV significantly improves point cloud\nfeature extraction efficiency and accuracy, while TAP-LME outperforms\nconventional max-pooling in capturing fine-grained semantics from NeRF weights.",
        "url": "http://arxiv.org/abs/2507.20110v1",
        "published_date": "2025-07-27T03:11:08+00:00",
        "updated_date": "2025-07-27T03:11:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "I.4; I.5"
        ],
        "authors": [
            "Shiyu Liu",
            "Lianlei Shan"
        ],
        "tldr": "NeuroVoxel-LM enhances 3D scene understanding in VLMs by using dynamic voxelization and a lightweight meta-embedding technique, improving efficiency and accuracy when processing large, sparse point clouds.",
        "tldr_zh": "NeuroVoxel-LM通过动态体素化和轻量级元嵌入技术增强了VLM中的3D场景理解，提高了处理大型稀疏点云的效率和准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "The Devil is in the EOS: Sequence Training for Detailed Image Captioning",
        "summary": "Despite significant advances in vision-language models (VLMs), image\ncaptioning often suffers from a lack of detail, with base models producing\nshort, generic captions. This limitation persists even though VLMs are equipped\nwith strong vision and language backbones. While supervised data and complex\nreward functions have been proposed to improve detailed image captioning, we\nidentify a simpler underlying issue: a bias towards the end-of-sequence (EOS)\ntoken, which is introduced during cross-entropy training. We propose an\nunsupervised method to debias the model's tendency to predict the EOS token\nprematurely. By reducing this bias, we encourage the generation of longer, more\ndetailed captions without the need for intricate reward functions or\nsupervision. Our approach is straightforward, effective, and easily applicable\nto any pretrained model. We demonstrate its effectiveness through experiments\nwith three VLMs and on three detailed captioning benchmarks. Our results show a\nsubstantial increase in caption length and relevant details, albeit with an\nexpected increase in the rate of hallucinations.",
        "url": "http://arxiv.org/abs/2507.20077v1",
        "published_date": "2025-07-26T23:00:43+00:00",
        "updated_date": "2025-07-26T23:00:43+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Abdelrahman Mohamed",
            "Yova Kementchedjhieva"
        ],
        "tldr": "This paper addresses the issue of premature EOS token prediction in image captioning VLMs, proposing an unsupervised debiasing method to generate more detailed captions. The method is simple, effective, and applicable to pretrained models.",
        "tldr_zh": "本文针对图像描述VLM中过早预测EOS token的问题，提出了一种无监督的去偏方法，以生成更详细的描述。该方法简单有效，适用于预训练模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TAPS : Frustratingly Simple Test Time Active Learning for VLMs",
        "summary": "Test-Time Optimization enables models to adapt to new data during inference\nby updating parameters on-the-fly. Recent advances in Vision-Language Models\n(VLMs) have explored learning prompts at test time to improve performance in\ndownstream tasks. In this work, we extend this idea by addressing a more\ngeneral and practical challenge: Can we effectively utilize an oracle in a\ncontinuous data stream where only one sample is available at a time, requiring\nan immediate query decision while respecting latency and memory constraints? To\ntackle this, we propose a novel Test-Time Active Learning (TTAL) framework that\nadaptively queries uncertain samples and updates prompts dynamically. Unlike\nprior methods that assume batched data or multiple gradient updates, our\napproach operates in a real-time streaming scenario with a single test sample\nper step. We introduce a dynamically adjusted entropy threshold for active\nquerying, a class-balanced replacement strategy for memory efficiency, and a\nclass-aware distribution alignment technique to enhance adaptation. The design\nchoices are justified using careful theoretical analysis. Extensive experiments\nacross 10 cross-dataset transfer benchmarks and 4 domain generalization\ndatasets demonstrate consistent improvements over state-of-the-art methods\nwhile maintaining reasonable latency and memory overhead. Our framework\nprovides a practical and effective solution for real-world deployment in\nsafety-critical applications such as autonomous systems and medical\ndiagnostics.",
        "url": "http://arxiv.org/abs/2507.20028v1",
        "published_date": "2025-07-26T18:04:49+00:00",
        "updated_date": "2025-07-26T18:04:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dhruv Sarkar",
            "Aprameyo Chakrabartty",
            "Bibhudatta Bhanja"
        ],
        "tldr": "The paper proposes a Test-Time Active Learning (TTAL) framework for VLMs that adaptively queries uncertain samples and updates prompts in a real-time streaming scenario, demonstrating improved performance on cross-dataset transfer and domain generalization benchmarks.",
        "tldr_zh": "该论文提出了一种用于视觉语言模型的测试时主动学习（TTAL）框架，该框架可在实时流场景中自适应地查询不确定样本并更新提示，并在跨数据集迁移和领域泛化基准测试中表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Detecting Visual Information Manipulation Attacks in Augmented Reality: A Multimodal Semantic Reasoning Approach",
        "summary": "The virtual content in augmented reality (AR) can introduce misleading or\nharmful information, leading to semantic misunderstandings or user errors. In\nthis work, we focus on visual information manipulation (VIM) attacks in AR\nwhere virtual content changes the meaning of real-world scenes in subtle but\nimpactful ways. We introduce a taxonomy that categorizes these attacks into\nthree formats: character, phrase, and pattern manipulation, and three purposes:\ninformation replacement, information obfuscation, and extra wrong information.\nBased on the taxonomy, we construct a dataset, AR-VIM. It consists of 452\nraw-AR video pairs spanning 202 different scenes, each simulating a real-world\nAR scenario. To detect such attacks, we propose a multimodal semantic reasoning\nframework, VIM-Sense. It combines the language and visual understanding\ncapabilities of vision-language models (VLMs) with optical character\nrecognition (OCR)-based textual analysis. VIM-Sense achieves an attack\ndetection accuracy of 88.94% on AR-VIM, consistently outperforming vision-only\nand text-only baselines. The system reaches an average attack detection latency\nof 7.07 seconds in a simulated video processing framework and 7.17 seconds in a\nreal-world evaluation conducted on a mobile Android AR application.",
        "url": "http://arxiv.org/abs/2507.20356v2",
        "published_date": "2025-07-27T17:04:50+00:00",
        "updated_date": "2025-07-31T03:23:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanming Xiu",
            "Maria Gorlatova"
        ],
        "tldr": "This paper introduces a taxonomy and dataset (AR-VIM) for visual information manipulation (VIM) attacks in AR and proposes a multimodal semantic reasoning framework (VIM-Sense) for detecting these attacks, achieving 88.94% accuracy.",
        "tldr_zh": "该论文提出了增强现实(AR)中视觉信息操纵(VIM)攻击的分类法和数据集(AR-VIM)，并提出了一种用于检测这些攻击的多模态语义推理框架(VIM-Sense)，实现了88.94%的准确率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Motion-example-controlled Co-speech Gesture Generation Leveraging Large Language Models",
        "summary": "The automatic generation of controllable co-speech gestures has recently\ngained growing attention. While existing systems typically achieve gesture\ncontrol through predefined categorical labels or implicit pseudo-labels derived\nfrom motion examples, these approaches often compromise the rich details\npresent in the original motion examples. We present MECo, a framework for\nmotion-example-controlled co-speech gesture generation by leveraging large\nlanguage models (LLMs). Our method capitalizes on LLMs' comprehension\ncapabilities through fine-tuning to simultaneously interpret speech audio and\nmotion examples, enabling the synthesis of gestures that preserve\nexample-specific characteristics while maintaining speech congruence. Departing\nfrom conventional pseudo-labeling paradigms, we position motion examples as\nexplicit query contexts within the prompt structure to guide gesture\ngeneration. Experimental results demonstrate state-of-the-art performance\nacross three metrics: Fr\\'echet Gesture Distance (FGD), motion diversity, and\nexample-gesture similarity. Furthermore, our framework enables granular control\nof individual body parts and accommodates diverse input modalities including\nmotion clips, static poses, human video sequences, and textual descriptions.\nOur code, pre-trained models, and videos are available at\nhttps://robinwitch.github.io/MECo-Page.",
        "url": "http://arxiv.org/abs/2507.20220v1",
        "published_date": "2025-07-27T10:59:29+00:00",
        "updated_date": "2025-07-27T10:59:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bohong Chen",
            "Yumeng Li",
            "Youyi Zheng",
            "Yao-Xiang Ding",
            "Kun Zhou"
        ],
        "tldr": "The paper introduces MECo, a framework that uses LLMs to generate co-speech gestures controlled by motion examples, achieving state-of-the-art performance by incorporating motion examples directly into the prompt and allowing granular control over body parts and various input modalities.",
        "tldr_zh": "该论文介绍了一种名为MECo的框架，它利用大型语言模型生成由运动示例控制的协同语音手势，通过将运动示例直接纳入提示中，并允许对身体部位和各种输入模式进行精细控制，从而实现最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Player-Centric Multimodal Prompt Generation for Large Language Model Based Identity-Aware Basketball Video Captioning",
        "summary": "Existing sports video captioning methods often focus on the action yet\noverlook player identities, limiting their applicability. Although some methods\nintegrate extra information to generate identity-aware descriptions, the player\nidentities are sometimes incorrect because the extra information is independent\nof the video content. This paper proposes a player-centric multimodal prompt\ngeneration network for identity-aware sports video captioning (LLM-IAVC), which\nfocuses on recognizing player identities from a visual perspective.\nSpecifically, an identity-related information extraction module (IRIEM) is\ndesigned to extract player-related multimodal embeddings. IRIEM includes a\nplayer identification network (PIN) for extracting visual features and player\nnames, and a bidirectional semantic interaction module (BSIM) to link player\nfeatures with video content for mutual enhancement. Additionally, a visual\ncontext learning module (VCLM) is designed to capture the key video context\ninformation. Finally, by integrating the outputs of the above modules as the\nmultimodal prompt for the large language model (LLM), it facilitates the\ngeneration of descriptions with player identities. To support this work, we\nconstruct a new benchmark called NBA-Identity, a large identity-aware\nbasketball video captioning dataset with 9,726 videos covering 9 major event\ntypes. The experimental results on NBA-Identity and VC-NBA-2022 demonstrate\nthat our proposed model achieves advanced performance. Code and dataset are\npublicly available at https://github.com/Zeyu1226-mt/LLM-IAVC.",
        "url": "http://arxiv.org/abs/2507.20163v1",
        "published_date": "2025-07-27T07:30:56+00:00",
        "updated_date": "2025-07-27T07:30:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Xi",
            "Haoying Sun",
            "Yaofei Wu",
            "Junchi Yan",
            "Haoran Zhang",
            "Lifang Wu",
            "Liang Wang",
            "Changwen Chen"
        ],
        "tldr": "The paper introduces a player-centric multimodal prompt generation network (LLM-IAVC) for identity-aware basketball video captioning, utilizing a novel dataset (NBA-Identity) and achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了一种以球员为中心的多模态提示生成网络 (LLM-IAVC)，用于生成具有身份识别的篮球视频字幕。它使用了一个新的数据集 (NBA-Identity)，并实现了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "A Multi-Agent System Enables Versatile Information Extraction from the Chemical Literature",
        "summary": "To fully expedite AI-powered chemical research, high-quality chemical\ndatabases are the cornerstone. Automatic extraction of chemical information\nfrom the literature is essential for constructing reaction databases, but it is\ncurrently limited by the multimodality and style variability of chemical\ninformation. In this work, we developed a multimodal large language model\n(MLLM)-based multi-agent system for robust and automated chemical information\nextraction. It utilizes the MLLM's strong reasoning capability to understand\nthe structure of diverse chemical graphics, decompose the extraction task into\nsub-tasks, and coordinate a set of specialized agents, each combining the\ncapabilities of the MLLM with the precise, domain-specific strengths of\ndedicated tools, to solve them accurately and integrate the results into a\nunified output. Our system achieved an F1 score of 80.8% on a benchmark dataset\nof sophisticated multimodal chemical reaction graphics from the literature,\nsurpassing the previous state-of-the-art model (F1 score of 35.6%) by a\nsignificant margin. Additionally, it demonstrated consistent improvements in\nkey sub-tasks, including molecular image recognition, reaction image parsing,\nnamed entity recognition and text-based reaction extraction. This work is a\ncritical step toward automated chemical information extraction into structured\ndatasets, which will be a strong promoter of AI-driven chemical research.",
        "url": "http://arxiv.org/abs/2507.20230v2",
        "published_date": "2025-07-27T11:16:57+00:00",
        "updated_date": "2025-07-29T02:55:37+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Yufan Chen",
            "Ching Ting Leung",
            "Bowen Yu",
            "Jianwei Sun",
            "Yong Huang",
            "Linyan Li",
            "Hao Chen",
            "Hanyu Gao"
        ],
        "tldr": "This paper introduces a multi-agent system using MLLMs for chemical information extraction from literature, significantly improving the F1 score on a benchmark dataset compared to previous state-of-the-art methods.",
        "tldr_zh": "该论文介绍了一种基于多模态大型语言模型（MLLM）的多智能体系统，用于从化学文献中提取信息。与之前的最先进方法相比，该系统在一个基准数据集上显著提高了F1分数。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]