[
    {
        "title": "Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking",
        "summary": "While autoregressive (AR) models have demonstrated remarkable success in\nimage generation, extending them to layout-conditioned generation remains\nchallenging due to the sparse nature of layout conditions and the risk of\nfeature entanglement. We present Structured Masking for AR-based\nLayout-to-Image (SMARLI), a novel framework for layoutto-image generation that\neffectively integrates spatial layout constraints into AR-based image\ngeneration. To equip AR model with layout control, a specially designed\nstructured masking strategy is applied to attention computation to govern the\ninteraction among the global prompt, layout, and image tokens. This design\nprevents mis-association between different regions and their descriptions while\nenabling sufficient injection of layout constraints into the generation\nprocess. To further enhance generation quality and layout accuracy, we\nincorporate Group Relative Policy Optimization (GRPO) based post-training\nscheme with specially designed layout reward functions for next-set-based AR\nmodels. Experimental results demonstrate that SMARLI is able to seamlessly\nintegrate layout tokens with text and image tokens without compromising\ngeneration quality. It achieves superior layoutaware control while maintaining\nthe structural simplicity and generation efficiency of AR models.",
        "url": "http://arxiv.org/abs/2509.12046v1",
        "published_date": "2025-09-15T15:27:29+00:00",
        "updated_date": "2025-09-15T15:27:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zirui Zheng",
            "Takashi Isobe",
            "Tong Shen",
            "Xu Jia",
            "Jianbin Zhao",
            "Xiaomin Li",
            "Mengmeng Ge",
            "Baolu Li",
            "Qinghe Wang",
            "Dong Li",
            "Dong Zhou",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "Emad Barsoum"
        ],
        "tldr": "The paper introduces SMARLI, a novel autoregressive framework for layout-conditioned text-to-image generation using structured masking and group relative policy optimization, achieving superior layout control and generation efficiency.",
        "tldr_zh": "该论文介绍了一种新的自回归框架SMARLI，用于布局条件下的文本到图像生成，它使用结构化掩码和组相对策略优化，实现了卓越的布局控制和生成效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization",
        "summary": "Conventional deep neural nets (DNNs) initialize network parameters at random\nand then optimize each one via stochastic gradient descent (SGD), resulting in\nsubstantial risk of poor-performing local minima.Focusing on the image\ninterpolation problem and leveraging a recent theorem that maps a\n(pseudo-)linear interpolator {\\Theta} to a directed graph filter that is a\nsolution to a MAP problem regularized with a graph shift variation (GSV) prior,\nwe first initialize a directed graph adjacency matrix A based on a known\ninterpolator {\\Theta}, establishing a baseline performance.Then, towards\nfurther gain, we learn perturbation matrices P and P(2) from data to augment A,\nwhose restoration effects are implemented via Douglas-Rachford (DR) iterations,\nwhich we unroll into a lightweight interpretable neural net.Experimental\nresults demonstrate state-of-the-art image interpolation results, while\ndrastically reducing network parameters.",
        "url": "http://arxiv.org/abs/2509.11926v1",
        "published_date": "2025-09-15T13:43:55+00:00",
        "updated_date": "2025-09-15T13:43:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xue Zhang",
            "Bingshuo Hu",
            "Gene Cheung"
        ],
        "tldr": "This paper introduces a novel image interpolation method based on graph algorithm unrolling with Douglas-Rachford iterations and guaranteed initialization, achieving state-of-the-art results with fewer parameters.",
        "tldr_zh": "本文提出了一种新颖的图像插值方法，该方法基于图算法展开和Douglas-Rachford迭代，并保证初始化，以更少的参数实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration",
        "summary": "Existing all-in-one image restoration approaches, which aim to handle\nmultiple weather degradations within a single framework, are predominantly\ntrained and evaluated using mixed single-weather synthetic datasets. However,\nthese datasets often differ significantly in resolution, style, and domain\ncharacteristics, leading to substantial domain gaps that hinder the development\nand fair evaluation of unified models. Furthermore, the lack of a large-scale,\nreal-world all-in-one weather restoration dataset remains a critical bottleneck\nin advancing this field. To address these limitations, we present a real-world\nall-in-one adverse weather image restoration benchmark dataset, which contains\nimage pairs captured under various weather conditions, including rain, snow,\nand haze, as well as diverse outdoor scenes and illumination settings. The\nresulting dataset provides precisely aligned degraded and clean images,\nenabling supervised learning and rigorous evaluation. We conduct comprehensive\nexperiments by benchmarking a variety of task-specific, task-general, and\nall-in-one restoration methods on our dataset. Our dataset offers a valuable\nfoundation for advancing robust and practical all-in-one image restoration in\nreal-world scenarios. The dataset has been publicly released and is available\nat https://github.com/guanqiyuan/WeatherBench.",
        "url": "http://arxiv.org/abs/2509.11642v1",
        "published_date": "2025-09-15T07:24:29+00:00",
        "updated_date": "2025-09-15T07:24:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiyuan Guan",
            "Qianfeng Yang",
            "Xiang Chen",
            "Tianyu Song",
            "Guiyue Jin",
            "Jiyu Jin"
        ],
        "tldr": "The paper introduces WeatherBench, a new real-world dataset for all-in-one adverse weather image restoration, addressing the limitations of existing synthetic datasets and enabling more robust training and evaluation of restoration models.",
        "tldr_zh": "该论文介绍了WeatherBench，这是一个新的真实世界数据集，用于一体化恶劣天气图像恢复，解决了现有合成数据集的局限性，并支持对恢复模型进行更稳健的训练和评估。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed",
        "summary": "Diffusion models have shown promising results in free-form inpainting. Recent\nstudies based on refined diffusion samplers or novel architectural designs led\nto realistic results and high data consistency. However, random initialization\nseed (noise) adopted in vanilla diffusion process may introduce mismatched\nsemantic information in masked regions, leading to biased inpainting results,\ne.g., low consistency and low coherence with the other unmasked area. To\naddress this issue, we propose the Initial Seed refined Diffusion Model\n(IS-Diff), a completely training-free approach incorporating distributional\nharmonious seeds to produce harmonious results. Specifically, IS-Diff employs\ninitial seeds sampled from unmasked areas to imitate the masked data\ndistribution, thereby setting a promising direction for the diffusion\nprocedure. Moreover, a dynamic selective refinement mechanism is proposed to\ndetect severe unharmonious inpaintings in intermediate latent and adjust the\nstrength of our initialization prior dynamically. We validate our method on\nboth standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet,\nand Places2 datasets, demonstrating its effectiveness across all metrics\ncompared to state-of-the-art inpainting methods.",
        "url": "http://arxiv.org/abs/2509.11638v1",
        "published_date": "2025-09-15T07:16:03+00:00",
        "updated_date": "2025-09-15T07:16:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongzhe Lyu",
            "Yu Wu",
            "Yutian Lin",
            "Bo Du"
        ],
        "tldr": "The paper introduces IS-Diff, a training-free approach for diffusion-based image inpainting that improves consistency and coherence by using initial seeds sampled from unmasked areas and a dynamic refinement mechanism.",
        "tldr_zh": "该论文介绍了一种名为IS-Diff的免训练方法，用于基于扩散的图像修复，通过使用从非掩蔽区域采样的初始种子和动态细化机制，提高了图像的一致性和连贯性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition",
        "summary": "Intelligent tableware cleaning is a critical application in food safety and\nsmart homes, but existing methods are limited by coarse-grained classification\nand scarcity of few-shot data, making it difficult to meet industrialization\nrequirements. We propose DTGen, a few-shot data augmentation scheme based on\ngenerative diffusion models, specifically designed for fine-grained dirty\ntableware recognition. DTGen achieves efficient domain specialization through\nLoRA, generates diverse dirty images via structured prompts, and ensures data\nquality through CLIP-based cross-modal filtering. Under extremely limited real\nfew-shot conditions, DTGen can synthesize virtually unlimited high-quality\nsamples, significantly improving classifier performance and supporting\nfine-grained dirty tableware recognition. We further elaborate on lightweight\ndeployment strategies, promising to transfer DTGen's benefits to embedded\ndishwashers and integrate with cleaning programs to intelligently regulate\nenergy consumption and detergent usage. Research results demonstrate that DTGen\nnot only validates the value of generative AI in few-shot industrial vision but\nalso provides a feasible deployment path for automated tableware cleaning and\nfood safety monitoring.",
        "url": "http://arxiv.org/abs/2509.11661v1",
        "published_date": "2025-09-15T07:59:34+00:00",
        "updated_date": "2025-09-15T07:59:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lifei Hao",
            "Yue Cheng",
            "Baoqi Huang",
            "Bing Jia",
            "Xuandong Zhao"
        ],
        "tldr": "The paper introduces DTGen, a generative diffusion model-based data augmentation method for fine-grained dirty tableware recognition in few-shot settings, showing significant improvements in classifier performance and potential for deployment in smart dishwashers.",
        "tldr_zh": "该论文介绍了DTGen，一种基于生成扩散模型的数据增强方法，用于小样本环境下的细粒度脏餐具识别，显著提高了分类器性能，并有望部署在智能洗碗机中。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]