[
    {
        "title": "Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching",
        "summary": "Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.",
        "url": "http://arxiv.org/abs/2602.05951v1",
        "published_date": "2026-02-05T18:08:20+00:00",
        "updated_date": "2026-02-05T18:08:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Junwan Kim",
            "Jiho Park",
            "Seonghu Jeon",
            "Seungryong Kim"
        ],
        "tldr": "This paper introduces a method to learn a condition-dependent source distribution for flow matching in text-to-image generation, demonstrating improved convergence and performance compared to using a standard Gaussian source distribution. It addresses instability issues and analyzes target representation spaces.",
        "tldr_zh": "该论文提出了一种在文本到图像生成中学习条件依赖源分布的流匹配方法，与使用标准高斯源分布相比，该方法展示了更好的收敛性和性能。它解决了不稳定性问题，并分析了目标表示空间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
        "summary": "Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.",
        "url": "http://arxiv.org/abs/2602.05966v1",
        "published_date": "2026-02-05T18:21:02+00:00",
        "updated_date": "2026-02-05T18:21:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mirlan Karimov",
            "Teodora Spasojevic",
            "Markus Braun",
            "Julian Wiederer",
            "Vasileios Belagiannis",
            "Marc Pollefeys"
        ],
        "tldr": "The paper introduces Localized Semantic Alignment (LSA), a fine-tuning method to enhance temporal consistency in traffic video generation by aligning semantic features between real and generated videos, improving performance without requiring control signals at inference time.",
        "tldr_zh": "该论文介绍了局部语义对齐（LSA），一种微调方法，通过对齐真实视频和生成视频之间的语义特征来增强交通视频生成中的时间一致性，提高了性能，且在推理时不需要控制信号。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]