[
    {
        "title": "SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution",
        "summary": "Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims\nto enhance low-resolution (LR) contrasts leveraging high-resolution (HR)\nreferences, shortening acquisition time and improving imaging efficiency while\npreserving anatomical details. The main challenge lies in maintaining\nspatial-semantic consistency, ensuring anatomical structures remain\nwell-aligned and coherent despite structural discrepancies and motion between\nthe target and reference images. Conventional methods insufficiently model\nspatial-semantic consistency and underuse frequency-domain information, which\nleads to poor fine-grained alignment and inadequate recovery of high-frequency\ndetails. In this paper, we propose the Spatial-Semantic Consistent Model\n(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast\nspatial alignment, a Semantic-Aware Token Aggregation Block for long-range\nsemantic consistency, and a Spatial-Frequency Fusion Block for fine structure\nrestoration. Experiments on public and private datasets show that SSCM achieves\nstate-of-the-art performance with fewer parameters while ensuring spatially and\nsemantically consistent reconstructions.",
        "url": "http://arxiv.org/abs/2509.18593v1",
        "published_date": "2025-09-23T03:24:32+00:00",
        "updated_date": "2025-09-23T03:24:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoman Wu",
            "Lubin Gan",
            "Siying Wu",
            "Jing Zhang",
            "Yunwei Ou",
            "Xiaoyan Sun"
        ],
        "tldr": "The paper proposes SSCM, a novel model for multi-contrast MRI super-resolution that ensures spatial-semantic consistency using spatial warping, semantic aggregation, and spatial-frequency fusion, achieving state-of-the-art performance with fewer parameters.",
        "tldr_zh": "该论文提出了SSCM，一种用于多对比度MRI超分辨率的新模型，它使用空间扭曲、语义聚合和空间频率融合来确保空间-语义一致性，以更少的参数实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation",
        "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)\ncapable of image understanding and generation tasks. Unlike existing multimodal\ndiffsion language models such as MMaDa and Muddit which only support simple\nimage-level understanding tasks and low-resolution image generation, Lavida-O\nexhibits many new capabilities such as object grounding, image-editing, and\nhigh-resolution (1024px) image synthesis. It is also the first unified MDM that\nuses its understanding capabilities to improve image generation and editing\nresults through planning and iterative self-reflection. To allow effective and\nefficient training and sampling, Lavida-O ntroduces many novel techniques such\nas Elastic Mixture-of-Transformer architecture, universal text conditioning,\nand stratified sampling. \\ours~achieves state-of-the-art performance on a wide\nrange of benchmarks such as RefCOCO object grounding, GenEval text-to-image\ngeneration, and ImgEdit image editing, outperforming existing autoregressive\nand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while\noffering considerable speedup at inference.",
        "url": "http://arxiv.org/abs/2509.19244v1",
        "published_date": "2025-09-23T17:05:46+00:00",
        "updated_date": "2025-09-23T17:05:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shufan Li",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Zijun Wei",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "tldr": "Lavida-O is a unified masked diffusion model (MDM) that achieves state-of-the-art performance in image understanding and generation tasks, including object grounding, image editing, and high-resolution image synthesis, through planning and iterative self-reflection.",
        "tldr_zh": "Lavida-O 是一种统一的掩码扩散模型 (MDM)，通过规划和迭代自反思，在图像理解和生成任务（包括对象定位、图像编辑和高分辨率图像合成）方面实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models",
        "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.",
        "url": "http://arxiv.org/abs/2509.18917v1",
        "published_date": "2025-09-23T12:35:07+00:00",
        "updated_date": "2025-09-23T12:35:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Amirhesam Aghanouri",
            "Cristina Olaverri-Monreal"
        ],
        "tldr": "This paper introduces a DDPM-based method with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic LiDAR point cloud data for autonomous vehicle perception, demonstrating superior performance compared to SOTA methods in mitigating noise and sparsity.",
        "tldr_zh": "本文提出了一种基于DDPM的方法，采用新颖的噪声调度和时间步嵌入技术，生成用于自动驾驶车辆感知的高质量合成激光雷达点云数据，并在减轻噪声和稀疏性方面表现出优于现有最佳方法的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring",
        "summary": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS.",
        "url": "http://arxiv.org/abs/2509.18898v1",
        "published_date": "2025-09-23T11:21:54+00:00",
        "updated_date": "2025-09-23T11:21:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pengteng Li",
            "Yunfan Lu",
            "Pinhao Song",
            "Weiyu Guo",
            "Huizai Yao",
            "F. Richard Yu",
            "Hui Xiong"
        ],
        "tldr": "DeblurSplat is a novel SfM-free 3D Gaussian Splatting method using event cameras for robust deblurring, leveraging a dense stereo module for initial point clouds and event streams for fine-grained supervision.",
        "tldr_zh": "DeblurSplat 是一种新颖的无SfM的3D高斯溅射方法，它使用事件相机进行稳健的去模糊处理，利用密集立体模块获取初始点云，并利用事件流进行细粒度监督。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters",
        "summary": "Recent advances in diffusion models have significantly improved image and\nvideo synthesis. In addition, several concept control methods have been\nproposed to enable fine-grained, continuous, and flexible control over\nfree-form text prompts. However, these methods not only require intensive\ntraining time and GPU memory usage to learn the sliders or embeddings but also\nneed to be retrained for different diffusion backbones, limiting their\nscalability and adaptability. To address these limitations, we introduce Text\nSlider, a lightweight, efficient and plug-and-play framework that identifies\nlow-rank directions within a pre-trained text encoder, enabling continuous\ncontrol of visual concepts while significantly reducing training time, GPU\nmemory consumption, and the number of trainable parameters. Furthermore, Text\nSlider supports multi-concept composition and continuous control, enabling\nfine-grained and flexible manipulation in both image and video synthesis. We\nshow that Text Slider enables smooth and continuous modulation of specific\nattributes while preserving the original spatial layout and structure of the\ninput. Text Slider achieves significantly better efficiency: 5$\\times$ faster\ntraining than Concept Slider and 47$\\times$ faster than Attribute Control,\nwhile reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$,\nrespectively.",
        "url": "http://arxiv.org/abs/2509.18831v1",
        "published_date": "2025-09-23T09:17:18+00:00",
        "updated_date": "2025-09-23T09:17:18+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Pin-Yen Chiu",
            "I-Sheng Fang",
            "Jun-Cheng Chen"
        ],
        "tldr": "The paper introduces Text Slider, a lightweight and efficient framework for continuous concept control in image/video synthesis using LoRA adapters, achieving significant improvements in training time and memory usage compared to existing methods.",
        "tldr_zh": "该论文介绍了Text Slider，一个轻量级且高效的框架，使用LoRA适配器实现图像/视频合成中的连续概念控制，与现有方法相比，在训练时间和内存使用方面实现了显著改进。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prompt-Guided Dual Latent Steering for Inversion Problems",
        "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.",
        "url": "http://arxiv.org/abs/2509.18619v1",
        "published_date": "2025-09-23T04:11:06+00:00",
        "updated_date": "2025-09-23T04:11:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Wu",
            "Xu Liu",
            "Chenxuan Zhao",
            "Xinyu Wu"
        ],
        "tldr": "The paper introduces Prompt-Guided Dual Latent Steering (PDLS), a training-free framework using Rectified Flow models to improve image inversion by separating structural and semantic guidance paths, achieving better fidelity and semantic accuracy in image restoration tasks.",
        "tldr_zh": "该论文介绍了Prompt-Guided Dual Latent Steering (PDLS)，一个利用Rectified Flow模型来改进图像反演的免训练框架，通过分离结构和语义引导路径，在图像恢复任务中实现了更好的保真度和语义准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation",
        "summary": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based\ntraining-free framework that enables controllable fusion of multiple reference\nstyles in diffusion models. Most of the existing reference-based methods are\nlimited by (a) acceptance of only one style image, thus prohibiting hybrid\naesthetics and scalability to more styles, and (b) lack of a principled\nmechanism to balance several stylistic influences. AMSF mitigates these\nchallenges by encoding all style images and textual hints with a semantic token\ndecomposition module that is adaptively injected into every cross-attention\nlayer of an frozen diffusion model. A similarity-aware re-weighting module then\nrecalibrates, at each denoising step, the attention allocated to every style\ncomponent, yielding balanced and user-controllable blends without any\nfine-tuning or external adapters. Both qualitative and quantitative evaluations\nshow that AMSF produces multi-style fusion results that consistently outperform\nthe state-of-the-art approaches, while its fusion design scales seamlessly to\ntwo or more styles. These capabilities position AMSF as a practical step toward\nexpressive multi-style generation in diffusion models.",
        "url": "http://arxiv.org/abs/2509.18602v1",
        "published_date": "2025-09-23T03:47:59+00:00",
        "updated_date": "2025-09-23T03:47:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Liu",
            "Yibo Lu",
            "Xinxian Wang",
            "Xinyu Wu"
        ],
        "tldr": "The paper introduces Adaptive Multi-Style Fusion (AMSF), a training-free framework for controllable multi-style fusion in diffusion models using semantic token decomposition and similarity-aware re-weighting, outperforming existing methods.",
        "tldr_zh": "该论文介绍了一种自适应多风格融合（AMSF）方法，该方法是一个无需训练的框架，通过语义令牌分解和相似性感知重加权，在扩散模型中实现可控的多风格融合，并且性能优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps",
        "summary": "Despite steady progress in layout-to-image generation, current methods still\nstruggle with layouts containing significant overlap between bounding boxes. We\nidentify two primary challenges: (1) large overlapping regions and (2)\noverlapping instances with minimal semantic distinction. Through both\nqualitative examples and quantitative analysis, we demonstrate how these\nfactors degrade generation quality. To systematically assess this issue, we\nintroduce OverLayScore, a novel metric that quantifies the complexity of\noverlapping bounding boxes. Our analysis reveals that existing benchmarks are\nbiased toward simpler cases with low OverLayScore values, limiting their\neffectiveness in evaluating model performance under more challenging\nconditions. To bridge this gap, we present OverLayBench, a new benchmark\nfeaturing high-quality annotations and a balanced distribution across different\nlevels of OverLayScore. As an initial step toward improving performance on\ncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a\ncurated amodal mask dataset. Together, our contributions lay the groundwork for\nmore robust layout-to-image generation under realistic and challenging\nscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.",
        "url": "http://arxiv.org/abs/2509.19282v1",
        "published_date": "2025-09-23T17:50:00+00:00",
        "updated_date": "2025-09-23T17:50:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bingnan Li",
            "Chen-Yu Wang",
            "Haiyang Xu",
            "Xiang Zhang",
            "Ethan Armand",
            "Divyansh Srivastava",
            "Xiaojun Shan",
            "Zeyuan Chen",
            "Jianwen Xie",
            "Zhuowen Tu"
        ],
        "tldr": "The paper introduces OverLayBench, a new benchmark for layout-to-image generation that specifically addresses the challenges posed by dense overlaps between bounding boxes, along with a new metric and a baseline model to improve performance on this challenging task.",
        "tldr_zh": "本文介绍了一个新的布局到图像生成基准测试 OverLayBench，专门解决边界框之间密集重叠带来的挑战，并提出了一种新的度量标准和一个基线模型，以提高在此挑战性任务上的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Application Aligned Synthetic Surgical Image Synthesis",
        "summary": "The scarcity of annotated surgical data poses a significant challenge for\ndeveloping deep learning systems in computer-assisted interventions. While\ndiffusion models can synthesize realistic images, they often suffer from data\nmemorization, resulting in inconsistent or non-diverse samples that may fail to\nimprove, or even harm, downstream performance. We introduce \\emph{Surgical\nApplication-Aligned Diffusion} (SAADi), a new framework that aligns diffusion\nmodels with samples preferred by downstream models. Our method constructs pairs\nof \\emph{preferred} and \\emph{non-preferred} synthetic images and employs\nlightweight fine-tuning of diffusion models to align the image generation\nprocess with downstream objectives explicitly. Experiments on three surgical\ndatasets demonstrate consistent gains of $7$--$9\\%$ in classification and\n$2$--$10\\%$ in segmentation tasks, with the considerable improvements observed\nfor underrepresented classes. Iterative refinement of synthetic samples further\nboosts performance by $4$--$10\\%$. Unlike baseline approaches, our method\novercomes sample degradation and establishes task-aware alignment as a key\nprinciple for mitigating data scarcity and advancing surgical vision\napplications.",
        "url": "http://arxiv.org/abs/2509.18796v1",
        "published_date": "2025-09-23T08:40:40+00:00",
        "updated_date": "2025-09-23T08:40:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Danush Kumar Venkatesh",
            "Stefanie Speidel"
        ],
        "tldr": "The paper introduces Surgical Application-Aligned Diffusion (SAADi), a framework that fine-tunes diffusion models using preferred and non-preferred synthetic image pairs to improve downstream performance in surgical image classification and segmentation tasks, particularly for underrepresented classes.",
        "tldr_zh": "该论文介绍了一种名为Surgical Application-Aligned Diffusion (SAADi)的框架，该框架通过使用首选和非首选合成图像对来微调扩散模型，从而提高手术图像分类和分割任务的下游性能，尤其是在代表性不足的类别中。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Reconstruction of Optical Coherence Tomography Images from Wavelength-space Using Deep-learning",
        "summary": "Conventional Fourier-domain Optical Coherence Tomography (FD-OCT) systems\ndepend on resampling into wavenumber (k) domain to extract the depth profile.\nThis either necessitates additional hardware resources or amplifies the\nexisting computational complexity. Moreover, the OCT images also suffer from\nspeckle noise, due to systemic reliance on low coherence interferometry. We\npropose a streamlined and computationally efficient approach based on\nDeep-Learning (DL) which enables reconstructing speckle-reduced OCT images\ndirectly from the wavelength domain. For reconstruction, two encoder-decoder\nstyled networks namely Spatial Domain Convolution Neural Network (SD-CNN) and\nFourier Domain CNN (FD-CNN) are used sequentially. The SD-CNN exploits the\nhighly degraded images obtained by Fourier transforming the domain fringes to\nreconstruct the deteriorated morphological structures along with suppression of\nunwanted noise. The FD-CNN leverages this output to enhance the image quality\nfurther by optimization in Fourier domain (FD). We quantitatively and visually\ndemonstrate the efficacy of the method in obtaining high-quality OCT images.\nFurthermore, we illustrate the computational complexity reduction by harnessing\nthe power of DL models. We believe that this work lays the framework for\nfurther innovations in the realm of OCT image reconstruction.",
        "url": "http://arxiv.org/abs/2509.18783v1",
        "published_date": "2025-09-23T08:21:53+00:00",
        "updated_date": "2025-09-23T08:21:53+00:00",
        "categories": [
            "physics.optics",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Maryam Viqar",
            "Erdem Sahin",
            "Elena Stoykova",
            "Violeta Madjarova"
        ],
        "tldr": "This paper proposes a deep learning-based approach to reconstruct Optical Coherence Tomography (OCT) images directly from the wavelength domain, reducing speckle noise and computational complexity compared to traditional methods.",
        "tldr_zh": "本文提出了一种基于深度学习的方法，可以直接从波长域重建光学相干断层扫描 (OCT) 图像，与传统方法相比，可减少散斑噪声和计算复杂度。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation",
        "summary": "Recent works have made notable advancements in enhancing unified models for\ntext-to-image generation through the Chain-of-Thought (CoT). However, these\nreasoning methods separate the processes of understanding and generation, which\nlimits their ability to guide the reasoning of unified models in addressing the\ndeficiencies of their generative capabilities. To this end, we propose a novel\nreasoning framework for unified models, Understanding-in-Generation (UiG),\nwhich harnesses the robust understanding capabilities of unified models to\nreinforce their performance in image generation. The core insight of our UiG is\nto integrate generative guidance by the strong understanding capabilities\nduring the reasoning process, thereby mitigating the limitations of generative\nabilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse\nunderstanding into the generation process. Initially, we verify the generated\nimage and incorporate the understanding of unified models into the editing\ninstructions. Subsequently, we enhance the generated image step by step,\ngradually infusing the understanding into the generation process. Our UiG\nframework demonstrates a significant performance improvement in text-to-image\ngeneration over existing text-to-image reasoning methods, e.g., a 3.92% gain on\nthe long prompt setting of the TIIF benchmark. The project code:\nhttps://github.com/QC-LY/UiG",
        "url": "http://arxiv.org/abs/2509.18639v1",
        "published_date": "2025-09-23T04:52:39+00:00",
        "updated_date": "2025-09-23T04:52:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanhuiyi Lyu",
            "Chi Kit Wong",
            "Chenfei Liao",
            "Lutao Jiang",
            "Xu Zheng",
            "Zexin Lu",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "tldr": "The paper introduces Understanding-in-Generation (UiG), a novel framework that integrates understanding into the text-to-image generation process of unified models using image editing as a bridge, achieving improved performance, especially with long prompts.",
        "tldr_zh": "该论文介绍了一种名为理解内生成 (UiG) 的新框架，该框架通过使用图像编辑作为桥梁，将理解融入统一模型的文本到图像生成过程中，从而提高性能，尤其是在处理长提示时。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MoiréNet: A Compact Dual-Domain Network for Image Demoiréing",
        "summary": "Moir\\'e patterns arise from spectral aliasing between display pixel lattices\nand camera sensor grids, manifesting as anisotropic, multi-scale artifacts that\npose significant challenges for digital image demoir\\'eing. We propose\nMoir\\'eNet, a convolutional neural U-Net-based framework that synergistically\nintegrates frequency and spatial domain features for effective artifact\nremoval. Moir\\'eNet introduces two key components: a Directional\nFrequency-Spatial Encoder (DFSE) that discerns moir\\'e orientation via\ndirectional difference convolution, and a Frequency-Spatial Adaptive Selector\n(FSAS) that enables precise, feature-adaptive suppression. Extensive\nexperiments demonstrate that Moir\\'eNet achieves state-of-the-art performance\non public and actively used datasets while being highly parameter-efficient.\nWith only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,\nMoir\\'eNet combines superior restoration quality with parameter efficiency,\nmaking it well-suited for resource-constrained applications including\nsmartphone photography, industrial imaging, and augmented reality.",
        "url": "http://arxiv.org/abs/2509.18910v1",
        "published_date": "2025-09-23T12:33:23+00:00",
        "updated_date": "2025-09-23T12:33:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuwei Guo",
            "Simin Luan",
            "Yan Ke",
            "Zeyd Boukhers",
            "John See",
            "Cong Yang"
        ]
    }
]