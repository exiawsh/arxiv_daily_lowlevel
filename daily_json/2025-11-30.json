[
    {
        "title": "Visual Generation Tuning",
        "summary": "Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.",
        "url": "http://arxiv.org/abs/2511.23469v1",
        "published_date": "2025-11-28T18:57:13+00:00",
        "updated_date": "2025-11-28T18:57:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Guo",
            "Sinan Du",
            "Jingfeng Yao",
            "Wenyu Liu",
            "Bo Li",
            "Haoxiang Cao",
            "Kun Gai",
            "Chun Yuan",
            "Kai Wu",
            "Xinggang Wang"
        ],
        "tldr": "The paper introduces Visual Generation Tuning (VGT), a method to enable visual generation capabilities in pre-trained Vision Language Models (VLMs) by efficiently aligning semantic encoders with pixel decoders, achieving state-of-the-art results in image reconstruction and generation.",
        "tldr_zh": "该论文介绍了视觉生成调整 (VGT)，这是一种通过有效对齐语义编码器和像素解码器，使预训练的视觉语言模型 (VLM) 具备视觉生成能力的方法，在图像重建和生成方面取得了最先进的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]