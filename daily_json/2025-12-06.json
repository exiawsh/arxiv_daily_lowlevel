[
    {
        "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
        "summary": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
        "url": "http://arxiv.org/abs/2512.05112v1",
        "published_date": "2025-12-04T18:59:53+00:00",
        "updated_date": "2025-12-04T18:59:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Dongzhi Jiang",
            "Renrui Zhang",
            "Haodong Li",
            "Zhuofan Zong",
            "Ziyu Guo",
            "Jun He",
            "Claire Guo",
            "Junyan Ye",
            "Rongyao Fang",
            "Weijia Li",
            "Rui Liu",
            "Hongsheng Li"
        ],
        "tldr": "The paper introduces DraCo, a novel chain-of-thought (CoT) method for text-to-image generation that uses a low-resolution draft image as visual guidance for iterative refinement, addressing issues of coarse textual planning and rare concept generation. It significantly outperforms existing CoT methods.",
        "tldr_zh": "该论文介绍了DraCo，一种新颖的用于文本到图像生成的思维链（CoT）方法，它使用低分辨率的草图图像作为视觉指导进行迭代细化，解决了粗糙的文本规划和稀有概念生成的问题。它明显优于现有的CoT方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation",
        "summary": "Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion φ-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. φ-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, φ-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, φ-PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \\href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.",
        "url": "http://arxiv.org/abs/2512.05106v1",
        "published_date": "2025-12-04T18:59:18+00:00",
        "updated_date": "2025-12-04T18:59:18+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Yu Zeng",
            "Charles Ochoa",
            "Mingyuan Zhou",
            "Vishal M. Patel",
            "Vitor Guizilini",
            "Rowan McAllister"
        ],
        "tldr": "The paper introduces Phase-Preserving Diffusion (φ-PD), a novel diffusion model that preserves input phase information, enabling structure-aligned image generation for tasks requiring geometric consistency without architectural changes or extra parameters, and demonstrates its effectiveness in re-rendering and sim-to-real enhancement.",
        "tldr_zh": "该论文介绍了一种新的相位保持扩散（φ-PD）模型，该模型保留了输入相位信息，从而实现了结构对齐的图像生成，适用于需要几何一致性的任务，无需架构更改或额外参数，并展示了其在重渲染和模拟到真实增强方面的有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation",
        "summary": "All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.",
        "url": "http://arxiv.org/abs/2512.05104v1",
        "published_date": "2025-12-04T18:59:10+00:00",
        "updated_date": "2025-12-04T18:59:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaqi Ma",
            "Shengkai Hu",
            "Jun Wan",
            "Jiaxing Huang",
            "Lefei Zhang",
            "Salman Khan"
        ],
        "tldr": "The paper introduces EvoIR, an All-in-One Image Restoration framework that uses evolutionary frequency modulation to dynamically adapt to diverse degradations, outperforming existing methods on multiple benchmarks.",
        "tldr_zh": "该论文介绍了EvoIR，一个全能图像恢复框架，它使用进化频率调制来动态适应各种退化，并在多个基准测试中优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]