[
    {
        "title": "InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior",
        "summary": "Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacher's strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean/noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.",
        "url": "http://arxiv.org/abs/2511.14208v1",
        "published_date": "2025-11-18T07:40:38+00:00",
        "updated_date": "2025-11-18T07:40:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weimin Bai",
            "Suzhe Xu",
            "Yiwei Ren",
            "Jinhua Hao",
            "Ming Sun",
            "Wenzheng Chen",
            "He Sun"
        ],
        "tldr": "InstantViR distills a video diffusion model into a fast, single-pass autoregressive network for real-time video restoration, achieving state-of-the-art quality with significant speedups.",
        "tldr_zh": "InstantViR将视频扩散模型提炼成一个快速的单程自回归网络，用于实时视频修复，在显著提速的同时实现了最先进的质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "A Generative Data Framework with Authentic Supervision for Underwater Image Restoration and Enhancement",
        "summary": "Underwater image restoration and enhancement are crucial for correcting color distortion and restoring image details, thereby establishing a fundamental basis for subsequent underwater visual tasks. However, current deep learning methodologies in this area are frequently constrained by the scarcity of high-quality paired datasets. Since it is difficult to obtain pristine reference labels in underwater scenes, existing benchmarks often rely on manually selected results from enhancement algorithms, providing debatable reference images that lack globally consistent color and authentic supervision. This limits the model's capabilities in color restoration, image enhancement, and generalization. To overcome this limitation, we propose using in-air natural images as unambiguous reference targets and translating them into underwater-degraded versions, thereby constructing synthetic datasets that provide authentic supervision signals for model learning. Specifically, we establish a generative data framework based on unpaired image-to-image translation, producing a large-scale dataset that covers 6 representative underwater degradation types. The framework constructs synthetic datasets with precise ground-truth labels, which facilitate the learning of an accurate mapping from degraded underwater images to their pristine scene appearances. Extensive quantitative and qualitative experiments across 6 representative network architectures and 3 independent test sets show that models trained on our synthetic data achieve comparable or superior color restoration and generalization performance to those trained on existing benchmarks. This research provides a reliable and scalable data-driven solution for underwater image restoration and enhancement. The generated dataset is publicly available at: https://github.com/yftian2025/SynUIEDatasets.git.",
        "url": "http://arxiv.org/abs/2511.14521v1",
        "published_date": "2025-11-18T14:20:17+00:00",
        "updated_date": "2025-11-18T14:20:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yufeng Tian",
            "Yifan Chen",
            "Zhe Sun",
            "Libang Chen",
            "Mingyu Dou",
            "Jijun Lu",
            "Ye Zheng",
            "Xuelong Li"
        ],
        "tldr": "This paper introduces a generative data framework for creating synthetic underwater image datasets with authentic supervision, addressing the lack of high-quality paired data for training underwater image restoration and enhancement models. Models trained on the generated dataset demonstrate comparable or superior performance to those trained on existing benchmarks.",
        "tldr_zh": "本文介绍了一种生成式数据框架，用于创建具有真实监督的合成水下图像数据集，解决了缺乏高质量配对数据来训练水下图像恢复和增强模型的问题。在生成的数据集上训练的模型表现出与在现有基准上训练的模型相当或更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Segmentation-Aware Latent Diffusion for Satellite Image Super-Resolution: Enabling Smallholder Farm Boundary Delineation",
        "summary": "Delineating farm boundaries through segmentation of satellite images is a fundamental step in many agricultural applications. The task is particularly challenging for smallholder farms, where accurate delineation requires the use of high resolution (HR) imagery which are available only at low revisit frequencies (e.g., annually). To support more frequent (sub-) seasonal monitoring, HR images could be combined as references (ref) with low resolution (LR) images -- having higher revisit frequency (e.g., weekly) -- using reference-based super-resolution (Ref-SR) methods. However, current Ref-SR methods optimize perceptual quality and smooth over crucial features needed for downstream tasks, and are unable to meet the large scale-factor requirements for this task. Further, previous two-step approaches of SR followed by segmentation do not effectively utilize diverse satellite sources as inputs. We address these problems through a new approach, $\\textbf{SEED-SR}$, which uses a combination of conditional latent diffusion models and large-scale multi-spectral, multi-source geo-spatial foundation models. Our key innovation is to bypass the explicit SR task in the pixel space and instead perform SR in a segmentation-aware latent space. This unique approach enables us to generate segmentation maps at an unprecedented 20$\\times$ scale factor, and rigorous experiments on two large, real datasets demonstrate up to $\\textbf{25.5}$ and $\\textbf{12.9}$ relative improvement in instance and semantic segmentation metrics respectively over approaches based on state-of-the-art Ref-SR methods.",
        "url": "http://arxiv.org/abs/2511.14481v1",
        "published_date": "2025-11-18T13:21:58+00:00",
        "updated_date": "2025-11-18T13:21:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aditi Agarwal",
            "Anjali Jain",
            "Nikita Saxena",
            "Ishan Deshpande",
            "Michal Kazmierski",
            "Abigail Annkah",
            "Nadav Sherman",
            "Karthikeyan Shanmugam",
            "Alok Talekar",
            "Vaibhav Rajan"
        ],
        "tldr": "The paper introduces SEED-SR, a segmentation-aware latent diffusion model for satellite image super-resolution, enabling high-resolution farm boundary delineation from low-resolution images, achieving significant improvements in segmentation metrics at a 20x scale factor.",
        "tldr_zh": "该论文介绍了一种名为SEED-SR的分割感知潜在扩散模型，用于卫星图像超分辨率，能够从低分辨率图像中实现高分辨率的农场边界划分，并在20倍的尺度因子下，在分割指标上取得了显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Measurement-Constrained Sampling for Text-Prompted Blind Face Restoration",
        "summary": "Blind face restoration (BFR) may correspond to multiple plausible high-quality (HQ) reconstructions under extremely low-quality (LQ) inputs. However, existing methods typically produce deterministic results, struggling to capture this one-to-many nature. In this paper, we propose a Measurement-Constrained Sampling (MCS) approach that enables diverse LQ face reconstructions conditioned on different textual prompts. Specifically, we formulate BFR as a measurement-constrained generative task by constructing an inverse problem through controlled degradations of coarse restorations, which allows posterior-guided sampling within text-to-image diffusion. Measurement constraints include both Forward Measurement, which ensures results align with input structures, and Reverse Measurement, which produces projection spaces, ensuring that the solution can align with various prompts. Experiments show that our MCS can generate prompt-aligned results and outperforms existing BFR methods. Codes will be released after acceptance.",
        "url": "http://arxiv.org/abs/2511.14213v1",
        "published_date": "2025-11-18T07:44:54+00:00",
        "updated_date": "2025-11-18T07:44:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjie Li",
            "Yulun Zhang",
            "Guangwei Gao",
            "Heng Guo",
            "Zhanyu Ma"
        ],
        "tldr": "The paper introduces Measurement-Constrained Sampling (MCS) for text-prompted blind face restoration, enabling diverse high-quality reconstructions from low-quality inputs by integrating forward and reverse measurement constraints within a text-to-image diffusion framework.",
        "tldr_zh": "该论文介绍了测量约束采样(MCS)，用于文本提示的盲人脸恢复，通过在文本到图像扩散框架中集成正向和反向测量约束，从而能够从低质量输入中生成多样的高质量重建。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GloTok: Global Perspective Tokenizer for Image Reconstruction and Generation",
        "summary": "Existing state-of-the-art image tokenization methods leverage diverse semantic features from pre-trained vision models for additional supervision, to expand the distribution of latent representations and thereby improve the quality of image reconstruction and generation. These methods employ a locally supervised approach for semantic supervision, which limits the uniformity of semantic distribution. However, VA-VAE proves that a more uniform feature distribution yields better generation performance. In this work, we introduce a Global Perspective Tokenizer (GloTok), which utilizes global relational information to model a more uniform semantic distribution of tokenized features. Specifically, a codebook-wise histogram relation learning method is proposed to transfer the semantics, which are modeled by pre-trained models on the entire dataset, to the semantic codebook. Then, we design a residual learning module that recovers the fine-grained details to minimize the reconstruction error caused by quantization. Through the above design, GloTok delivers more uniformly distributed semantic latent representations, which facilitates the training of autoregressive (AR) models for generating high-quality images without requiring direct access to pre-trained models during the training process. Experiments on the standard ImageNet-1k benchmark clearly show that our proposed method achieves state-of-the-art reconstruction performance and generation quality.",
        "url": "http://arxiv.org/abs/2511.14184v1",
        "published_date": "2025-11-18T06:40:26+00:00",
        "updated_date": "2025-11-18T06:40:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuan Zhao",
            "Zhongyu Zhang",
            "Yuge Huang",
            "Yuxi Mi",
            "Guodong Mu",
            "Shouhong Ding",
            "Jun Wang",
            "Rizen Guo",
            "Shuigeng Zhou"
        ],
        "tldr": "The paper introduces GloTok, a novel image tokenizer that utilizes global relational information to create more uniform semantic latent representations, achieving state-of-the-art image reconstruction and generation performance without needing direct access to pre-trained models during training.",
        "tldr_zh": "该论文介绍了一种名为GloTok的新型图像标记器，它利用全局关系信息来创建更均匀的语义潜在表示，从而实现最先进的图像重建和生成性能，且在训练过程中无需直接访问预训练模型。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UniSER: A Foundation Model for Unified Soft Effects Removal",
        "summary": "Digital images are often degraded by soft effects such as lens flare, haze, shadows, and reflections, which reduce aesthetics even though the underlying pixels remain partially visible. The prevailing works address these degradations in isolation, developing highly specialized, specialist models that lack scalability and fail to exploit the shared underlying essences of these restoration problems. While specialist models are limited, recent large-scale pretrained generalist models offer powerful, text-driven image editing capabilities. while recent general-purpose systems (e.g., GPT-4o, Flux Kontext, Nano Banana) require detailed prompts and often fail to achieve robust removal on these fine-grained tasks or preserve identity of the scene. Leveraging the common essence of soft effects, i.e., semi-transparent occlusions, we introduce a foundational versatile model UniSER, capable of addressing diverse degradations caused by soft effects within a single framework. Our methodology centers on curating a massive 3.8M-pair dataset to ensure robustness and generalization, which includes novel, physically-plausible data to fill critical gaps in public benchmarks, and a tailored training pipeline that fine-tunes a Diffusion Transformer to learn robust restoration priors from this diverse data, integrating fine-grained mask and strength controls. This synergistic approach allows UniSER to significantly outperform both specialist and generalist models, achieving robust, high-fidelity restoration in the wild.",
        "url": "http://arxiv.org/abs/2511.14183v1",
        "published_date": "2025-11-18T06:39:39+00:00",
        "updated_date": "2025-11-18T06:39:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingdong Zhang",
            "Lingzhi Zhang",
            "Qing Liu",
            "Mang Tik Chiu",
            "Connelly Barnes",
            "Yizhou Wang",
            "Haoran You",
            "Xiaoyang Liu",
            "Yuqian Zhou",
            "Zhe Lin",
            "Eli Shechtman",
            "Sohrab Amirghodsi",
            "Xin Li",
            "Wenping Wang",
            "Xiaohang Zhan"
        ],
        "tldr": "The paper introduces UniSER, a foundation model for unified removal of soft effects (lens flare, haze, etc.) in images, trained on a large dataset with a diffusion transformer architecture, outperforming specialized and generalist models.",
        "tldr_zh": "本文介绍UniSER，一个用于统一去除图像中柔和效果（如镜头光晕、雾霾等）的基础模型，它在一个大型数据集上使用扩散transformer架构进行训练，性能优于专业模型和通用模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration",
        "summary": "All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.",
        "url": "http://arxiv.org/abs/2511.14099v1",
        "published_date": "2025-11-18T03:33:10+00:00",
        "updated_date": "2025-11-18T03:33:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingren Liu",
            "Shuning Xu",
            "Qirui Yang",
            "Yun Wang",
            "Xiangyu Chen",
            "Zhong Ji"
        ],
        "tldr": "The paper introduces FAPE-IR, a frequency-aware planning and execution framework for all-in-one image restoration, using a frozen MLLM planner and a LoRA-MoE executor, achieving SOTA performance and strong zero-shot generalization.",
        "tldr_zh": "该论文介绍了FAPE-IR，一个用于一体化图像恢复的频率感知规划和执行框架，采用冻结的MLLM规划器和LoRA-MoE执行器，实现了SOTA性能和强大的零样本泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Flood-LDM: Generalizable Latent Diffusion Models for rapid and accurate zero-shot High-Resolution Flood Mapping",
        "summary": "Flood prediction is critical for emergency planning and response to mitigate human and economic losses. Traditional physics-based hydrodynamic models generate high-resolution flood maps using numerical methods requiring fine-grid discretization; which are computationally intensive and impractical for real-time large-scale applications. While recent studies have applied convolutional neural networks for flood map super-resolution with good accuracy and speed, they suffer from limited generalizability to unseen areas. In this paper, we propose a novel approach that leverages latent diffusion models to perform super-resolution on coarse-grid flood maps, with the objective of achieving the accuracy of fine-grid flood maps while significantly reducing inference time. Experimental results demonstrate that latent diffusion models substantially decrease the computational time required to produce high-fidelity flood maps without compromising on accuracy, enabling their use in real-time flood risk management. Moreover, diffusion models exhibit superior generalizability across different physical locations, with transfer learning further accelerating adaptation to new geographic regions. Our approach also incorporates physics-informed inputs, addressing the common limitation of black-box behavior in machine learning, thereby enhancing interpretability. Code is available at https://github.com/neosunhan/flood-diff.",
        "url": "http://arxiv.org/abs/2511.14033v1",
        "published_date": "2025-11-18T01:24:38+00:00",
        "updated_date": "2025-11-18T01:24:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sun Han Neo",
            "Sachith Seneviratne",
            "Herath Mudiyanselage Viraj Vidura Herath",
            "Abhishek Saha",
            "Sanka Rasnayaka",
            "Lucy Amanda Marshall"
        ],
        "tldr": "This paper introduces Flood-LDM, a latent diffusion model for generating high-resolution flood maps from coarse-grid data, achieving accuracy comparable to fine-grid simulations with significantly reduced computational time and improved generalizability.",
        "tldr_zh": "本文介绍 Flood-LDM，一种用于从粗网格数据生成高分辨率洪水图的潜在扩散模型，其精度与细网格模拟相当，同时显著减少了计算时间并提高了泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CD-DPE: Dual-Prompt Expert Network based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution",
        "summary": "Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.",
        "url": "http://arxiv.org/abs/2511.14014v1",
        "published_date": "2025-11-18T00:42:41+00:00",
        "updated_date": "2025-11-18T00:42:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xianming Gu",
            "Lihui Wang",
            "Ying Cao",
            "Zeyu Deng",
            "Yingfeng Ou",
            "Guodong Hu",
            "Yi Chen"
        ],
        "tldr": "The paper introduces a novel dual-prompt expert network (CD-DPE) with convolutional dictionary feature decoupling for multi-contrast MRI super-resolution, demonstrating superior performance and generalization on public datasets.",
        "tldr_zh": "该论文提出了一种新的双提示专家网络（CD-DPE），采用卷积字典特征解耦方法，用于多对比度MRI超分辨率重建，并在公共数据集上展示了卓越的性能和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Back to Basics: Let Denoising Generative Models Denoise",
        "summary": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.",
        "url": "http://arxiv.org/abs/2511.13720v1",
        "published_date": "2025-11-17T18:59:57+00:00",
        "updated_date": "2025-11-17T18:59:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianhong Li",
            "Kaiming He"
        ],
        "tldr": "This paper proposes a denoising diffusion model that directly predicts clean images using large-patch Transformers, demonstrating competitive performance on ImageNet without tokenizers or pre-training.",
        "tldr_zh": "本文提出了一种去噪扩散模型，该模型直接使用大块Transformer预测干净图像，并在ImageNet上展示了具有竞争力的性能，无需分词器或预训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images",
        "summary": "Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks. However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development. While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details. In this paper, we introduce D-PerceptCT, a novel architecture inspired by key principles of the Human Visual System (HVS) to enhance LDCT images. The objective is to guide the model to enhance or preserve perceptually relevant features, thereby providing radiologists with CT images where critical anatomical structures and fine pathological details are perceptu- ally visible. D-PerceptCT consists of two main blocks: 1) a Visual Dual-path Extractor (ViDex), which integrates semantic priors from a pretrained DINOv2 model with local spatial features, allowing the network to incorporate semantic-awareness during enhancement; (2) a Global-Local State-Space block that captures long-range information and multiscale features to preserve the important structures and fine details for diagnosis. In addition, we propose a novel deep perceptual loss, designated as the Deep Perceptual Relevancy Loss Function (DPRLF), which is inspired by human contrast sensitivity, to further emphasize perceptually important features. Extensive experiments on the Mayo2016 dataset demonstrate the effectiveness of D-PerceptCT method for LDCT enhancement, showing better preservation of structural and textural information within LDCT images compared to SOTA methods.",
        "url": "http://arxiv.org/abs/2511.14518v1",
        "published_date": "2025-11-18T14:14:59+00:00",
        "updated_date": "2025-11-18T14:14:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taifour Yousra Nabila",
            "Azeddine Beghdadi",
            "Marie Luong",
            "Zuheng Ming",
            "Habib Zaidi",
            "Faouzi Alaya Cheikh"
        ],
        "tldr": "The paper introduces D-PerceptCT, a novel deep learning architecture for enhancing low-dose CT images, incorporating human visual system principles and a new perceptual loss function to improve image quality while preserving critical details.",
        "tldr_zh": "本文介绍了一种名为D-PerceptCT的新型深度学习架构，用于增强低剂量CT图像。该架构结合了人类视觉系统原理和新的感知损失函数，旨在提高图像质量，同时保留关键细节。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Gaussian Splatting-based Low-Rank Tensor Representation for Multi-Dimensional Image Recovery",
        "summary": "Tensor singular value decomposition (t-SVD) is a promising tool for multi-dimensional image representation, which decomposes a multi-dimensional image into a latent tensor and an accompanying transform matrix. However, two critical limitations of t-SVD methods persist: (1) the approximation of the latent tensor (e.g., tensor factorizations) is coarse and fails to accurately capture spatial local high-frequency information; (2) The transform matrix is composed of fixed basis atoms (e.g., complex exponential atoms in DFT and cosine atoms in DCT) and cannot precisely capture local high-frequency information along the mode-3 fibers. To address these two limitations, we propose a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework, which compactly and continuously represents multi-dimensional images. Specifically, we leverage tailored 2D Gaussian splatting and 1D Gaussian splatting to generate the latent tensor and transform matrix, respectively. The 2D and 1D Gaussian splatting are indispensable and complementary under this representation framework, which enjoys a powerful representation capability, especially for local high-frequency information. To evaluate the representation ability of the proposed GSLR, we develop an unsupervised GSLR-based multi-dimensional image recovery model. Extensive experiments on multi-dimensional image recovery demonstrate that GSLR consistently outperforms state-of-the-art methods, particularly in capturing local high-frequency information.",
        "url": "http://arxiv.org/abs/2511.14270v1",
        "published_date": "2025-11-18T09:04:09+00:00",
        "updated_date": "2025-11-18T09:04:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Zeng",
            "Xi-Le Zhao",
            "Wei-Hao Wu",
            "Teng-Yu Ji",
            "Chao Wang"
        ],
        "tldr": "This paper introduces a Gaussian Splatting-based Low-rank tensor Representation (GSLR) framework for multi-dimensional image recovery, addressing limitations of t-SVD in capturing local high-frequency information using tailored 2D and 1D Gaussian splatting. Experiments demonstrate its superior performance compared to state-of-the-art methods.",
        "tldr_zh": "该论文提出了一种基于高斯溅射的低秩张量表示（GSLR）框架，用于多维图像恢复，利用定制的2D和1D高斯溅射解决t-SVD在捕获局部高频信息方面的局限性。实验表明，该方法优于现有技术。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Semantic Context Matters: Improving Conditioning for Autoregressive Models",
        "summary": "Recently, autoregressive (AR) models have shown strong potential in image generation, offering better scalability and easier integration with unified multi-modal systems compared to diffusion-based methods. However, extending AR models to general image editing remains challenging due to weak and inefficient conditioning, often leading to poor instruction adherence and visual artifacts. To address this, we propose SCAR, a Semantic-Context-driven method for Autoregressive models. SCAR introduces two key components: Compressed Semantic Prefilling, which encodes high-level semantics into a compact and efficient prefix, and Semantic Alignment Guidance, which aligns the last visual hidden states with target semantics during autoregressive decoding to enhance instruction fidelity. Unlike decoding-stage injection methods, SCAR builds upon the flexibility and generality of vector-quantized-based prefilling while overcoming its semantic limitations and high cost. It generalizes across both next-token and next-set AR paradigms with minimal architectural changes. SCAR achieves superior visual fidelity and semantic alignment on both instruction editing and controllable generation benchmarks, outperforming prior AR-based methods while maintaining controllability. All code will be released.",
        "url": "http://arxiv.org/abs/2511.14063v1",
        "published_date": "2025-11-18T02:42:24+00:00",
        "updated_date": "2025-11-18T02:42:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongyang Jin",
            "Ryan Xu",
            "Jianhao Zeng",
            "Rui Lan",
            "Yancheng Bai",
            "Lei Sun",
            "Xiangxiang Chu"
        ],
        "tldr": "The paper introduces SCAR, a method to improve instruction adherence and visual fidelity in autoregressive image generation models by enhancing semantic conditioning through compressed semantic prefilling and semantic alignment guidance.",
        "tldr_zh": "该论文介绍了SCAR，一种通过压缩语义预填充和语义对齐指导来增强语义条件，从而提高自回归图像生成模型中指令遵循和视觉保真度的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FashionMAC: Deformation-Free Fashion Image Generation with Fine-Grained Model Appearance Customization",
        "summary": "Garment-centric fashion image generation aims to synthesize realistic and controllable human models dressing a given garment, which has attracted growing interest due to its practical applications in e-commerce. The key challenges of the task lie in two aspects: (1) faithfully preserving the garment details, and (2) gaining fine-grained controllability over the model's appearance. Existing methods typically require performing garment deformation in the generation process, which often leads to garment texture distortions. Also, they fail to control the fine-grained attributes of the generated models, due to the lack of specifically designed mechanisms. To address these issues, we propose FashionMAC, a novel diffusion-based deformation-free framework that achieves high-quality and controllable fashion showcase image generation. The core idea of our framework is to eliminate the need for performing garment deformation and directly outpaint the garment segmented from a dressed person, which enables faithful preservation of the intricate garment details. Moreover, we propose a novel region-adaptive decoupled attention (RADA) mechanism along with a chained mask injection strategy to achieve fine-grained appearance controllability over the synthesized human models. Specifically, RADA adaptively predicts the generated regions for each fine-grained text attribute and enforces the text attribute to focus on the predicted regions by a chained mask injection strategy, significantly enhancing the visual fidelity and the controllability. Extensive experiments validate the superior performance of our framework compared to existing state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2511.14031v1",
        "published_date": "2025-11-18T01:22:14+00:00",
        "updated_date": "2025-11-18T01:22:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rong Zhang",
            "Jinxiao Li",
            "Jingnan Wang",
            "Zhiwen Zuo",
            "Jianfeng Dong",
            "Wei Li",
            "Chi Wang",
            "Weiwei Xu",
            "Xun Wang"
        ],
        "tldr": "FashionMAC is a diffusion-based framework for generating realistic fashion images with fine-grained appearance control, eliminating garment deformation to preserve detail and using region-adaptive decoupled attention for attribute control.",
        "tldr_zh": "FashionMAC是一个基于扩散模型的框架，用于生成具有精细外观控制的逼真时尚图像，通过消除服装变形来保留细节，并使用区域自适应解耦注意力机制来实现属性控制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]