[
    {
        "title": "Accelerating Inference of Masked Image Generators via Reinforcement Learning",
        "summary": "Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.",
        "url": "http://arxiv.org/abs/2512.01094v1",
        "published_date": "2025-11-30T21:28:00+00:00",
        "updated_date": "2025-11-30T21:28:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pranav Subbaraman",
            "Shufan Li",
            "Siyan Zhao",
            "Aditya Grover"
        ],
        "tldr": "This paper introduces Speed-RL, a reinforcement learning-based method for accelerating masked generative models (MGMs) by optimizing for both image quality and speed, achieving a 3x speedup with comparable quality.",
        "tldr_zh": "本文提出了一种基于强化学习的加速掩码生成模型（MGM）的方法Speed-RL，通过优化图像质量和速度，实现了3倍的加速且保持了相当的质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]