[
    {
        "title": "IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks",
        "summary": "Generative Adversarial Networks (GANs) face a significant challenge of striking an optimal balance between high-quality image generation and training stability. Recent techniques, such as DCGAN, BigGAN, and StyleGAN, improve visual fidelity; however, such techniques usually struggle with mode collapse and unstable gradients at high network depth. This paper proposes a novel GAN structural model that incorporates deeper inception-inspired convolution and dilated convolution. This novel model is termed the Inception Generative Adversarial Network (IGAN). The IGAN model generates high-quality synthetic images while maintaining training stability, by reducing mode collapse as well as preventing vanishing and exploding gradients. Our proposed IGAN model achieves the Frechet Inception Distance (FID) of 13.12 and 15.08 on the CUB-200 and ImageNet datasets, respectively, representing a 28-33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. Finally, the two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting. These findings confirm that the IGAN model potentially balances training stability with image generation quality, constituting a scalable and computationally efficient framework for high-fidelity image synthesis.",
        "url": "http://arxiv.org/abs/2601.08332v1",
        "published_date": "2026-01-13T08:42:46+00:00",
        "updated_date": "2026-01-13T08:42:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ahmed A. Hashim",
            "Ali Al-Shuwaili",
            "Asraa Saeed",
            "Ali Al-Bayaty"
        ],
        "tldr": "The paper introduces IGAN, a novel GAN architecture incorporating inception-inspired convolution and dilated convolution, achieving improved FID and IS scores on CUB-200 and ImageNet datasets while enhancing training stability.",
        "tldr_zh": "该论文介绍了一种新的GAN架构IGAN，它结合了基于Inception的卷积和空洞卷积，在CUB-200和ImageNet数据集上实现了改进的FID和IS分数，同时增强了训练稳定性。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "From Local Windows to Adaptive Candidates via Individualized Exploratory: Rethinking Attention for Image Super-Resolution",
        "summary": "Single Image Super-Resolution (SISR) is a fundamental computer vision task that aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) input. Transformer-based methods have achieved remarkable performance by modeling long-range dependencies in degraded images. However, their feature-intensive attention computation incurs high computational cost. To improve efficiency, most existing approaches partition images into fixed groups and restrict attention within each group. Such group-wise attention overlooks the inherent asymmetry in token similarities, thereby failing to enable flexible and token-adaptive attention computation. To address this limitation, we propose the Individualized Exploratory Transformer (IET), which introduces a novel Individualized Exploratory Attention (IEA) mechanism that allows each token to adaptively select its own content-aware and independent attention candidates. This token-adaptive and asymmetric design enables more precise information aggregation while maintaining computational efficiency. Extensive experiments on standard SR benchmarks demonstrate that IET achieves state-of-the-art performance under comparable computational complexity.",
        "url": "http://arxiv.org/abs/2601.08341v1",
        "published_date": "2026-01-13T09:01:20+00:00",
        "updated_date": "2026-01-13T09:01:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunyu Meng",
            "Wei Long",
            "Shuhang Gu"
        ],
        "tldr": "The paper introduces Individualized Exploratory Transformer (IET) with Individualized Exploratory Attention (IEA) for image super-resolution, enabling token-adaptive attention candidates to improve performance and efficiency.",
        "tldr_zh": "该论文介绍了用于图像超分辨率的个性化探索Transformer (IET)，它通过个性化探索注意力(IEA)实现token自适应的注意力候选，以提高性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
        "summary": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.",
        "url": "http://arxiv.org/abs/2601.08303v1",
        "published_date": "2026-01-13T07:46:46+00:00",
        "updated_date": "2026-01-13T07:46:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dongting Hu",
            "Aarush Gupta",
            "Magzhan Gabidolla",
            "Arpit Sahni",
            "Huseyin Coskun",
            "Yanyu Li",
            "Yerlan Idelbayev",
            "Ahsan Mahmood",
            "Aleksei Lebedev",
            "Dishani Lahiri",
            "Anujraaj Goyal",
            "Ju Hu",
            "Mingming Gong",
            "Sergey Tulyakov",
            "Anil Kag"
        ],
        "tldr": "The paper presents an efficient Diffusion Transformer (DiT) framework for image generation on edge devices, combining a compact DiT architecture, elastic training, and knowledge-guided distillation to achieve high-fidelity and low-latency generation.",
        "tldr_zh": "本文提出了一种高效的扩散Transformer (DiT)框架，用于在边缘设备上进行图像生成，它结合了紧凑的DiT架构、弹性训练和知识引导的蒸馏，以实现高保真和低延迟的生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images",
        "summary": "The development of robust artificial intelligence models for histopathology diagnosis is severely constrained by the scarcity of expert-annotated lesion data, particularly for rare pathologies and underrepresented disease subtypes. While data augmentation offers a potential solution, existing methods fail to generate sufficiently realistic lesion morphologies that preserve the complex spatial relationships and cellular architectures characteristic of histopathological tissues. Here we present PathoGen, a diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images. Unlike conventional augmentation techniques, PathoGen leverages the iterative refinement process of diffusion models to synthesize lesions with natural tissue boundaries, preserved cellular structures, and authentic staining characteristics. We validate PathoGen across four diverse datasets representing distinct diagnostic challenges: kidney, skin, breast, and prostate pathology. Quantitative assessment confirms that PathoGen outperforms state-of-the-art generative baselines, including conditional GAN and Stable Diffusion, in image fidelity and distributional similarity. Crucially, we show that augmenting training sets with PathoGen-synthesized lesions enhances downstream segmentation performance compared to traditional geometric augmentations, particularly in data-scarce regimes. Besides, by simultaneously generating realistic morphology and pixel-level ground truth, PathoGen effectively overcomes the manual annotation bottleneck. This approach offers a scalable pathway for developing generalizable medical AI systems despite limited expert-labeled data.",
        "url": "http://arxiv.org/abs/2601.08127v1",
        "published_date": "2026-01-13T01:45:32+00:00",
        "updated_date": "2026-01-13T01:45:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohamad Koohi-Moghadam",
            "Mohammad-Ali Nikouei Mahani",
            "Kyongtae Tyler Bae"
        ],
        "tldr": "The paper introduces PathoGen, a diffusion-based model for synthesizing realistic lesions in histopathology images to augment training data and improve AI model performance in data-scarce medical domains.",
        "tldr_zh": "该论文介绍了PathoGen，一种基于扩散模型的病理图像病灶合成方法，用于扩充训练数据并提高数据稀缺医疗领域中人工智能模型的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "A Single-Parameter Factor-Graph Image Prior",
        "summary": "We propose a novel piecewise smooth image model with piecewise constant local parameters that are automatically adapted to each image. Technically, the model is formulated in terms of factor graphs with NUP (normal with unknown parameters) priors, and the pertinent computations amount to iterations of conjugate-gradient steps and Gaussian message passing. The proposed model and algorithms are demonstrated with applications to denoising and contrast enhancement.",
        "url": "http://arxiv.org/abs/2601.08749v1",
        "published_date": "2026-01-13T17:26:53+00:00",
        "updated_date": "2026-01-13T17:26:53+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "eess.SP"
        ],
        "authors": [
            "Tianyang Wang",
            "Ender Konukoglu",
            "Hans-Andrea Loeliger"
        ],
        "tldr": "This paper proposes a novel piecewise smooth image model using factor graphs with NUP priors for image denoising and contrast enhancement, with parameters adapted to each image.",
        "tldr_zh": "该论文提出了一种新的分段平滑图像模型，使用具有NUP先验的因子图进行图像去噪和对比度增强，参数可以自适应于每张图像。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Blind Deconvolution in Astronomy: How Does a Standalone U-Net Perform?",
        "summary": "Aims: This study investigates whether a U-Net architecture can perform standalone end-to-end blind deconvolution of astronomical images without any prior knowledge of the Point Spread Function (PSF) or noise characteristics. Our goal is to evaluate its performance against the number of training images, classical Tikhonov deconvolution and to assess its generalization capability under varying seeing conditions and noise levels.\n  Methods: Realistic astronomical observations are simulated using the GalSim toolkit, incorporating random transformations, PSF convolution (accounting for both optical and atmospheric effects), and Gaussian white noise. A U-Net model is trained using a Mean Square Error (MSE) loss function on datasets of varying sizes, up to 40,000 images of size 48x48 from the COSMOS Real Galaxy Dataset. Performance is evaluated using PSNR, SSIM, and cosine similarity metrics, with the latter employed in a two-model framework to assess solution stability.\n  Results: The U-Net model demonstrates effectiveness in blind deconvolution, with performance improving consistently as the training dataset size increases, saturating beyond 5,000 images. Cosine similarity analysis reveals convergence between independently trained models, indicating stable solutions. Remarkably, the U-Net outperforms the oracle-like Tikhonov method in challenging conditions (low PSNR/medium SSIM). The model also generalizes well to unseen seeing and noise conditions, although optimal performance is achieved when training parameters include validation conditions. Experiments on synthetic $C^α$ images further support the hypothesis that the U-Net learns a geometry-adaptive harmonic basis, akin to sparse representations observed in denoising tasks. These results align with recent mathematical insights into its adaptive learning capabilities.",
        "url": "http://arxiv.org/abs/2601.08666v1",
        "published_date": "2026-01-13T15:43:57+00:00",
        "updated_date": "2026-01-13T15:43:57+00:00",
        "categories": [
            "astro-ph.IM",
            "cs.CV"
        ],
        "authors": [
            "Jean-Eric Campagne"
        ],
        "tldr": "This paper demonstrates that a U-Net can perform end-to-end blind deconvolution of astronomical images, outperforming Tikhonov deconvolution and generalizing well to unseen conditions after training on a relatively small dataset.",
        "tldr_zh": "该论文展示了U-Net可以在天文图像上执行端到端的盲反卷积，其性能优于Tikhonov反卷积，并且在相对较小的数据集上训练后，可以很好地泛化到未见过的条件。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]