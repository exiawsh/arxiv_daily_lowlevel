[
    {
        "title": "OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion",
        "summary": "Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.",
        "url": "http://arxiv.org/abs/2601.20308v1",
        "published_date": "2026-01-28T06:59:55+00:00",
        "updated_date": "2026-01-28T06:59:55+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Shuoyan Wei",
            "Feng Li",
            "Chen Zhou",
            "Runmin Cong",
            "Yao Zhao",
            "Huihui Bai"
        ],
        "tldr": "The paper introduces OSDEnhancer, a novel one-step diffusion framework for real-world space-time video super-resolution (STVSR) that addresses complex degradations by employing a temporal refinement and spatial enhancement mixture of experts and a bidirectional deformable VAE decoder.",
        "tldr_zh": "本文介绍了一种名为OSDEnhancer的新型单步扩散框架，用于解决真实世界中的时空视频超分辨率(STVSR)问题。该框架通过使用时间细化和空间增强的混合专家以及双向可变形VAE解码器来处理复杂的降级。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration",
        "summary": "All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.",
        "url": "http://arxiv.org/abs/2601.20306v1",
        "published_date": "2026-01-28T06:55:07+00:00",
        "updated_date": "2026-01-28T06:55:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanjie Tu",
            "Qingsen Yan",
            "Axi Niu",
            "Jiacong Tang"
        ],
        "tldr": "The paper introduces TPGDiff, a diffusion-based image restoration network that leverages hierarchical triple priors (degradation, structural, and semantic) to improve restoration performance, particularly in severely degraded regions. It shows improvements across different image restoration tasks.",
        "tldr_zh": "该论文介绍了一种名为 TPGDiff 的基于扩散的图像修复网络，该网络利用分层三重先验（退化先验、结构先验和语义先验）来提高修复性能，尤其是在严重退化的区域。它在不同的图像修复任务中表现出改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs",
        "summary": "Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\\times$ (up to $1.77\\times$).",
        "url": "http://arxiv.org/abs/2601.20273v1",
        "published_date": "2026-01-28T05:42:07+00:00",
        "updated_date": "2026-01-28T05:42:07+00:00",
        "categories": [
            "cs.DC",
            "cs.CV"
        ],
        "authors": [
            "Jiacheng Yang",
            "Jun Wu",
            "Yaoyao Ding",
            "Zhiying Xu",
            "Yida Wang",
            "Gennady Pekhimenko"
        ],
        "tldr": "The paper introduces StreamFusion, a novel DiT serving engine that addresses the limitations of existing sequence parallelism techniques for distributed diffusion transformer inference on GPUs, achieving significant performance improvements.",
        "tldr_zh": "该论文介绍了StreamFusion，一种新型DiT服务引擎，旨在解决现有序列并行技术在GPU上进行分布式扩散Transformer推理的局限性，并实现了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment",
        "summary": "Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \\textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.",
        "url": "http://arxiv.org/abs/2601.20218v1",
        "published_date": "2026-01-28T03:39:05+00:00",
        "updated_date": "2026-01-28T03:39:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyou Deng",
            "Keyu Yan",
            "Chaojie Mao",
            "Xiang Wang",
            "Yu Liu",
            "Changxin Gao",
            "Nong Sang"
        ],
        "tldr": "This paper introduces DenseGRPO, a novel framework that addresses the sparse reward problem in flow matching models for text-to-image generation by using dense rewards and a reward-aware exploration scheme, leading to improved human preference alignment.",
        "tldr_zh": "该论文介绍了DenseGRPO，一种新颖的框架，通过使用密集奖励和奖励感知探索方案，解决了文本到图像生成中流匹配模型的稀疏奖励问题，从而改善了人类偏好对齐。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]