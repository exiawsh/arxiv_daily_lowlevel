[
    {
        "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
        "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
        "url": "http://arxiv.org/abs/2508.17434v1",
        "published_date": "2025-08-24T16:17:33+00:00",
        "updated_date": "2025-08-24T16:17:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linwei Dong",
            "Qingnan Fan",
            "Yuhang Yu",
            "Qi Zhang",
            "Jinwei Chen",
            "Yawei Luo",
            "Changqing Zou"
        ],
        "tldr": "TinySR proposes a compact diffusion model for real-world image super-resolution using pruning and compression techniques to achieve real-time performance with significant speedup and parameter reduction compared to existing methods.",
        "tldr_zh": "TinySR 提出了一种紧凑的扩散模型，用于真实世界的图像超分辨率，采用剪枝和压缩技术，与现有方法相比，实现了实时性能，并显著提高了速度和减少了参数。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction",
        "summary": "Spatial proteomics maps protein distributions in tissues, providing\ntransformative insights for life sciences. However, current sequencing-based\ntechnologies suffer from low spatial resolution, and substantial inter-tissue\nvariability in protein expression further compromises the performance of\nexisting molecular data prediction methods. In this work, we introduce the\nnovel task of spatial super-resolution for sequencing-based spatial proteomics\n(seq-SP) and, to the best of our knowledge, propose the first deep learning\nmodel for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a\nprotein reconstruction problem in continuous space by training a dedicated\nnetwork for each tissue. The model comprises a Spatial Modeling Module, which\nlearns tissue-specific protein spatial distributions, and a Morphology Modeling\nModule, which extracts tissue-specific morphological features. Furthermore, to\nfacilitate rigorous evaluation, we establish an open-source benchmark dataset,\nPseudo-Visium SP, for this task. Experimental results demonstrate that NPF\nachieves state-of-the-art performance with fewer learnable parameters,\nunderscoring its potential for advancing spatial proteomics research. Our code\nand dataset are publicly available at https://github.com/Bokai-Zhao/NPF.",
        "url": "http://arxiv.org/abs/2508.17389v1",
        "published_date": "2025-08-24T14:53:12+00:00",
        "updated_date": "2025-08-24T14:53:12+00:00",
        "categories": [
            "q-bio.QM",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Bokai Zhao",
            "Weiyang Shi",
            "Hanqing Chao",
            "Zijiang Yang",
            "Yiyang Zhang",
            "Ming Song",
            "Tianzi Jiang"
        ],
        "tldr": "The paper introduces Neural Proteomics Fields (NPF), a deep learning model for spatial super-resolution in sequencing-based spatial proteomics, and provides a new benchmark dataset for evaluation.",
        "tldr_zh": "该论文介绍了神经蛋白质组学场（NPF），一种用于基于测序的空间蛋白质组学中的空间超分辨率的深度学习模型，并提供了一个新的基准数据集用于评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation",
        "summary": "The image-to-image generation task aims to produce controllable images by\nleveraging conditional inputs and prompt instructions. However, existing\nmethods often train separate control branches for each type of condition,\nleading to redundant model structures and inefficient use of computational\nresources. To address this, we propose a Unified image-to-image Generation\n(UniGen) framework that supports diverse conditional inputs while enhancing\ngeneration efficiency and expressiveness. Specifically, to tackle the widely\nexisting parameter redundancy and computational inefficiency in controllable\nconditional generation architectures, we propose the Condition Modulated Expert\n(CoMoE) module. This module aggregates semantically similar patch features and\nassigns them to dedicated expert modules for visual representation and\nconditional modeling. By enabling independent modeling of foreground features\nunder different conditions, CoMoE effectively mitigates feature entanglement\nand redundant computation in multi-condition scenarios. Furthermore, to bridge\nthe information gap between the backbone and control branches, we propose\nWeaveNet, a dynamic, snake-like connection mechanism that enables effective\ninteraction between global text-level control from the backbone and\nfine-grained control from conditional branches. Extensive experiments on the\nSubjects-200K and MultiGen-20M datasets across various conditional image\ngeneration tasks demonstrate that our method consistently achieves\nstate-of-the-art performance, validating its advantages in both versatility and\neffectiveness. The code has been uploaded to\nhttps://github.com/gavin-gqzhang/UniGen.",
        "url": "http://arxiv.org/abs/2508.17364v1",
        "published_date": "2025-08-24T13:47:10+00:00",
        "updated_date": "2025-08-24T13:47:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guoqing Zhang",
            "Xingtong Ge",
            "Lu Shi",
            "Xin Zhang",
            "Muqing Xue",
            "Wanru Xu",
            "Yigang Cen"
        ],
        "tldr": "The paper introduces UniGen, a framework for universal and controllable image generation using a Condition Modulated Expert (CoMoE) module and WeaveNet to improve efficiency and expressiveness across diverse conditional inputs.",
        "tldr_zh": "该论文介绍了UniGen，一个用于通用和可控图像生成的框架，它使用条件调制专家（CoMoE）模块和WeaveNet来提高跨各种条件输入的效率和表现力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Structural Damage Detection Using AI Super Resolution and Visual Language Model",
        "summary": "Natural disasters pose significant challenges to timely and accurate damage\nassessment due to their sudden onset and the extensive areas they affect.\nTraditional assessment methods are often labor-intensive, costly, and hazardous\nto personnel, making them impractical for rapid response, especially in\nresource-limited settings. This study proposes a novel, cost-effective\nframework that leverages aerial drone footage, an advanced AI-based video\nsuper-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a\n27 billion parameter Visual Language Model (VLM). This integrated system is\ndesigned to improve low-resolution disaster footage, identify structural\ndamage, and classify buildings into four damage categories, ranging from\nno/slight damage to total destruction, along with associated risk levels. The\nmethodology was validated using pre- and post-event drone imagery from the 2023\nTurkey earthquakes (courtesy of The Guardian) and satellite data from the 2013\nMoore Tornado (xBD dataset). The framework achieved a classification accuracy\nof 84.5%, demonstrating its ability to provide highly accurate results.\nFurthermore, the system's accessibility allows non-technical users to perform\npreliminary analyses, thereby improving the responsiveness and efficiency of\ndisaster management efforts.",
        "url": "http://arxiv.org/abs/2508.17130v1",
        "published_date": "2025-08-23T20:12:06+00:00",
        "updated_date": "2025-08-23T20:12:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Catherine Hoier",
            "Khandaker Mamun Ahmed"
        ],
        "tldr": "This paper presents a framework using drone footage, AI super-resolution (VRT), and a visual language model (Gemma3:27b) to detect and classify structural damage after natural disasters, achieving 84.5% accuracy.",
        "tldr_zh": "本文提出了一种框架，该框架利用无人机镜头、人工智能超分辨率 (VRT) 和视觉语言模型 (Gemma3:27b) 来检测和分类自然灾害后的结构性损坏，准确率达到 84.5%。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Underwater Images via Deep Learning: A Comparative Study of VGG19 and ResNet50-Based Approaches",
        "summary": "This paper addresses the challenging problem of image enhancement in complex\nunderwater scenes by proposing a solution based on deep learning. The proposed\nmethod skillfully integrates two deep convolutional neural network models,\nVGG19 and ResNet50, leveraging their powerful feature extraction capabilities\nto perform multi-scale and multi-level deep feature analysis of underwater\nimages. By constructing a unified model, the complementary advantages of the\ntwo models are effectively integrated, achieving a more comprehensive and\naccurate image enhancement effect.To objectively evaluate the enhancement\neffect, this paper introduces image quality assessment metrics such as PSNR,\nUCIQE, and UIQM to quantitatively compare images before and after enhancement\nand deeply analyzes the performance of different models in different\nscenarios.Furthermore, to improve the practicality and stability of the\nunderwater visual enhancement system, this paper also provides practical\nsuggestions from aspects such as model optimization, multi-model fusion, and\nhardware selection, aiming to provide strong technical support for visual\nenhancement tasks in complex underwater environments.",
        "url": "http://arxiv.org/abs/2508.17397v1",
        "published_date": "2025-08-24T15:10:44+00:00",
        "updated_date": "2025-08-24T15:10:44+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Aoqi Li",
            "Yanghui Song",
            "Jichao Dao",
            "Chengfu Yang"
        ],
        "tldr": "This paper presents a deep learning approach for underwater image enhancement, integrating VGG19 and ResNet50 to improve image quality based on PSNR, UCIQE, and UIQM metrics, while also discussing practical implementation considerations.",
        "tldr_zh": "本文提出了一种基于深度学习的水下图像增强方法，该方法集成了 VGG19 和 ResNet50，并通过 PSNR、UCIQE 和 UIQM 指标来提升图像质量，同时还讨论了实际应用中的考量。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing",
        "summary": "Echocardiography plays a central role in cardiac imaging, offering dynamic\nviews of the heart that are essential for diagnosis and monitoring. However,\nimage quality can be significantly degraded by haze arising from multipath\nreverberations, particularly in difficult-to-image patients. In this work, we\npropose a semantic-guided, diffusion-based dehazing algorithm developed for the\nMICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method\nintegrates a pixel-wise noise model, derived from semantic segmentation of hazy\ninputs into a diffusion posterior sampling framework guided by a generative\nprior trained on clean ultrasound data. Quantitative evaluation on the\nchallenge dataset demonstrates strong performance across contrast and fidelity\nmetrics. Code for the submitted algorithm is available at\nhttps://github.com/tristan-deep/semantic-diffusion-echo-dehazing.",
        "url": "http://arxiv.org/abs/2508.17326v1",
        "published_date": "2025-08-24T12:20:18+00:00",
        "updated_date": "2025-08-24T12:20:18+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Tristan S. W. Stevens",
            "Oisín Nolan",
            "Ruud J. G. van Sloun"
        ],
        "tldr": "This paper presents a semantic-guided diffusion-based dehazing algorithm for cardiac ultrasound images, addressing the issue of haze caused by multipath reverberations. It shows strong performance on a challenge dataset.",
        "tldr_zh": "本文提出了一种基于语义引导的扩散模型去雾算法，用于处理心脏超声图像中由多径混响引起的雾霾问题。 该方法在挑战数据集上表现出强大的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]