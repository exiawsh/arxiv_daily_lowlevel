[
    {
        "title": "Scaling Sequence-to-Sequence Generative Neural Rendering",
        "summary": "We present Kaleido, a family of generative models designed for\nphotorealistic, unified object- and scene-level neural rendering. Kaleido\noperates on the principle that 3D can be regarded as a specialised sub-domain\nof video, expressed purely as a sequence-to-sequence image synthesis task.\nThrough a systemic study of scaling sequence-to-sequence generative neural\nrendering, we introduce key architectural innovations that enable our model to:\ni) perform generative view synthesis without explicit 3D representations; ii)\ngenerate any number of 6-DoF target views conditioned on any number of\nreference views via a masked autoregressive framework; and iii) seamlessly\nunify 3D and video modelling within a single decoder-only rectified flow\ntransformer. Within this unified framework, Kaleido leverages large-scale video\ndata for pre-training, which significantly improves spatial consistency and\nreduces reliance on scarce, camera-labelled 3D datasets -- all without any\narchitectural modifications. Kaleido sets a new state-of-the-art on a range of\nview synthesis benchmarks. Its zero-shot performance substantially outperforms\nother generative methods in few-view settings, and, for the first time, matches\nthe quality of per-scene optimisation methods in many-view settings.",
        "url": "http://arxiv.org/abs/2510.04236v1",
        "published_date": "2025-10-05T15:03:31+00:00",
        "updated_date": "2025-10-05T15:03:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shikun Liu",
            "Kam Woh Ng",
            "Wonbong Jang",
            "Jiadong Guo",
            "Junlin Han",
            "Haozhe Liu",
            "Yiannis Douratsos",
            "Juan C. Pérez",
            "Zijian Zhou",
            "Chi Phung",
            "Tao Xiang",
            "Juan-Manuel Pérez-Rúa"
        ],
        "tldr": "Kaleido is a generative model family that uses sequence-to-sequence image synthesis to achieve state-of-the-art neural rendering, unifying 3D and video modeling and showing significant improvements in view synthesis, particularly in few-view settings.",
        "tldr_zh": "Kaleido是一种生成模型家族，它使用序列到序列的图像合成来实现最先进的神经渲染，统一了3D和视频建模，并在视图合成方面显示出显著的改进，尤其是在少视图设置中。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering",
        "summary": "Autoregressive (AR) models have shown great promise in image generation, yet\nthey face a fundamental inefficiency stemming from their core component: a\nvast, unstructured vocabulary of visual tokens. This conventional approach\ntreats tokens as a flat vocabulary, disregarding the intrinsic structure of the\ntoken embedding space where proximity often correlates with semantic\nsimilarity. This oversight results in a highly complex prediction task, which\nhinders training efficiency and limits final generation quality. To resolve\nthis, we propose Manifold-Aligned Semantic Clustering (MASC), a principled\nframework that constructs a hierarchical semantic tree directly from the\ncodebook's intrinsic structure. MASC employs a novel geometry-aware distance\nmetric and a density-driven agglomerative construction to model the underlying\nmanifold of the token embeddings. By transforming the flat, high-dimensional\nprediction task into a structured, hierarchical one, MASC introduces a\nbeneficial inductive bias that significantly simplifies the learning problem\nfor the AR model. MASC is designed as a plug-and-play module, and our extensive\nexperiments validate its effectiveness: it accelerates training by up to 57%\nand significantly improves generation quality, reducing the FID of LlamaGen-XL\nfrom 2.87 to 2.58. MASC elevates existing AR frameworks to be highly\ncompetitive with state-of-the-art methods, establishing that structuring the\nprediction space is as crucial as architectural innovation for scalable\ngenerative modeling.",
        "url": "http://arxiv.org/abs/2510.04220v1",
        "published_date": "2025-10-05T14:23:51+00:00",
        "updated_date": "2025-10-05T14:23:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Lixuan He",
            "Shikang Zheng",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces MASC, a framework that enhances autoregressive image generation by imposing a hierarchical semantic structure on the token vocabulary, leading to improved training efficiency and generation quality. It's a plug-and-play module that shows improvements with LlamaGen-XL.",
        "tldr_zh": "该论文介绍了MASC，一种通过在token词汇表上施加分层语义结构来增强自回归图像生成的框架，从而提高训练效率和生成质量。 这是一个即插即用模块，在使用LlamaGen-XL时显示出改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Super-resolution image projection over an extended depth of field using a diffractive decoder",
        "summary": "Image projection systems must be efficient in data storage, computation and\ntransmission while maintaining a large space-bandwidth-product (SBP) at their\noutput. Here, we introduce a hybrid image projection system that achieves\nextended depth-of-field (DOF) with improved resolution, combining a\nconvolutional neural network (CNN)-based digital encoder with an all-optical\ndiffractive decoder. A CNN-based encoder compresses input images into compact\nphase representations, which are subsequently displayed by a low-resolution\n(LR) projector and processed by an analog diffractive decoder for all-optical\nimage reconstruction. This optical decoder is completely passive, designed to\nsynthesize pixel super-resolved image projections that feature an extended DOF\nwhile eliminating the need for additional power consumption for super-resolved\nimage reconstruction. Our pixel super-resolution (PSR) image projection system\ndemonstrates high-fidelity image synthesis over an extended DOF of ~267xW,\nwhere W is the illumination wavelength, concurrently offering up to ~16-fold\nSBP improvement at each lateral plane. The proof of concept of this approach is\nvalidated through an experiment conducted in the THz spectrum, and the system\nis scalable across different parts of the electromagnetic spectrum. This image\nprojection architecture can reduce data storage and transmission requirements\nfor display systems without imposing additional power constraints on the\noptical decoder. Beyond extended DOF PSR image projection, the underlying\nprinciples of this approach can be extended to various applications, including\noptical metrology and microscopy.",
        "url": "http://arxiv.org/abs/2510.03938v1",
        "published_date": "2025-10-04T20:42:57+00:00",
        "updated_date": "2025-10-04T20:42:57+00:00",
        "categories": [
            "physics.optics",
            "cs.CV",
            "cs.NE",
            "physics.app-ph"
        ],
        "authors": [
            "Hanlong Chen",
            "Cagatay Isil",
            "Tianyi Gan",
            "Mona Jarrahi",
            "Aydogan Ozcan"
        ],
        "tldr": "This paper presents a hybrid image projection system using a CNN encoder and a diffractive optical decoder to achieve super-resolution with extended depth-of-field, demonstrating significant space-bandwidth-product improvement and potential applications in various fields.",
        "tldr_zh": "本文提出了一种混合图像投影系统，该系统采用 CNN 编码器和衍射光学解码器，以实现超分辨率和扩展景深，展示了显着的空间带宽积改进，并具有在各个领域中的潜在应用。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]