[
    {
        "title": "An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation",
        "summary": "Computed tomography (CT) is one of the most widely used non-invasive imaging\nmodalities for medical diagnosis. In clinical practice, CT images are usually\nacquired with large slice thicknesses due to the high cost of memory storage\nand operation time, resulting in an anisotropic CT volume with much lower\ninter-slice resolution than in-plane resolution. Since such inconsistent\nresolution may lead to difficulties in disease diagnosis, deep learning-based\nvolumetric super-resolution methods have been developed to improve inter-slice\nresolution. Most existing methods conduct single-image super-resolution on the\nthrough-plane or synthesize intermediate slices from adjacent slices; however,\nthe anisotropic characteristic of 3D CT volume has not been well explored. In\nthis paper, we propose a novel cross-view texture transfer approach for CT\nslice interpolation by fully utilizing the anisotropic nature of 3D CT volume.\nSpecifically, we design a unique framework that takes high-resolution in-plane\ntexture details as a reference and transfers them to low-resolution\nthrough-plane images. To this end, we introduce a multi-reference non-local\nattention module that extracts meaningful features for reconstructing\nthrough-plane high-frequency details from multiple in-plane images. Through\nextensive experiments, we demonstrate that our method performs significantly\nbetter in CT slice interpolation than existing competing methods on public CT\ndatasets including a real-paired benchmark, verifying the effectiveness of the\nproposed framework. The source code of this work is available at\nhttps://github.com/khuhm/ACVTT.",
        "url": "http://arxiv.org/abs/2509.20242v1",
        "published_date": "2025-09-24T15:32:39+00:00",
        "updated_date": "2025-09-24T15:32:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kwang-Hyun Uhm",
            "Hyunjun Cho",
            "Sung-Hoo Hong",
            "Seung-Won Jung"
        ],
        "tldr": "This paper introduces a novel cross-view texture transfer approach using multi-reference non-local attention for CT slice interpolation, leveraging the anisotropic nature of 3D CT volumes to improve inter-slice resolution.",
        "tldr_zh": "本文提出了一种新颖的交叉视角纹理传递方法，该方法使用多参考非局部注意力进行CT切片插值，利用3D CT体各向异性的特性来提高切片间分辨率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "StrCGAN: A Generative Framework for Stellar Image Restoration",
        "summary": "We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to\nenhance low-resolution astrophotography images. Our goal is to reconstruct\nhigh-fidelity ground truth-like representations of celestial objects, a task\nthat is challenging due to the limited resolution and quality of\nsmall-telescope observations such as the MobilTelesco dataset. Traditional\nmodels such as CycleGAN provide a foundation for image-to-image translation but\nare restricted to 2D mappings and often distort the morphology of stars and\ngalaxies. To overcome these limitations, we extend the CycleGAN framework with\nthree key innovations: 3D convolutional layers to capture volumetric spatial\ncorrelations, multi-spectral fusion to align optical and near-infrared (NIR)\ndomains, and astrophysical regularization modules to preserve stellar\nmorphology. Ground-truth references from multi-mission all-sky surveys spanning\noptical to NIR guide the training process, ensuring that reconstructions remain\nconsistent across spectral bands. Together, these components allow StrCGAN to\ngenerate reconstructions that are not only visually sharper but also physically\nconsistent, outperforming standard GAN models in the task of astrophysical\nimage enhancement.",
        "url": "http://arxiv.org/abs/2509.19805v1",
        "published_date": "2025-09-24T06:42:32+00:00",
        "updated_date": "2025-09-24T06:42:32+00:00",
        "categories": [
            "cs.CV",
            "astro-ph.IM",
            "astro-ph.SR"
        ],
        "authors": [
            "Shantanusinh Parmar"
        ],
        "tldr": "The paper introduces StrCGAN, a novel generative model for enhancing low-resolution astrophotography images by incorporating 3D convolutions, multi-spectral fusion, and astrophysical regularization within a CycleGAN framework to improve image quality and preserve stellar morphology.",
        "tldr_zh": "该论文介绍了StrCGAN，一种新型生成模型，通过在CycleGAN框架中结合3D卷积、多光谱融合和天体物理正则化来增强低分辨率天体摄影图像，从而提高图像质量并保持恒星形态。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]