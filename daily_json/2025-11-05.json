[
    {
        "title": "KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image",
        "summary": "Satellite image inpainting is a crucial task in remote sensing, where\naccurately restoring missing or occluded regions is essential for robust image\nanalysis. In this paper, we propose KAO, a novel framework that utilizes\nKernel-Adaptive Optimization within diffusion models for satellite image\ninpainting. KAO is specifically designed to address the challenges posed by\nvery high-resolution (VHR) satellite datasets, such as DeepGlobe and the\nMassachusetts Roads Dataset. Unlike existing methods that rely on\npreconditioned models requiring extensive retraining or postconditioned models\nwith significant computational overhead, KAO introduces a Latent Space\nConditioning approach, optimizing a compact latent space to achieve efficient\nand accurate inpainting. Furthermore, we incorporate Explicit Propagation into\nthe diffusion process, facilitating forward-backward fusion, which improves the\nstability and precision of the method. Experimental results demonstrate that\nKAO sets a new benchmark for VHR satellite image restoration, providing a\nscalable, high-performance solution that balances the efficiency of\npreconditioned models with the flexibility of postconditioned models.",
        "url": "http://arxiv.org/abs/2511.02462v1",
        "published_date": "2025-11-04T10:44:36+00:00",
        "updated_date": "2025-11-04T10:44:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Teerapong Panboonyuen"
        ],
        "tldr": "The paper introduces KAO, a kernel-adaptive optimization framework within diffusion models for efficient and accurate satellite image inpainting, addressing challenges in very high-resolution datasets by using latent space conditioning and explicit propagation.",
        "tldr_zh": "本文提出了 KAO，一个扩散模型中的内核自适应优化框架，用于高效准确的卫星图像修复，通过使用潜在空间调节和显式传播来解决超高分辨率数据集中的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "High-Resolution Magnetic Particle Imaging System Matrix Recovery Using a Vision Transformer with Residual Feature Network",
        "summary": "This study presents a hybrid deep learning framework, the Vision Transformer\nwith Residual Feature Network (VRF-Net), for recovering high-resolution system\nmatrices in Magnetic Particle Imaging (MPI). MPI resolution often suffers from\ndownsampling and coil sensitivity variations. VRF-Net addresses these\nchallenges by combining transformer-based global attention with residual\nconvolutional refinement, enabling recovery of both large-scale structures and\nfine details. To reflect realistic MPI conditions, the system matrix is\ndegraded using a dual-stage downsampling strategy. Training employed\npaired-image super-resolution on the public Open MPI dataset and a simulated\ndataset incorporating variable coil sensitivity profiles. For system matrix\nrecovery on the Open MPI dataset, VRF-Net achieved nRMSE = 0.403, pSNR = 39.08\ndB, and SSIM = 0.835 at 2x scaling, and maintained strong performance even at\nchallenging scale 8x (pSNR = 31.06 dB, SSIM = 0.717). For the simulated\ndataset, VRF-Net achieved nRMSE = 4.44, pSNR = 28.52 dB, and SSIM = 0.771 at 2x\nscaling, with stable performance at higher scales. On average, it reduced nRMSE\nby 88.2%, increased pSNR by 44.7%, and improved SSIM by 34.3% over\ninterpolation and CNN-based methods. In image reconstruction of Open MPI\nphantoms, VRF-Net further reduced reconstruction error to nRMSE = 1.79 at 2x\nscaling, while preserving structural fidelity (pSNR = 41.58 dB, SSIM = 0.960),\noutperforming existing methods. These findings demonstrate that VRF-Net enables\nsharper, artifact-free system matrix recovery and robust image reconstruction\nacross multiple scales, offering a promising direction for future in vivo\napplications.",
        "url": "http://arxiv.org/abs/2511.02212v1",
        "published_date": "2025-11-04T03:03:39+00:00",
        "updated_date": "2025-11-04T03:03:39+00:00",
        "categories": [
            "physics.med-ph",
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Abuobaida M. Khair",
            "Wenjing Jiang",
            "Yousuf Babiker M. Osman",
            "Wenjun Xia",
            "Xiaopeng Ma"
        ],
        "tldr": "The paper introduces VRF-Net, a Vision Transformer with Residual Feature Network, for high-resolution system matrix recovery in Magnetic Particle Imaging (MPI), demonstrating improved performance in image reconstruction and reduced artifacts compared to existing methods.",
        "tldr_zh": "该论文介绍了一种名为VRF-Net的视觉Transformer与残差特征网络的混合深度学习框架，用于磁性粒子成像（MPI）中的高分辨率系统矩阵恢复，与现有方法相比，在图像重建方面表现出更好的性能并减少了伪影。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers",
        "summary": "Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta\npositron emission tomography (Abeta-PET), which is limited by high cost and\nlimited accessibility. This study explores whether Abeta-PET spatial patterns\ncan be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We\ncollected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566\nparticipants. A language-enhanced generative model, driven by a large language\nmodel (LLM) and multimodal information fusion, was developed to synthesize PET\nimages. Synthesized images were evaluated for image quality, diagnostic\nconsistency, and clinical applicability within a fully automated diagnostic\npipeline. Findings: The synthetic PET images closely resemble real PET scans in\nboth structural details (SSIM = 0.920 +/- 0.003) and regional patterns\n(Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show\nhigh agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic\nPET, we developed a fully automatic AD diagnostic pipeline integrating PET\nsynthesis and classification. The synthetic PET-based model (AUC = 0.78)\noutperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while\ncombining synthetic PET and BBMs further improved performance (AUC = 0.79).\nAblation analysis supports the advantages of LLM integration and prompt\nengineering. Interpretation: Our language-enhanced generative model synthesizes\nrealistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial\npattern assessment and improving the diagnostic workflow for Alzheimer's\ndisease.",
        "url": "http://arxiv.org/abs/2511.02206v1",
        "published_date": "2025-11-04T02:53:25+00:00",
        "updated_date": "2025-11-04T02:53:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengjie Zhang",
            "Xiaoxie Mao",
            "Qihao Guo",
            "Shaoting Zhang",
            "Qi Huang",
            "Mu Zhou",
            "Fang Xie",
            "Mianxin Liu"
        ],
        "tldr": "This paper presents a language-enhanced generative model to synthesize amyloid-beta PET images from MRI and blood biomarkers, showing promising results for Alzheimer's disease diagnosis and potentially reducing the reliance on expensive PET scans.",
        "tldr_zh": "本文提出了一种语言增强生成模型，用于从MRI和血液生物标志物合成淀粉样蛋白-β PET图像，在阿尔茨海默病诊断方面显示出有希望的结果，并可能减少对昂贵PET扫描的依赖。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Locally-Supervised Global Image Restoration",
        "summary": "We address the problem of image reconstruction from incomplete measurements,\nencompassing both upsampling and inpainting, within a learning-based framework.\nConventional supervised approaches require fully sampled ground truth data,\nwhile self-supervised methods allow incomplete ground truth but typically rely\non random sampling that, in expectation, covers the entire image. In contrast,\nwe consider fixed, deterministic sampling patterns with inherently incomplete\ncoverage, even in expectation. To overcome this limitation, we exploit multiple\ninvariances of the underlying image distribution, which theoretically allows us\nto achieve the same reconstruction performance as fully supervised approaches.\nWe validate our method on optical-resolution image upsampling in photoacoustic\nmicroscopy (PAM), demonstrating competitive or superior results while requiring\nsubstantially less ground truth data.",
        "url": "http://arxiv.org/abs/2511.01998v1",
        "published_date": "2025-11-03T19:12:25+00:00",
        "updated_date": "2025-11-03T19:12:25+00:00",
        "categories": [
            "cs.CV",
            "cs.NA",
            "math.NA"
        ],
        "authors": [
            "Benjamin Walder",
            "Daniel Toader",
            "Robert Nuster",
            "Günther Paltauf",
            "Peter Burgholzer",
            "Gregor Langer",
            "Lukas Krainer",
            "Markus Haltmeier"
        ],
        "tldr": "This paper presents a learning-based image restoration method for upsampling and inpainting with incomplete and deterministic sampling patterns, achieving competitive performance with less ground truth data.",
        "tldr_zh": "本文提出了一种基于学习的图像恢复方法，用于在不完整和确定性的采样模式下进行上采样和图像修复，并以更少的真实数据实现了有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]