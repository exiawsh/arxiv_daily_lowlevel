[
    {
        "title": "CloudBreaker: Breaking the Cloud Covers of Sentinel-2 Images using Multi-Stage Trained Conditional Flow Matching on Sentinel-1",
        "summary": "Cloud cover and nighttime conditions remain significant limitations in\nsatellite-based remote sensing, often restricting the availability and\nusability of multi-spectral imagery. In contrast, Sentinel-1 radar images are\nunaffected by cloud cover and can provide consistent data regardless of weather\nor lighting conditions. To address the challenges of limited satellite imagery,\nwe propose CloudBreaker, a novel framework that generates high-quality\nmulti-spectral Sentinel-2 signals from Sentinel-1 data. This includes the\nreconstruction of optical (RGB) images as well as critical vegetation and water\nindices such as NDVI and NDWI.We employed a novel multi-stage training approach\nbased on conditional latent flow matching and, to the best of our knowledge,\nare the first to integrate cosine scheduling with flow matching. CloudBreaker\ndemonstrates strong performance, achieving a Frechet Inception Distance (FID)\nscore of 0.7432, indicating high fidelity and realism in the generated optical\nimagery. The model also achieved Structural Similarity Index Measure (SSIM) of\n0.6156 for NDWI and 0.6874 for NDVI, indicating a high degree of structural\nsimilarity. This establishes CloudBreaker as a promising solution for a wide\nrange of remote sensing applications where multi-spectral data is typically\nunavailable or unreliable",
        "url": "http://arxiv.org/abs/2508.03608v1",
        "published_date": "2025-08-05T16:25:18+00:00",
        "updated_date": "2025-08-05T16:25:18+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Saleh Sakib Ahmed",
            "Sara Nowreen",
            "M. Sohel Rahman"
        ],
        "tldr": "The paper introduces CloudBreaker, a novel framework using multi-stage trained conditional flow matching to generate high-quality Sentinel-2 multi-spectral imagery (RGB, NDVI, NDWI) from Sentinel-1 radar data, overcoming cloud cover limitations.",
        "tldr_zh": "该论文介绍了CloudBreaker，一种新颖的框架，它使用多阶段训练的条件流匹配从Sentinel-1雷达数据生成高质量的Sentinel-2多光谱图像（RGB，NDVI，NDWI），克服了云层覆盖的限制。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CADD: Context aware disease deviations via restoration of brain images using normative conditional diffusion models",
        "summary": "Applying machine learning to real-world medical data, e.g. from hospital\narchives, has the potential to revolutionize disease detection in brain images.\nHowever, detecting pathology in such heterogeneous cohorts is a difficult\nchallenge. Normative modeling, a form of unsupervised anomaly detection, offers\na promising approach to studying such cohorts where the ``normal'' behavior is\nmodeled and can be used at subject level to detect deviations relating to\ndisease pathology. Diffusion models have emerged as powerful tools for anomaly\ndetection due to their ability to capture complex data distributions and\ngenerate high-quality images. Their performance relies on image restoration;\ndifferences between the original and restored images highlight potential\nabnormalities. However, unlike normative models, these diffusion model\napproaches do not incorporate clinical information which provides important\ncontext to guide the disease detection process. Furthermore, standard\napproaches often poorly restore healthy regions, resulting in poor\nreconstructions and suboptimal detection performance. We present CADD, the\nfirst conditional diffusion model for normative modeling in 3D images. To guide\nthe healthy restoration process, we propose a novel inference inpainting\nstrategy which balances anomaly removal with retention of subject-specific\nfeatures. Evaluated on three challenging datasets, including clinical scans,\nwhich may have lower contrast, thicker slices, and motion artifacts, CADD\nachieves state-of-the-art performance in detecting neurological abnormalities\nin heterogeneous cohorts.",
        "url": "http://arxiv.org/abs/2508.03594v1",
        "published_date": "2025-08-05T15:59:19+00:00",
        "updated_date": "2025-08-05T15:59:19+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Ana Lawry Aguila",
            "Ayodeji Ijishakin",
            "Juan Eugenio Iglesias",
            "Tomomi Takenaga",
            "Yukihiro Nomura",
            "Takeharu Yoshikawa",
            "Osamu Abe",
            "Shouhei Hanaoka"
        ],
        "tldr": "The paper introduces CADD, a conditional diffusion model for normative modeling in 3D brain images, which incorporates clinical information for improved anomaly detection and outperforms existing methods on heterogeneous clinical datasets.",
        "tldr_zh": "该论文介绍了CADD，一种用于3D脑图像规范建模的条件扩散模型，它结合了临床信息以改进异常检测，并在异构临床数据集上优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation",
        "summary": "Emotional Image Content Generation (EICG) aims to generate semantically clear\nand emotionally faithful images based on given emotion categories, with broad\napplication prospects. While recent text-to-image diffusion models excel at\ngenerating concrete concepts, they struggle with the complexity of abstract\nemotions. There have also emerged methods specifically designed for EICG, but\nthey excessively rely on word-level attribute labels for guidance, which suffer\nfrom semantic incoherence, ambiguity, and limited scalability. To address these\nchallenges, we propose CoEmoGen, a novel pipeline notable for its semantic\ncoherence and high scalability. Specifically, leveraging multimodal large\nlanguage models (MLLMs), we construct high-quality captions focused on\nemotion-triggering content for context-rich semantic guidance. Furthermore,\ninspired by psychological insights, we design a Hierarchical Low-Rank\nAdaptation (HiLoRA) module to cohesively model both polarity-shared low-level\nfeatures and emotion-specific high-level semantics. Extensive experiments\ndemonstrate CoEmoGen's superiority in emotional faithfulness and semantic\ncoherence from quantitative, qualitative, and user study perspectives. To\nintuitively showcase scalability, we curate EmoArt, a large-scale dataset of\nemotionally evocative artistic images, providing endless inspiration for\nemotion-driven artistic creation. The dataset and code are available at\nhttps://github.com/yuankaishen2001/CoEmoGen.",
        "url": "http://arxiv.org/abs/2508.03535v1",
        "published_date": "2025-08-05T15:04:34+00:00",
        "updated_date": "2025-08-05T15:04:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaishen Yuan",
            "Yuting Zhang",
            "Shang Gao",
            "Yijie Zhu",
            "Wenshuo Chen",
            "Yutao Yue"
        ],
        "tldr": "CoEmoGen is a new pipeline for generating emotionally faithful and semantically coherent images by leveraging MLLMs for high-quality captions and a HiLoRA module for modeling emotion-specific semantics. They also introduce a new large-scale dataset, EmoArt.",
        "tldr_zh": "CoEmoGen是一个新的图像生成流程，它利用多模态大型语言模型生成高质量的图像描述，并使用HiLoRA模块来建模特定于情感的语义，从而生成情感真实且语义连贯的图像。此外，他们还引入了一个新的大型数据集EmoArt。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RAAG: Ratio Aware Adaptive Guidance",
        "summary": "Flow-based generative models have recently achieved remarkable progress in\nimage and video synthesis, with classifier-free guidance (CFG) becoming the\nstandard tool for high-fidelity, controllable generation. However, despite\ntheir practical success, little is known about how guidance interacts with\ndifferent stages of the sampling process-especially in the fast, low-step\nregimes typical of modern flow-based pipelines. In this work, we uncover and\nanalyze a fundamental instability: the earliest reverse steps are acutely\nsensitive to the guidance scale, owing to a pronounced spike in the relative\nstrength (RATIO) of conditional to unconditional predictions. Through rigorous\ntheoretical analysis and empirical validation, we show that this RATIO spike is\nintrinsic to the data distribution, independent of the model architecture, and\ncauses exponential error amplification when paired with strong guidance. To\naddress this, we propose a simple, theoretically grounded, RATIO-aware adaptive\nguidance schedule that automatically dampens the guidance scale at early steps\nbased on the evolving RATIO, using a closed-form exponential decay. Our method\nis lightweight, requires no additional inference overhead, and is compatible\nwith standard flow frameworks. Experiments across state-of-the-art image\n(SD3.5, Lumina) and video (WAN2.1) models demonstrate that our approach enables\nup to 3x faster sampling while maintaining or improving generation quality,\nrobustness, and semantic alignment. Extensive ablation studies further confirm\nthe generality and stability of our schedule across models, datasets, and\nhyperparameters. Our findings highlight the critical role of stepwise guidance\nadaptation in unlocking the full potential of fast flow-based generative\nmodels.",
        "url": "http://arxiv.org/abs/2508.03442v1",
        "published_date": "2025-08-05T13:41:05+00:00",
        "updated_date": "2025-08-05T13:41:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shangwen Zhu",
            "Qianyu Peng",
            "Yuting Hu",
            "Zhantao Yang",
            "Han Zhang",
            "Zhao Pu",
            "Ruili Feng",
            "Fan Cheng"
        ],
        "tldr": "This paper identifies and addresses an instability in early steps of classifier-free guidance for flow-based generative models, proposing a Ratio Aware Adaptive Guidance (RAAG) schedule to improve sampling speed and quality.",
        "tldr_zh": "该论文发现并解决了基于流的生成模型中无分类器指导在早期步骤中存在的不稳定性，提出了一种比例感知自适应指导（RAAG）策略，以提高采样速度和质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration",
        "summary": "Diffusion models have revealed powerful potential in all-in-one image\nrestoration (AiOIR), which is talented in generating abundant texture details.\nThe existing AiOIR methods either retrain a diffusion model or fine-tune the\npretrained diffusion model with extra conditional guidance. However, they often\nsuffer from high inference costs and limited adaptability to diverse\ndegradation types. In this paper, we propose an efficient AiOIR method,\nDiffusion Once and Done (DOD), which aims to achieve superior restoration\nperformance with only one-step sampling of Stable Diffusion (SD) models.\nSpecifically, multi-degradation feature modulation is first introduced to\ncapture different degradation prompts with a pretrained diffusion model. Then,\nparameter-efficient conditional low-rank adaptation integrates the prompts to\nenable the fine-tuning of the SD model for adapting to different degradation\ntypes. Besides, a high-fidelity detail enhancement module is integrated into\nthe decoder of SD to improve structural and textural details. Experiments\ndemonstrate that our method outperforms existing diffusion-based restoration\napproaches in both visual quality and inference efficiency.",
        "url": "http://arxiv.org/abs/2508.03373v1",
        "published_date": "2025-08-05T12:26:28+00:00",
        "updated_date": "2025-08-05T12:26:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ni Tang",
            "Xiaotong Luo",
            "Zihan Cheng",
            "Liangtai Zhou",
            "Dongxiao Zhang",
            "Yanyun Qu"
        ],
        "tldr": "The paper introduces Diffusion Once and Done (DOD), an efficient all-in-one image restoration method that uses one-step sampling of Stable Diffusion with degradation-aware LoRA and detail enhancement, outperforming existing diffusion-based approaches in speed and quality.",
        "tldr_zh": "该论文介绍了一种高效的一体化图像修复方法 Diffusion Once and Done (DOD)，它使用 Stable Diffusion 的单步采样，结合感知退化的 LoRA 和细节增强，在速度和质量上优于现有的基于扩散的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Illumination: Fine-Grained Detail Preservation in Extreme Dark Image Restoration",
        "summary": "Recovering fine-grained details in extremely dark images remains challenging\ndue to severe structural information loss and noise corruption. Existing\nenhancement methods often fail to preserve intricate details and sharp edges,\nlimiting their effectiveness in downstream applications like text and edge\ndetection. To address these deficiencies, we propose an efficient dual-stage\napproach centered on detail recovery for dark images. In the first stage, we\nintroduce a Residual Fourier-Guided Module (RFGM) that effectively restores\nglobal illumination in the frequency domain. RFGM captures inter-stage and\ninter-channel dependencies through residual connections, providing robust\npriors for high-fidelity frequency processing while mitigating error\naccumulation risks from unreliable priors. The second stage employs\ncomplementary Mamba modules specifically designed for textural structure\nrefinement: (1) Patch Mamba operates on channel-concatenated non-downsampled\npatches, meticulously modeling pixel-level correlations to enhance fine-grained\ndetails without resolution loss. (2) Grad Mamba explicitly focuses on\nhigh-gradient regions, alleviating state decay in state space models and\nprioritizing reconstruction of sharp edges and boundaries. Extensive\nexperiments on multiple benchmark datasets and downstream applications\ndemonstrate that our method significantly improves detail recovery performance\nwhile maintaining efficiency. Crucially, the proposed modules are lightweight\nand can be seamlessly integrated into existing Fourier-based frameworks with\nminimal computational overhead. Code is available at\nhttps://github.com/bywlzts/RFGM.",
        "url": "http://arxiv.org/abs/2508.03336v1",
        "published_date": "2025-08-05T11:31:08+00:00",
        "updated_date": "2025-08-05T11:31:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tongshun Zhang",
            "Pingping Liu",
            "Zixuan Zhong",
            "Zijian Zhang",
            "Qiuzhan Zhou"
        ],
        "tldr": "This paper introduces a dual-stage dark image restoration approach using a Residual Fourier-Guided Module and complementary Mamba modules for fine-grained detail preservation, achieving state-of-the-art performance with efficiency.",
        "tldr_zh": "该论文提出了一种双阶段的暗图像恢复方法，使用残差傅里叶引导模块和互补的Mamba模块来精细地保留细节，实现了最先进的性能，同时保持了效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation",
        "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model\nthat unifies image understanding, text-to-image generation, and image editing\nwithin a single architecture-eliminating the need for task-specific adapters or\ninter-module connectors-and demonstrate that compact multimodal systems can\nachieve state-of-the-art performance on commodity hardware. Skywork UniPic\nachieves a GenEval score of 0.86, surpassing most existing unified models; sets\na new DPG-Bench complex-generation record of 85.5; attains 5.83 on\nGEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x\n1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled\nencoding strategy that leverages a masked autoregressive encoder for synthesis\nand a SigLIP2 encoder for understanding, all feeding a shared autoregressive\ndecoder; (2) a progressive, resolution-aware training schedule scaling from 256\nx 256 to 1024 x 1024 while dynamically unfreezing parameters to balance\ncapacity and stability; and (3) meticulously curated, 100 million-scale\ndatasets augmented with task-specific reward models to refine generation and\nediting objectives. By demonstrating that high-fidelity multimodal integration\nneed not incur prohibitive resource demands, Skywork UniPic establishes a\npractical paradigm for deployable, high-fidelity multimodal AI. Code and\nweights are publicly available at\nhttps://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
        "url": "http://arxiv.org/abs/2508.03320v1",
        "published_date": "2025-08-05T10:59:01+00:00",
        "updated_date": "2025-08-05T10:59:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peiyu Wang",
            "Yi Peng",
            "Yimeng Gan",
            "Liang Hu",
            "Tianyidan Xie",
            "Xiaokun Wang",
            "Yichen Wei",
            "Chuanxin Tang",
            "Bo Zhu",
            "Changshi Li",
            "Hongyang Wei",
            "Eric Li",
            "Xuchen Song",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "tldr": "Skywork UniPic is a 1.5B parameter autoregressive model that unifies image understanding, generation, and editing, achieving SOTA performance with efficient resource usage and publicly available code and weights.",
        "tldr_zh": "Skywork UniPic 是一个15亿参数的自回归模型，它统一了图像理解、生成和编辑，以高效的资源利用实现了SOTA性能，并公开发布了代码和权重。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution",
        "summary": "Event cameras offer unparalleled advantages such as high temporal resolution,\nlow latency, and high dynamic range. However, their limited spatial resolution\nposes challenges for fine-grained perception tasks. In this work, we propose an\nultra-lightweight, stream-based event-to-event super-resolution method based on\nSpiking Neural Networks (SNNs), designed for real-time deployment on\nresource-constrained devices. To further reduce model size, we introduce a\nnovel Dual-Forward Polarity-Split Event Encoding strategy that decouples\npositive and negative events into separate forward paths through a shared SNN.\nFurthermore, we propose a Learnable Spatio-temporal Polarity-aware Loss\n(LearnSTPLoss) that adaptively balances temporal, spatial, and polarity\nconsistency using learnable uncertainty-based weights. Experimental results\ndemonstrate that our method achieves competitive super-resolution performance\non multiple datasets while significantly reducing model size and inference\ntime. The lightweight design enables embedding the module into event cameras or\nusing it as an efficient front-end preprocessing for downstream vision tasks.",
        "url": "http://arxiv.org/abs/2508.03244v1",
        "published_date": "2025-08-05T09:24:02+00:00",
        "updated_date": "2025-08-05T09:24:02+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Chuanzhi Xu",
            "Haoxian Zhou",
            "Langyi Chen",
            "Yuk Ying Chung",
            "Qiang Qu"
        ],
        "tldr": "The paper proposes an ultralight Spiking Neural Network (SNN) for event-stream super-resolution, using a polarity-split encoding and a learnable spatio-temporal polarity-aware loss to reduce model size and inference time.",
        "tldr_zh": "该论文提出了一种用于事件流超分辨率的超轻量级脉冲神经网络（SNN），它使用极性分离编码和可学习的时空极性感知损失来减少模型大小和推理时间。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution",
        "summary": "Arbitrary-resolution super-resolution (ARSR) provides crucial flexibility for\nmedical image analysis by adapting to diverse spatial resolutions. However,\ntraditional CNN-based methods are inherently ill-suited for ARSR, as they are\ntypically designed for fixed upsampling factors. While INR-based methods\novercome this limitation, they still struggle to effectively process and\nleverage multi-modal images with varying resolutions and details. In this\npaper, we propose Nexus-INR, a Diverse Knowledge-guided ARSR framework, which\nemploys varied information and downstream tasks to achieve high-quality,\nadaptive-resolution medical image super-resolution. Specifically, Nexus-INR\ncontains three key components. A dual-branch encoder with an auxiliary\nclassification task to effectively disentangle shared anatomical structures and\nmodality-specific features; a knowledge distillation module using cross-modal\nattention that guides low-resolution modality reconstruction with\nhigh-resolution reference, enhanced by self-supervised consistency loss; an\nintegrated segmentation module that embeds anatomical semantics to improve both\nreconstruction quality and downstream segmentation performance. Experiments on\nthe BraTS2020 dataset for both super-resolution and downstream segmentation\ndemonstrate that Nexus-INR outperforms state-of-the-art methods across various\nmetrics.",
        "url": "http://arxiv.org/abs/2508.03073v1",
        "published_date": "2025-08-05T04:44:35+00:00",
        "updated_date": "2025-08-05T04:44:35+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Bo Zhang",
            "JianFei Huo",
            "Zheng Zhang",
            "Wufan Wang",
            "Hui Gao",
            "Xiangyang Gong",
            "Wendong Wang"
        ],
        "tldr": "The paper introduces Nexus-INR, a novel INR-based framework for arbitrary-scale multimodal medical image super-resolution, leveraging diverse knowledge and downstream tasks to improve reconstruction quality and segmentation performance.",
        "tldr_zh": "该论文介绍了 Nexus-INR，一种新颖的基于 INR 的任意尺度多模态医学图像超分辨率框架，利用多样化的知识和下游任务来提高重建质量和分割性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Models with Adaptive Negative Sampling Without External Resources",
        "summary": "Diffusion models (DMs) have demonstrated an unparalleled ability to create\ndiverse and high-fidelity images from text prompts. However, they are also\nwell-known to vary substantially regarding both prompt adherence and quality.\nNegative prompting was introduced to improve prompt compliance by specifying\nwhat an image must not contain. Previous works have shown the existence of an\nideal negative prompt that can maximize the odds of the positive prompt. In\nthis work, we explore relations between negative prompting and classifier-free\nguidance (CFG) to develop a sampling procedure, {\\it Adaptive Negative Sampling\nWithout External Resources} (ANSWER), that accounts for both positive and\nnegative conditions from a single prompt. This leverages the internal\nunderstanding of negation by the diffusion model to increase the odds of\ngenerating images faithful to the prompt. ANSWER is a training-free technique,\napplicable to any model that supports CFG, and allows for negative grounding of\nimage concepts without an explicit negative prompts, which are lossy and\nincomplete. Experiments show that adding ANSWER to existing DMs outperforms the\nbaselines on multiple benchmarks and is preferred by humans 2x more over the\nother methods.",
        "url": "http://arxiv.org/abs/2508.02973v1",
        "published_date": "2025-08-05T00:45:54+00:00",
        "updated_date": "2025-08-05T00:45:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alakh Desai",
            "Nuno Vasconcelos"
        ],
        "tldr": "This paper introduces ANSWER, a training-free method for improving prompt adherence in diffusion models by leveraging the model's internal understanding of negation, outperforming existing baselines without needing external resources.",
        "tldr_zh": "该论文介绍了ANSWER，一种无需训练的方法，通过利用扩散模型对否定的内部理解来提高扩散模型中的提示遵循度，在不需要外部资源的情况下优于现有的基线。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Robust Image Denoising with Scale Equivariance",
        "summary": "Despite notable advances in image denoising, existing models often struggle\nto generalize beyond in-distribution noise patterns, particularly when\nconfronted with out-of-distribution (OOD) conditions characterized by spatially\nvariant noise. This generalization gap remains a fundamental yet underexplored\nchallenge. In this work, we investigate \\emph{scale equivariance} as a core\ninductive bias for improving OOD robustness. We argue that incorporating\nscale-equivariant structures enables models to better adapt from training on\nspatially uniform noise to inference on spatially non-uniform degradations.\nBuilding on this insight, we propose a robust blind denoising framework\nequipped with two key components: a Heterogeneous Normalization Module (HNM)\nand an Interactive Gating Module (IGM). HNM stabilizes feature distributions\nand dynamically corrects features under varying noise intensities, while IGM\nfacilitates effective information modulation via gated interactions between\nsignal and feature paths. Extensive evaluations demonstrate that our model\nconsistently outperforms state-of-the-art methods on both synthetic and\nreal-world benchmarks, especially under spatially heterogeneous noise. Code\nwill be made publicly available.",
        "url": "http://arxiv.org/abs/2508.02967v1",
        "published_date": "2025-08-05T00:06:28+00:00",
        "updated_date": "2025-08-05T00:06:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dawei Zhang",
            "Xiaojie Guo"
        ],
        "tldr": "This paper proposes a scale-equivariant denoising framework with Heterogeneous Normalization and Interactive Gating modules to improve robustness against spatially variant noise, demonstrating state-of-the-art performance.",
        "tldr_zh": "本文提出了一种具有异构归一化和交互门控模块的尺度等变去噪框架，以提高对空间变异噪声的鲁棒性，并展示了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution",
        "summary": "The Maximum A Posteriori (MAP) estimation is a widely used framework in blind\ndeconvolution to recover sharp images from blurred observations. The estimated\nimage and blur filter are defined as the maximizer of the posterior\ndistribution. However, when paired with sparsity-promoting image priors, MAP\nestimation has been shown to favors blurry solutions, limiting its\neffectiveness. In this paper, we revisit this result using diffusion-based\npriors, a class of models that capture realistic image distributions. Through\nan empirical examination of the prior's likelihood landscape, we uncover two\nkey properties: first, blurry images tend to have higher likelihoods; second,\nthe landscape contains numerous local minimizers that correspond to natural\nimages. Building on these insights, we provide a theoretical analysis of the\nblind deblurring posterior. This reveals that the MAP estimator tends to\nproduce sharp filters (close to the Dirac delta function) and blurry solutions.\nHowever local minimizers of the posterior, which can be obtained with gradient\ndescent, correspond to realistic, natural images, effectively solving the blind\ndeconvolution problem. Our findings suggest that overcoming MAP's limitations\nrequires good local initialization to local minima in the posterior landscape.\nWe validate our analysis with numerical experiments, demonstrating the\npractical implications of our insights for designing improved priors and\noptimization techniques.",
        "url": "http://arxiv.org/abs/2508.02923v1",
        "published_date": "2025-08-04T21:53:12+00:00",
        "updated_date": "2025-08-04T21:53:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minh-Hai Nguyen",
            "Edouard Pauwels",
            "Pierre Weiss"
        ],
        "tldr": "This paper analyzes why MAP estimation with diffusion priors favors blurry solutions in blind deconvolution, showing that local minimizers of the posterior correspond to realistic images, suggesting improved initialization strategies.",
        "tldr_zh": "该论文分析了在使用扩散先验的盲反卷积中，最大后验估计（MAP）倾向于模糊解的原因，表明后验的局部最小值对应于真实的图像，并提出了改进的初始化策略。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy",
        "summary": "Miniaturized endoscopy has advanced accurate visual perception within the\nhuman body. Prevailing research remains limited to conventional cameras\nemploying convex lenses, where the physical constraints with millimetre-scale\nthickness impose serious impediments on the micro-level clinical. Recently,\nwith the emergence of meta-optics, ultra-micro imaging based on metalenses\n(micron-scale) has garnered great attention, serving as a promising solution.\nHowever, due to the physical difference of metalens, there is a large gap in\ndata acquisition and algorithm research. In light of this, we aim to bridge\nthis unexplored gap, advancing the novel metalens endoscopy. First, we\nestablish datasets for metalens endoscopy and conduct preliminary optical\nsimulation, identifying two derived optical issues that physically adhere to\nstrong optical priors. Second, we propose MetaScope, a novel optics-driven\nneural network tailored for metalens endoscopy driven by physical optics.\nMetaScope comprises two novel designs: Optics-informed Intensity Adjustment\n(OIA), rectifying intensity decay by learning optical embeddings, and\nOptics-informed Chromatic Correction (OCC), mitigating chromatic aberration by\nlearning spatial deformations informed by learned Point Spread Function (PSF)\ndistributions. To enhance joint learning, we further deploy a gradient-guided\ndistillation to transfer knowledge from the foundational model adaptively.\nExtensive experiments demonstrate that MetaScope not only outperforms\nstate-of-the-art methods in both metalens segmentation and restoration but also\nachieves impressive generalized ability in real biomedical scenes.",
        "url": "http://arxiv.org/abs/2508.03596v1",
        "published_date": "2025-08-05T16:01:00+00:00",
        "updated_date": "2025-08-05T16:01:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wuyang Li",
            "Wentao Pan",
            "Xiaoyuan Liu",
            "Zhendong Luo",
            "Chenxin Li",
            "Hengyu Liu",
            "Din Ping Tsai",
            "Mu Ku Chen",
            "Yixuan Yuan"
        ],
        "tldr": "The paper introduces MetaScope, an optics-driven neural network designed for metalens endoscopy, addressing optical issues like intensity decay and chromatic aberration by incorporating learned optical embeddings and PSF distributions.",
        "tldr_zh": "该论文介绍了一种名为 MetaScope 的光学驱动神经网络，专为金属透镜内窥镜设计，通过结合学习的光学嵌入和点扩散函数分布来解决强度衰减和色差等光学问题。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation",
        "summary": "Diffusion Transformers (DiTs) have achieved impressive performance in\ntext-to-image generation. However, their high computational cost and large\nparameter sizes pose significant challenges for usage in resource-constrained\nscenarios. Post-training quantization (PTQ) is a promising solution to reduce\nmemory usage and accelerate inference, but existing PTQ methods suffer from\nsevere performance degradation under extreme low-bit settings. We identify two\nkey obstacles to low-bit post-training quantization for DiT models: (1) model\nweights follow a Gaussian-like distribution with long tails, causing uniform\nquantization to poorly allocate intervals and leading to significant errors;\n(2) two types of activation outliers: (i) Mild Outliers with slightly elevated\nvalues, and (ii) Salient Outliers with large magnitudes concentrated in\nspecific channels, which disrupt activation quantization. To address these\nissues, we propose LRQ-DiT, an efficient and accurate PTQ framework. We\nintroduce Twin-Log Quantization (TLQ), a log-based method that aligns well with\nthe weight distribution and reduces quantization errors. We also propose an\nAdaptive Rotation Scheme (ARS) that dynamically applies Hadamard or\noutlier-aware rotations based on activation fluctuation, effectively mitigating\nthe impact of both types of outliers. We evaluate LRQ-DiT on PixArt and FLUX\nunder various bit-width settings, and validate the performance on COCO, MJHQ,\nand sDCI datasets. LRQ-DiT achieves low-bit quantization of DiT models while\npreserving image quality, outperforming existing PTQ baselines.",
        "url": "http://arxiv.org/abs/2508.03485v1",
        "published_date": "2025-08-05T14:16:11+00:00",
        "updated_date": "2025-08-05T14:16:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lianwei Yang",
            "Haokun Lin",
            "Tianchen Zhao",
            "Yichen Wu",
            "Hongyu Zhu",
            "Ruiqi Xie",
            "Zhenan Sun",
            "Yu Wang",
            "Qingyi Gu"
        ],
        "tldr": "The paper introduces LRQ-DiT, a post-training quantization method for Diffusion Transformers that addresses challenges in low-bit quantization by using twin-log quantization for weights and an adaptive rotation scheme for activations, achieving improved performance on text-to-image generation tasks.",
        "tldr_zh": "该论文介绍了LRQ-DiT，一种用于扩散Transformer的后训练量化方法，通过对权重使用双对数量化和对激活使用自适应旋转方案，解决了低比特量化中的挑战，并在文本到图像生成任务上取得了更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN",
        "summary": "This paper presents Fd-CycleGAN, an image-to-image (I2I) translation\nframework that enhances latent representation learning to approximate real data\ndistributions. Building upon the foundation of CycleGAN, our approach\nintegrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to\ncapture fine-grained local pixel semantics while preserving structural\ncoherence from the source domain. We employ distribution-based loss metrics,\nincluding KL/JS divergence and log-based similarity measures, to explicitly\nquantify the alignment between real and generated image distributions in both\nspatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we\nconduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a\nsynthetically augmented Strike-off dataset. Compared to baseline CycleGAN and\nother state-of-the-art methods, our approach demonstrates superior perceptual\nquality, faster convergence, and improved mode diversity, particularly in\nlow-data regimes. By effectively capturing local and global distribution\ncharacteristics, Fd-CycleGAN achieves more visually coherent and semantically\nconsistent translations. Our results suggest that frequency-guided latent\nlearning significantly improves generalization in image translation tasks, with\npromising applications in document restoration, artistic style transfer, and\nmedical image synthesis. We also provide comparative insights with\ndiffusion-based generative models, highlighting the advantages of our\nlightweight adversarial approach in terms of training efficiency and\nqualitative output.",
        "url": "http://arxiv.org/abs/2508.03415v1",
        "published_date": "2025-08-05T12:59:37+00:00",
        "updated_date": "2025-08-05T12:59:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "authors": [
            "Shivangi Nigam",
            "Adarsh Prasad Behera",
            "Shekhar Verma",
            "P. Nagabhushan"
        ],
        "tldr": "The paper introduces Fd-CycleGAN, an improved CycleGAN variant using frequency-aware supervision and distribution-based losses for enhanced image-to-image translation, demonstrating better perceptual quality and convergence.",
        "tldr_zh": "该论文提出了Fd-CycleGAN，一种改进的CycleGAN变体，它使用频率感知监督和基于分布的损失，以增强图像到图像的转换，从而展示了更好的感知质量和收敛性。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models",
        "summary": "Explicitly disentangling style and content in vision models remains\nchallenging due to their semantic overlap and the subjectivity of human\nperception. Existing methods propose separation through generative or\ndiscriminative objectives, but they still face the inherent ambiguity of\ndisentangling intertwined concepts. Instead, we ask: Can we bypass explicit\ndisentanglement by learning to merge style and content invertibly, allowing\nseparation to emerge naturally? We propose SCFlow, a flow-matching framework\nthat learns bidirectional mappings between entangled and disentangled\nrepresentations. Our approach is built upon three key insights: 1) Training\nsolely to merge style and content, a well-defined task, enables invertible\ndisentanglement without explicit supervision; 2) flow matching bridges on\narbitrary distributions, avoiding the restrictive Gaussian priors of diffusion\nmodels and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51\nstyles $\\times$ 10,000 content samples) was curated to simulate disentanglement\nthrough systematic style-content pairing. Beyond controllable generation tasks,\nwe demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot\nsettings and achieves competitive performance, highlighting that\ndisentanglement naturally emerges from the invertible merging process.",
        "url": "http://arxiv.org/abs/2508.03402v1",
        "published_date": "2025-08-05T12:50:46+00:00",
        "updated_date": "2025-08-05T12:50:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Pingchuan Ma",
            "Xiaopei Yang",
            "Yusong Li",
            "Ming Gui",
            "Felix Krause",
            "Johannes Schusterbauer",
            "Björn Ommer"
        ],
        "tldr": "The paper introduces SCFlow, a flow-matching framework for learning style and content disentanglement by focusing on invertible merging rather than explicit separation, achieving competitive performance in zero-shot settings on ImageNet-1k and WikiArt.",
        "tldr_zh": "本文介绍了一种名为SCFlow的流匹配框架，通过关注可逆合并而非显式分离来学习风格和内容解耦，并在ImageNet-1k和WikiArt上的零样本设置中实现了有竞争力的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GL-LCM: Global-Local Latent Consistency Models for Fast High-Resolution Bone Suppression in Chest X-Ray Images",
        "summary": "Chest X-Ray (CXR) imaging for pulmonary diagnosis raises significant\nchallenges, primarily because bone structures can obscure critical details\nnecessary for accurate diagnosis. Recent advances in deep learning,\nparticularly with diffusion models, offer significant promise for effectively\nminimizing the visibility of bone structures in CXR images, thereby improving\nclarity and diagnostic accuracy. Nevertheless, existing diffusion-based methods\nfor bone suppression in CXR imaging struggle to balance the complete\nsuppression of bones with preserving local texture details. Additionally, their\nhigh computational demand and extended processing time hinder their practical\nuse in clinical settings. To address these limitations, we introduce a\nGlobal-Local Latent Consistency Model (GL-LCM) architecture. This model\ncombines lung segmentation, dual-path sampling, and global-local fusion,\nenabling fast high-resolution bone suppression in CXR images. To tackle\npotential boundary artifacts and detail blurring in local-path sampling, we\nfurther propose Local-Enhanced Guidance, which addresses these issues without\nadditional training. Comprehensive experiments on a self-collected dataset\nSZCH-X-Rays, and the public dataset JSRT, reveal that our GL-LCM delivers\nsuperior bone suppression and remarkable computational efficiency,\nsignificantly outperforming several competitive methods. Our code is available\nat https://github.com/diaoquesang/GL-LCM.",
        "url": "http://arxiv.org/abs/2508.03357v1",
        "published_date": "2025-08-05T12:02:38+00:00",
        "updated_date": "2025-08-05T12:02:38+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Yifei Sun",
            "Zhanghao Chen",
            "Hao Zheng",
            "Yuqing Lu",
            "Lixin Duan",
            "Fenglei Fan",
            "Ahmed Elazab",
            "Xiang Wan",
            "Changmiao Wang",
            "Ruiquan Ge"
        ],
        "tldr": "The paper introduces GL-LCM, a Global-Local Latent Consistency Model for fast and high-resolution bone suppression in chest X-ray images, addressing limitations of existing diffusion-based methods in balancing bone suppression and detail preservation.",
        "tldr_zh": "该论文介绍了一种全局-局部潜在一致性模型(GL-LCM)，用于快速、高分辨率的胸部X光片骨骼抑制，解决了现有基于扩散的方法在骨骼抑制和细节保留之间平衡的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Investigation on deep learning-based galaxy image translation models",
        "summary": "Galaxy image translation is an important application in galaxy physics and\ncosmology. With deep learning-based generative models, image translation has\nbeen performed for image generation, data quality enhancement, information\nextraction, and generalized for other tasks such as deblending and anomaly\ndetection. However, most endeavors on image translation primarily focus on the\npixel-level and morphology-level statistics of galaxy images. There is a lack\nof discussion on the preservation of complex high-order galaxy physical\ninformation, which would be more challenging but crucial for studies that rely\non high-fidelity image translation. Therefore, we investigated the\neffectiveness of generative models in preserving high-order physical\ninformation (represented by spectroscopic redshift) along with pixel-level and\nmorphology-level information. We tested four representative models, i.e. a Swin\nTransformer, an SRGAN, a capsule network, and a diffusion model, using the SDSS\nand CFHTLS galaxy images. We found that these models show different levels of\nincapabilities in retaining redshift information, even if the global structures\nof galaxies and morphology-level statistics can be roughly reproduced. In\nparticular, the cross-band peak fluxes of galaxies were found to contain\nmeaningful redshift information, whereas they are subject to noticeable\nuncertainties in the translation of images, which may substantially be due to\nthe nature of many-to-many mapping. Nonetheless, imperfect translated images\nmay still contain a considerable amount of information and thus hold promise\nfor downstream applications for which high image fidelity is not strongly\nrequired. Our work can facilitate further research on how complex physical\ninformation is manifested on galaxy images, and it provides implications on the\ndevelopment of image translation models for scientific use.",
        "url": "http://arxiv.org/abs/2508.03291v1",
        "published_date": "2025-08-05T10:08:26+00:00",
        "updated_date": "2025-08-05T10:08:26+00:00",
        "categories": [
            "astro-ph.IM",
            "astro-ph.GA",
            "cs.CV"
        ],
        "authors": [
            "Hengxin Ruan",
            "Qiufan Lin",
            "Shupei Chen",
            "Yang Wang",
            "Wei Zhang"
        ],
        "tldr": "This paper investigates the ability of deep learning-based image translation models to preserve high-order physical information (redshift) in galaxy images, finding limitations in current models despite good morphology preservation.",
        "tldr_zh": "本文研究了基于深度学习的图像翻译模型在星系图像中保留高阶物理信息（红移）的能力，发现当前模型在这方面存在局限性，尽管形态学上的保留效果良好。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation",
        "summary": "Existing handwritten text generation methods primarily focus on isolated\nwords. However, realistic handwritten text demands attention not only to\nindividual words but also to the relationships between them, such as vertical\nalignment and horizontal spacing. Therefore, generating entire text lines\nemerges as a more promising and comprehensive task. However, this task poses\nsignificant challenges, including the accurate modeling of complex style\npatterns encompassing both intra- and inter-word relationships, and maintaining\ncontent accuracy across numerous characters. To address these challenges, we\npropose DiffBrush, a novel diffusion-based model for handwritten text-line\ngeneration. Unlike existing methods, DiffBrush excels in both style imitation\nand content accuracy through two key strategies: (1) content-decoupled style\nlearning, which disentangles style from content to better capture intra-word\nand inter-word style patterns by using column- and row-wise masking; and (2)\nmulti-scale content learning, which employs line and word discriminators to\nensure global coherence and local accuracy of textual content. Extensive\nexperiments show that DiffBrush excels in generating high-quality text lines,\nparticularly in style reproduction and content preservation. Code is available\nat https://github.com/dailenson/DiffBrush.",
        "url": "http://arxiv.org/abs/2508.03256v1",
        "published_date": "2025-08-05T09:34:06+00:00",
        "updated_date": "2025-08-05T09:34:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gang Dai",
            "Yifan Zhang",
            "Yutao Qin",
            "Qiangya Guo",
            "Shuangping Huang",
            "Shuicheng Yan"
        ],
        "tldr": "The paper introduces DiffBrush, a diffusion-based model for generating handwritten text lines that improves upon existing methods by focusing on both intra- and inter-word relationships, leading to better style imitation and content accuracy.",
        "tldr_zh": "该论文介绍了一种基于扩散模型的手写文本行生成方法DiffBrush，通过关注词内和词间的关系，改进了现有方法，从而实现更好的风格模仿和内容准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance",
        "summary": "Synthesizing realistic and spatially precise anomalies is essential for\nenhancing the robustness of industrial anomaly detection systems. While recent\ndiffusion-based methods have demonstrated strong capabilities in modeling\ncomplex defect patterns, they often struggle with spatial controllability and\nfail to maintain fine-grained regional fidelity. To overcome these limitations,\nwe propose SARD (Segmentation-Aware anomaly synthesis via Region-constrained\nDiffusion with discriminative mask Guidance), a novel diffusion-based framework\nspecifically designed for anomaly generation. Our approach introduces a\nRegion-Constrained Diffusion (RCD) process that preserves the background by\nfreezing it and selectively updating only the foreground anomaly regions during\nthe reverse denoising phase, thereby effectively reducing background artifacts.\nAdditionally, we incorporate a Discriminative Mask Guidance (DMG) module into\nthe discriminator, enabling joint evaluation of both global realism and local\nanomaly fidelity, guided by pixel-level masks. Extensive experiments on the\nMVTec-AD and BTAD datasets show that SARD surpasses existing methods in\nsegmentation accuracy and visual quality, setting a new state-of-the-art for\npixel-level anomaly synthesis.",
        "url": "http://arxiv.org/abs/2508.03143v1",
        "published_date": "2025-08-05T06:43:01+00:00",
        "updated_date": "2025-08-05T06:43:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanshu Wang",
            "Xichen Xu",
            "Xiaoning Lei",
            "Guoyang Xie"
        ],
        "tldr": "The paper introduces SARD, a novel diffusion-based anomaly synthesis framework that improves spatial controllability and regional fidelity through region-constrained diffusion and discriminative mask guidance, achieving state-of-the-art results on anomaly detection datasets.",
        "tldr_zh": "该论文介绍了一种名为SARD的新型基于扩散的异常合成框架，通过区域约束扩散和判别掩码指导，提高了空间可控性和区域保真度，并在异常检测数据集上取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying",
        "summary": "In recent years, unified vision-language models (VLMs) have rapidly advanced,\neffectively tackling both visual understanding and generation tasks within a\nsingle design. While many unified VLMs have explored various design choices,\nthe recent hypothesis from OpenAI's GPT-4o suggests a promising generation\npipeline: Understanding VLM->Visual Feature->Projector->Diffusion Model->Image.\nThe understanding VLM is frozen, and only the generation-related modules are\ntrained. This pipeline maintains the strong capability of understanding VLM\nwhile enabling the image generation ability of the unified VLM. Although this\npipeline has shown very promising potential for the future development of\nunified VLM, how to easily enable image editing capability is still unexplored.\nIn this paper, we introduce a novel training-free framework named UniEdit-I to\nenable the unified VLM with image editing capability via three iterative steps:\nunderstanding, editing, and verifying. 1. The understanding step analyzes the\nsource image to create a source prompt through structured semantic analysis and\nmakes minimal word replacements to form the target prompt based on the editing\ninstruction. 2. The editing step introduces a time-adaptive offset, allowing\nfor coherent editing from coarse to fine throughout the denoising process. 3.\nThe verification step checks the alignment between the target prompt and the\nintermediate edited image, provides automatic consistency scores and corrective\nfeedback, and determines whether to stop early or continue the editing loop.\nThis understanding, editing, and verifying loop iterates until convergence,\ndelivering high-fidelity editing in a training-free manner. We implemented our\nmethod based on the latest BLIP3-o and achieved state-of-the-art (SOTA)\nperformance on the GEdit-Bench benchmark.",
        "url": "http://arxiv.org/abs/2508.03142v1",
        "published_date": "2025-08-05T06:42:09+00:00",
        "updated_date": "2025-08-05T06:42:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengyu Bai",
            "Jintao Chen",
            "Xiang Bai",
            "Yilong Chen",
            "Qi She",
            "Ming Lu",
            "Shanghang Zhang"
        ],
        "tldr": "The paper introduces UniEdit-I, a training-free image editing framework for unified vision-language models that iteratively understands, edits, and verifies images based on text prompts, achieving state-of-the-art results on the GEdit-Bench benchmark using BLIP3-o.",
        "tldr_zh": "该论文介绍了 UniEdit-I，一个无需训练的图像编辑框架，用于统一的视觉语言模型，它通过迭代地理解、编辑和验证图像，基于文本提示实现图像编辑，并使用 BLIP3-o 在 GEdit-Bench 基准测试上取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Evaluation of 3D Counterfactual Brain MRI Generation",
        "summary": "Counterfactual generation offers a principled framework for simulating\nhypothetical changes in medical imaging, with potential applications in\nunderstanding disease mechanisms and generating physiologically plausible data.\nHowever, generating realistic structural 3D brain MRIs that respect anatomical\nand causal constraints remains challenging due to data scarcity, structural\ncomplexity, and the lack of standardized evaluation protocols. In this work, we\nconvert six generative models into 3D counterfactual approaches by\nincorporating an anatomy-guided framework based on a causal graph, in which\nregional brain volumes serve as direct conditioning inputs. Each model is\nevaluated with respect to composition, reversibility, realism, effectiveness\nand minimality on T1-weighted brain MRIs (T1w MRIs) from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI). In addition, we test the\ngeneralizability of each model with respect to T1w MRIs of the National\nConsortium on Alcohol and Neurodevelopment in Adolescence (NCANDA). Our results\nindicate that anatomically grounded conditioning successfully modifies the\ntargeted anatomical regions; however, it exhibits limitations in preserving\nnon-targeted structures. Beyond laying the groundwork for more interpretable\nand clinically relevant generative modeling of brain MRIs, this benchmark\nhighlights the need for novel architectures that more accurately capture\nanatomical interdependencies.",
        "url": "http://arxiv.org/abs/2508.02880v1",
        "published_date": "2025-08-04T20:20:59+00:00",
        "updated_date": "2025-08-04T20:20:59+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Pengwei Sun",
            "Wei Peng",
            "Lun Yu Li",
            "Yixin Wang",
            "Kilian M. Pohl"
        ],
        "tldr": "This paper evaluates several generative models for counterfactual 3D brain MRI generation, focusing on anatomical accuracy and generalizability using an anatomy-guided framework. The models show promise in modifying targeted regions but struggle with preserving non-targeted structures.",
        "tldr_zh": "本文评估了几种用于反事实 3D 脑部 MRI 生成的生成模型，重点是使用解剖引导框架的解剖学精度和泛化能力。 这些模型在修改目标区域方面显示出希望，但在保留非目标结构方面遇到了困难。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation",
        "summary": "Deep learning-based semantic segmentation models achieve impressive results\nyet remain limited in handling distribution shifts between training and test\ndata. In this paper, we present SDGPA (Synthetic Data Generation and\nProgressive Adaptation), a novel method that tackles zero-shot domain adaptive\nsemantic segmentation, in which no target images are available, but only a text\ndescription of the target domain's style is provided. To compensate for the\nlack of target domain training data, we utilize a pretrained off-the-shelf\ntext-to-image diffusion model, which generates training images by transferring\nsource domain images to target style. Directly editing source domain images\nintroduces noise that harms segmentation because the layout of source images\ncannot be precisely maintained. To address inaccurate layouts in synthetic\ndata, we propose a method that crops the source image, edits small patches\nindividually, and then merges them back together, which helps improve spatial\nprecision. Recognizing the large domain gap, SDGPA constructs an augmented\nintermediate domain, leveraging easier adaptation subtasks to enable more\nstable model adaptation to the target domain. Additionally, to mitigate the\nimpact of noise in synthetic data, we design a progressive adaptation strategy,\nensuring robust learning throughout the training process. Extensive experiments\ndemonstrate that our method achieves state-of-the-art performance in zero-shot\nsemantic segmentation. The code is available at\nhttps://github.com/ROUJINN/SDGPA",
        "url": "http://arxiv.org/abs/2508.03300v1",
        "published_date": "2025-08-05T10:21:09+00:00",
        "updated_date": "2025-08-05T10:21:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jun Luo",
            "Zijing Zhao",
            "Yang Liu"
        ],
        "tldr": "This paper introduces SDGPA, a novel method for zero-shot domain adaptive semantic segmentation using synthetic data generation and progressive adaptation to address distribution shifts when target domain images are unavailable. It leverages text-to-image diffusion models and a patch-based editing strategy to mitigate noise and improve spatial precision.",
        "tldr_zh": "本文介绍了一种名为SDGPA的新型零样本域自适应语义分割方法，该方法利用合成数据生成和渐进式自适应来解决在目标域图像不可用时分布偏移的问题。它利用文本到图像的扩散模型和基于补丁的编辑策略来减轻噪声并提高空间精度。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]