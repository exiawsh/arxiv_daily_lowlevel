[
    {
        "title": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching",
        "summary": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.",
        "url": "http://arxiv.org/abs/2512.12604v1",
        "published_date": "2025-12-14T09:02:18+00:00",
        "updated_date": "2025-12-14T09:02:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tingyan Wen",
            "Haoyu Li",
            "Yihuang Chen",
            "Xing Zhou",
            "Lifei Zhu",
            "Xueqian Wang"
        ],
        "tldr": "The paper introduces X-Slim, a novel caching framework for accelerating diffusion models by exploiting redundancy across timesteps, blocks, and tokens, achieving significant speedups with minimal fidelity loss and even FID improvements in some cases.",
        "tldr_zh": "该论文介绍了一种名为 X-Slim 的新型缓存框架，通过利用时间步、块和 token 之间的冗余来加速扩散模型，在保证极小保真度损失的情况下实现了显著的加速，并且在某些情况下甚至提高了 FID。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
        "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
        "url": "http://arxiv.org/abs/2512.12751v1",
        "published_date": "2025-12-14T16:23:51+00:00",
        "updated_date": "2025-12-14T16:23:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenya Yang",
            "Zhe Liu",
            "Yuxiang Lu",
            "Liping Hou",
            "Chenxuan Miao",
            "Siyi Peng",
            "Bailan Feng",
            "Xiang Bai",
            "Hengshuang Zhao"
        ],
        "tldr": "GenieDrive is a physics-aware driving video generation framework using 4D occupancy and a VAE with Mutual Control Attention for improved forecasting accuracy and video quality, achieving better performance with fewer parameters.",
        "tldr_zh": "GenieDrive是一个物理感知的驾驶视频生成框架，它使用4D占用和带有互控注意力的VAE，以提高预测精度和视频质量，并以更少的参数实现了更好的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]