[
    {
        "title": "Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model",
        "summary": "Mammography is the most commonly used imaging modality for breast cancer\nscreening, driving an increasing demand for deep-learning techniques to support\nlarge-scale analysis. However, the development of accurate and robust methods\nis often limited by insufficient data availability and a lack of diversity in\nlesion characteristics. While generative models offer a promising solution for\ndata synthesis, current approaches often fail to adequately emphasize\nlesion-specific features and their relationships with surrounding tissues. In\nthis paper, we propose Gated Conditional Diffusion Model (GCDM), a novel\nframework designed to jointly synthesize holistic mammogram images and\nlocalized lesions. GCDM is built upon a latent denoising diffusion framework,\nwhere the noised latent image is concatenated with a soft mask embedding that\nrepresents breast, lesion, and their transitional regions, ensuring anatomical\ncoherence between them during the denoising process. To further emphasize\nlesion-specific features, GCDM incorporates a gated conditioning branch that\nguides the denoising process by dynamically selecting and fusing the most\nrelevant radiomic and geometric properties of lesions, effectively capturing\ntheir interplay. Experimental results demonstrate that GCDM achieves precise\ncontrol over small lesion areas while enhancing the realism and diversity of\nsynthesized mammograms. These advancements position GCDM as a promising tool\nfor clinical applications in mammogram synthesis. Our code is available at\nhttps://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/",
        "url": "http://arxiv.org/abs/2507.19201v1",
        "published_date": "2025-07-25T12:10:45+00:00",
        "updated_date": "2025-07-25T12:10:45+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xin Li",
            "Kaixiang Yang",
            "Qiang Li",
            "Zhiwei Wang"
        ],
        "tldr": "This paper introduces a Gated Conditional Diffusion Model (GCDM) for synthesizing realistic mammograms with controllable lesions, addressing the data scarcity and diversity challenges in breast cancer screening research.",
        "tldr_zh": "该论文介绍了一种门控条件扩散模型（GCDM），用于合成具有可控病灶的逼真乳房X光片，解决了乳腺癌筛查研究中数据稀缺和多样性挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI",
        "summary": "In medical imaging, generative models are increasingly relied upon for two\ndistinct but equally critical tasks: reconstruction, where the goal is to\nrestore medical imaging (usually inverse problems like inpainting or\nsuperresolution), and generation, where synthetic data is created to augment\ndatasets or carry out counterfactual analysis. Despite shared architecture and\nlearning frameworks, they prioritize different goals: generation seeks high\nperceptual quality and diversity, while reconstruction focuses on data fidelity\nand faithfulness. In this work, we introduce a \"generative model zoo\" and\nsystematically analyze how modern latent diffusion models and autoregressive\nmodels navigate the reconstruction-generation spectrum. We benchmark a suite of\ngenerative models across representative cardiac medical imaging tasks, focusing\non image inpainting with varying masking ratios and sampling strategies, as\nwell as unconditional image generation. Our findings show that diffusion models\noffer superior perceptual quality for unconditional generation but tend to\nhallucinate as masking ratios increase, whereas autoregressive models maintain\nstable perceptual performance across masking levels, albeit with generally\nlower fidelity.",
        "url": "http://arxiv.org/abs/2507.19186v1",
        "published_date": "2025-07-25T11:53:50+00:00",
        "updated_date": "2025-07-25T11:53:50+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Niklas Bubeck",
            "Yundi Zhang",
            "Suprosanna Shit",
            "Daniel Rueckert",
            "Jiazhen Pan"
        ],
        "tldr": "This paper benchmarks various generative models, including diffusion and autoregressive models, on cardiac MRI tasks, exploring the trade-offs between reconstruction fidelity and generation quality, particularly in inpainting and unconditional generation scenarios.",
        "tldr_zh": "本文针对心脏MRI任务，对包括扩散模型和自回归模型在内的各种生成模型进行了基准测试，探讨了重建保真度和生成质量之间的权衡，特别是在图像修复和无条件生成场景中。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks",
        "summary": "Restoration of images contaminated by different adverse weather conditions\nsuch as fog, snow, and rain is a challenging task due to the varying nature of\nthe weather conditions. Most of the existing methods focus on any one\nparticular weather conditions. However, for applications such as autonomous\ndriving, a unified model is necessary to perform restoration of corrupted\nimages due to different weather conditions. We propose a continual learning\napproach to propose a unified framework for image restoration. The proposed\nframework integrates three key innovations: (1) Selective Kernel Fusion layers\nthat dynamically combine global and local features for robust adaptive feature\nselection; (2) Elastic Weight Consolidation (EWC) to enable continual learning\nand mitigate catastrophic forgetting across multiple restoration tasks; and (3)\na novel Cycle-Contrastive Loss that enhances feature discrimination while\npreserving semantic consistency during domain translation. Further, we propose\nan unpaired image restoration approach to reduce the dependance of the proposed\napproach on the training data. Extensive experiments on standard benchmark\ndatasets for dehazing, desnowing and deraining tasks demonstrate significant\nimprovements in PSNR, SSIM, and perceptual quality over the state-of-the-art.",
        "url": "http://arxiv.org/abs/2507.19184v1",
        "published_date": "2025-07-25T11:47:40+00:00",
        "updated_date": "2025-07-25T11:47:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kotha Kartheek",
            "Lingamaneni Gnanesh Chowdary",
            "Snehasis Mukherjee"
        ],
        "tldr": "This paper introduces a continual learning-based unified model for unpaired image restoration across various weather conditions, using selective kernel fusion, elastic weight consolidation, and a cycle-contrastive loss.",
        "tldr_zh": "本文提出了一种基于持续学习的统一模型，用于在各种天气条件下进行非配对图像恢复，使用了选择性内核融合、弹性权重巩固和循环对比损失。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RealisVSR: Detail-enhanced Diffusion for Real-World 4K Video Super-Resolution",
        "summary": "Video Super-Resolution (VSR) has achieved significant progress through\ndiffusion models, effectively addressing the over-smoothing issues inherent in\nGAN-based methods. Despite recent advances, three critical challenges persist\nin VSR community: 1) Inconsistent modeling of temporal dynamics in foundational\nmodels; 2) limited high-frequency detail recovery under complex real-world\ndegradations; and 3) insufficient evaluation of detail enhancement and 4K\nsuper-resolution, as current methods primarily rely on 720P datasets with\ninadequate details. To address these challenges, we propose RealisVSR, a\nhigh-frequency detail-enhanced video diffusion model with three core\ninnovations: 1) Consistency Preserved ControlNet (CPC) architecture integrated\nwith the Wan2.1 video diffusion to model the smooth and complex motions and\nsuppress artifacts; 2) High-Frequency Rectified Diffusion Loss (HR-Loss)\ncombining wavelet decomposition and HOG feature constraints for texture\nrestoration; 3) RealisVideo-4K, the first public 4K VSR benchmark containing\n1,000 high-definition video-text pairs. Leveraging the advanced spatio-temporal\nguidance of Wan2.1, our method requires only 5-25% of the training data volume\ncompared to existing approaches. Extensive experiments on VSR benchmarks (REDS,\nSPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P) demonstrate our\nsuperiority, particularly in ultra-high-resolution scenarios.",
        "url": "http://arxiv.org/abs/2507.19138v1",
        "published_date": "2025-07-25T10:18:33+00:00",
        "updated_date": "2025-07-25T10:18:33+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Weisong Zhao",
            "Jingkai Zhou",
            "Xiangyu Zhu",
            "Weihua Chen",
            "Xiao-Yu Zhang",
            "Zhen Lei",
            "Fan Wang"
        ],
        "tldr": "The paper introduces RealisVSR, a diffusion-based video super-resolution method enhanced with a novel architecture (Consistency Preserved ControlNet), a high-frequency rectified diffusion loss, and a new 4K VSR benchmark dataset, demonstrating superior performance particularly in high-resolution scenarios.",
        "tldr_zh": "该论文介绍了RealisVSR，一种基于扩散的视频超分辨率方法，它通过新颖的架构（一致性保留控制网络）、高频校正扩散损失和新的4K VSR基准数据集增强，展示了优越的性能，尤其是在高分辨率场景中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment",
        "summary": "Contemporary image generation systems have achieved high fidelity and\nsuperior aesthetic quality beyond basic text-image alignment. However, existing\nevaluation frameworks have failed to evolve in parallel. This study reveals\nthat human preference reward models fine-tuned based on CLIP and BLIP\narchitectures have inherent flaws: they inappropriately assign low scores to\nimages with rich details and high aesthetic value, creating a significant\ndiscrepancy with actual human aesthetic preferences. To address this issue, we\ndesign a novel evaluation score, ICT (Image-Contained-Text) score, that\nachieves and surpasses the objectives of text-image alignment by assessing the\ndegree to which images represent textual content. Building upon this\nfoundation, we further train an HP (High-Preference) score model using solely\nthe image modality to enhance image aesthetics and detail quality while\nmaintaining text-image alignment. Experiments demonstrate that the proposed\nevaluation model improves scoring accuracy by over 10\\% compared to existing\nmethods, and achieves significant results in optimizing state-of-the-art\ntext-to-image models. This research provides theoretical and empirical support\nfor evolving image generation technology toward higher-order human aesthetic\npreferences. Code is available at https://github.com/BarretBa/ICTHP.",
        "url": "http://arxiv.org/abs/2507.19002v1",
        "published_date": "2025-07-25T07:01:50+00:00",
        "updated_date": "2025-07-25T07:01:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying Ba",
            "Tianyu Zhang",
            "Yalong Bai",
            "Wenyi Mo",
            "Tao Liang",
            "Bing Su",
            "Ji-Rong Wen"
        ],
        "tldr": "This paper introduces a new evaluation score (ICT) and reward model (HP) for image generation that better aligns with human aesthetic preferences by addressing the shortcomings of existing text-image alignment-based metrics. They show improved scoring accuracy and optimization of text-to-image models.",
        "tldr_zh": "本文介绍了一种新的图像生成评估分数 (ICT) 和奖励模型 (HP)，通过解决现有基于文本-图像对齐的指标的缺点，更好地与人类的审美偏好对齐。他们展示了改进的评分准确性和文本到图像模型的优化。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution",
        "summary": "Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and\nsparse textures of infrared data, requiring robust long-range modeling to\nmaintain global coherence. While State-Space Models like Mamba offer\nproficiency in modeling long-range dependencies for this task, their inherent\n1D causal scanning mechanism fragments the global context of 2D images,\nhindering fine-detail restoration. To address this, we propose Global Phase and\nSpectral Prompt-guided Mamba (GPSMamba), a framework that synergizes\narchitectural guidance with non-causal supervision. First, our Adaptive\nSemantic-Frequency State Space Module (ASF-SSM) injects a fused\nsemantic-frequency prompt directly into the Mamba block, integrating non-local\ncontext to guide reconstruction. Then, a novel Thermal-Spectral Attention and\nPhase Consistency Loss provides explicit, non-causal supervision to enforce\nglobal structural and spectral fidelity. By combining these two innovations,\nour work presents a systematic strategy to mitigate the limitations of causal\nmodeling. Extensive experiments demonstrate that GPSMamba achieves\nstate-of-the-art performance, validating our approach as a powerful new\nparadigm for infrared image restoration. Code is available at\nhttps://github.com/yongsongH/GPSMamba.",
        "url": "http://arxiv.org/abs/2507.18998v1",
        "published_date": "2025-07-25T06:56:16+00:00",
        "updated_date": "2025-07-25T06:56:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongsong Huang",
            "Tomo Miyazaki",
            "Xiaofeng Liu",
            "Shinichiro Omachi"
        ],
        "tldr": "The paper introduces GPSMamba, a novel Mamba-based architecture for infrared image super-resolution that leverages global phase and spectral information through a fused semantic-frequency prompt and non-causal supervision to overcome the limitations of causal scanning in standard Mamba.",
        "tldr_zh": "该论文介绍了GPSMamba，一种新颖的基于Mamba的红外图像超分辨率架构，它利用全局相位和频谱信息，通过融合的语义-频率提示和非因果监督来克服标准Mamba中因果扫描的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models",
        "summary": "We propose image-to-image diffusion models that are designed to enhance the\nrealism and details of generated brain images by introducing sharp edges, fine\ntextures, subtle anatomical features, and imaging noise. Generative models have\nbeen widely adopted in the biomedical domain, especially in image generation\napplications. Latent diffusion models achieve state-of-the-art results in\ngenerating brain MRIs. However, due to latent compression, generated images\nfrom these models are overly smooth, lacking fine anatomical structures and\nscan acquisition noise that are typically seen in real images. This work\nformulates the realism enhancing and detail adding process as image-to-image\ndiffusion models, which refines the quality of LDM-generated images. We employ\ncommonly used metrics like FID and LPIPS for image realism assessment.\nFurthermore, we introduce new metrics to demonstrate the realism of images\ngenerated by RealDeal in terms of image noise distribution, sharpness, and\ntexture.",
        "url": "http://arxiv.org/abs/2507.18830v1",
        "published_date": "2025-07-24T22:04:39+00:00",
        "updated_date": "2025-07-24T22:04:39+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shen Zhu",
            "Yinzhu Jin",
            "Tyler Spears",
            "Ifrah Zawar",
            "P. Thomas Fletcher"
        ],
        "tldr": "The paper introduces RealDeal, an image-to-image diffusion model designed to enhance the realism of generated brain MRIs by adding finer details and realistic noise, addressing the over-smoothness issue of latent diffusion models.",
        "tldr_zh": "该论文介绍了RealDeal，一种图像到图像的扩散模型，旨在通过添加更精细的细节和真实的噪声来增强生成的脑部MRI的真实感，解决了潜在扩散模型过度平滑的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Event-Based De-Snowing for Autonomous Driving",
        "summary": "Adverse weather conditions, particularly heavy snowfall, pose significant\nchallenges to both human drivers and autonomous vehicles. Traditional\nimage-based de-snowing methods often introduce hallucination artifacts as they\nrely solely on spatial information, while video-based approaches require high\nframe rates and suffer from alignment artifacts at lower frame rates. Camera\nparameters, such as exposure time, also influence the appearance of snowflakes,\nmaking the problem difficult to solve and heavily dependent on network\ngeneralization. In this paper, we propose to address the challenge of desnowing\nby using event cameras, which offer compressed visual information with\nsubmillisecond latency, making them ideal for de-snowing images, even in the\npresence of ego-motion. Our method leverages the fact that snowflake occlusions\nappear with a very distinctive streak signature in the spatio-temporal\nrepresentation of event data. We design an attention-based module that focuses\non events along these streaks to determine when a background point was occluded\nand use this information to recover its original intensity. We benchmark our\nmethod on DSEC-Snow, a new dataset created using a green-screen technique that\noverlays pre-recorded snowfall data onto the existing DSEC driving dataset,\nresulting in precise ground truth and synchronized image and event streams. Our\napproach outperforms state-of-the-art de-snowing methods by 3 dB in PSNR for\nimage reconstruction. Moreover, we show that off-the-shelf computer vision\nalgorithms can be applied to our reconstructions for tasks such as depth\nestimation and optical flow, achieving a $20\\%$ performance improvement over\nother de-snowing methods. Our work represents a crucial step towards enhancing\nthe reliability and safety of vision systems in challenging winter conditions,\npaving the way for more robust, all-weather-capable applications.",
        "url": "http://arxiv.org/abs/2507.20901v1",
        "published_date": "2025-07-25T17:48:28+00:00",
        "updated_date": "2025-07-25T17:48:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manasi Muglikar",
            "Nico Messikommer",
            "Marco Cannici",
            "Davide Scaramuzza"
        ],
        "tldr": "This paper introduces an event-based de-snowing method for autonomous driving using a novel attention-based module to remove snowflake occlusions, achieving state-of-the-art performance and improving downstream tasks.",
        "tldr_zh": "本文提出了一种基于事件的自动驾驶除雪方法，该方法使用一种新的基于注意力的模块来去除雪花遮挡，实现了最先进的性能，并改善了下游任务。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Cross-Subject Mind Decoding from Inaccurate Representations",
        "summary": "Decoding stimulus images from fMRI signals has advanced with pre-trained\ngenerative models. However, existing methods struggle with cross-subject\nmappings due to cognitive variability and subject-specific differences. This\nchallenge arises from sequential errors, where unidirectional mappings generate\npartially inaccurate representations that, when fed into diffusion models,\naccumulate errors and degrade reconstruction fidelity. To address this, we\npropose the Bidirectional Autoencoder Intertwining framework for accurate\ndecoded representation prediction. Our approach unifies multiple subjects\nthrough a Subject Bias Modulation Module while leveraging bidirectional mapping\nto better capture data distributions for precise representation prediction. To\nfurther enhance fidelity when decoding representations into stimulus images, we\nintroduce a Semantic Refinement Module to improve semantic representations and\na Visual Coherence Module to mitigate the effects of inaccurate visual\nrepresentations. Integrated with ControlNet and Stable Diffusion, our method\noutperforms state-of-the-art approaches on benchmark datasets in both\nqualitative and quantitative evaluations. Moreover, our framework exhibits\nstrong adaptability to new subjects with minimal training samples.",
        "url": "http://arxiv.org/abs/2507.19071v1",
        "published_date": "2025-07-25T08:45:02+00:00",
        "updated_date": "2025-07-25T08:45:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yangyang Xu",
            "Bangzhen Liu",
            "Wenqi Shao",
            "Yong Du",
            "Shengfeng He",
            "Tingting Zhu"
        ],
        "tldr": "This paper introduces a Bidirectional Autoencoder Intertwining framework to improve cross-subject fMRI decoding into stimulus images, addressing the issue of error accumulation in existing unidirectional methods. The framework leverages bidirectional mapping, subject bias modulation, semantic refinement, and visual coherence modules, demonstrating superior performance and adaptability.",
        "tldr_zh": "本文提出了一种双向自编码器交织框架，以改善将跨被试 fMRI 解码成刺激图像的效果，解决了现有单向方法中存在的误差累积问题。该框架利用双向映射、被试偏差调制、语义细化和视觉一致性模块，表现出卓越的性能和适应性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Dual Path Learning -- learning from noise and context for medical image denoising",
        "summary": "Medical imaging plays a critical role in modern healthcare, enabling\nclinicians to accurately diagnose diseases and develop effective treatment\nplans. However, noise, often introduced by imaging devices, can degrade image\nquality, leading to misinterpretation and compromised clinical outcomes.\nExisting denoising approaches typically rely either on noise characteristics or\non contextual information from the image. Moreover, they are commonly developed\nand evaluated for a single imaging modality and noise type. Motivated by Geng\net.al CNCL, which integrates both noise and context, this study introduces a\nDual-Pathway Learning (DPL) model architecture that effectively denoises\nmedical images by leveraging both sources of information and fusing them to\ngenerate the final output. DPL is evaluated across multiple imaging modalities\nand various types of noise, demonstrating its robustness and generalizability.\nDPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on\nGaussian noise and trained across all modalities. The code is available at\n10.5281/zenodo.15836053.",
        "url": "http://arxiv.org/abs/2507.19035v1",
        "published_date": "2025-07-25T07:43:50+00:00",
        "updated_date": "2025-07-25T07:43:50+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jitindra Fartiyal",
            "Pedro Freire",
            "Yasmeen Whayeb",
            "James S. Wolffsohn",
            "Sergei K. Turitsyn",
            "Sergei G. Sokolov"
        ],
        "tldr": "This paper introduces a Dual-Pathway Learning (DPL) model for medical image denoising that leverages both noise characteristics and contextual information, showing improved PSNR compared to UNet across multiple modalities and noise types.",
        "tldr_zh": "本文介绍了一种用于医学图像去噪的双路径学习 (DPL) 模型，该模型利用噪声特征和上下文信息，与 UNet 相比，在多种模式和噪声类型下显示出改进的 PSNR。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]