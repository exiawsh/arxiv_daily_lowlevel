[
    {
        "title": "CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing",
        "summary": "The processing mechanisms underlying language and image understanding in\nlarge vision-language models (LVLMs) have been extensively studied. However,\nthe internal reasoning mechanisms of LVLMs for spatiotemporal understanding\nremain poorly understood. In this work, we introduce a systematic,\ncircuit-based framework designed to investigate how spatiotemporal visual\nsemantics are represented and processed within these LVLMs. Specifically, our\nframework comprises three circuits: visual auditing circuit, semantic tracing\ncircuit, and attention flow circuit. Through the lens of these circuits, we\ndiscover that visual semantics are highly localized to specific object\ntokens--removing these tokens can degrade model performance by up to 92.6%.\nFurthermore, we identify that interpretable concepts of objects and actions\nemerge and become progressively refined in the middle-to-late layers of LVLMs.\nIn contrary to the current works that solely focus on objects in one image, we\nreveal that the middle-to-late layers of LVLMs exhibit specialized functional\nlocalization for spatiotemporal semantics. Our findings offer significant\nmechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a\nfoundation for designing more robust and interpretable models.",
        "url": "http://arxiv.org/abs/2507.19420v1",
        "published_date": "2025-07-25T16:38:18+00:00",
        "updated_date": "2025-07-25T16:38:18+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yiming Zhang",
            "Chengzhang Yu",
            "Zhuokai Zhao",
            "Kun Wang",
            "Qiankun Li",
            "Zihan Chen",
            "Yang Liu",
            "Zenghui Ding",
            "Yining Sun"
        ],
        "tldr": "This paper introduces a circuit-based framework to investigate spatiotemporal visual semantics in LVLMs, revealing functional localization in middle-to-late layers and the importance of specific object tokens.",
        "tldr_zh": "本文介绍了一个基于电路的框架，用于研究LVLM中的时空视觉语义，揭示了中后层的函数定位以及特定对象token的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences",
        "summary": "Large Vision-Language Models (LVLMs) have transformed image captioning,\nshifting from concise captions to detailed descriptions. We introduce LOTUS, a\nleaderboard for evaluating detailed captions, addressing three main gaps in\nexisting evaluations: lack of standardized criteria, bias-aware assessments,\nand user preference considerations. LOTUS comprehensively evaluates various\naspects, including caption quality (e.g., alignment, descriptiveness), risks\n(\\eg, hallucination), and societal biases (e.g., gender bias) while enabling\npreference-oriented evaluations by tailoring criteria to diverse user\npreferences. Our analysis of recent LVLMs reveals no single model excels across\nall criteria, while correlations emerge between caption detail and bias risks.\nPreference-oriented evaluations demonstrate that optimal model selection\ndepends on user priorities.",
        "url": "http://arxiv.org/abs/2507.19362v1",
        "published_date": "2025-07-25T15:12:42+00:00",
        "updated_date": "2025-07-25T15:12:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.CY",
            "cs.LG"
        ],
        "authors": [
            "Yusuke Hirota",
            "Boyi Li",
            "Ryo Hachiuma",
            "Yueh-Hua Wu",
            "Boris Ivanovic",
            "Yuta Nakashima",
            "Marco Pavone",
            "Yejin Choi",
            "Yu-Chiang Frank Wang",
            "Chao-Han Huck Yang"
        ],
        "tldr": "The paper introduces LOTUS, a new leaderboard for detailed image captioning that evaluates caption quality, risks, societal biases, and user preferences in Large Vision-Language Models (LVLMs). It reveals trade-offs between caption detail and bias risks and demonstrates the importance of user-centric evaluation.",
        "tldr_zh": "该论文介绍了一个名为LOTUS的新的详细图像描述排行榜，用于评估大型视觉语言模型(LVLMs)的描述质量、风险、社会偏见和用户偏好。研究表明，描述的细节与偏见风险之间存在权衡，并强调了以用户为中心的评估的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OVFact: Measuring and Improving Open-Vocabulary Factuality for Long Caption Models",
        "summary": "Large vision-language models (VLMs) often struggle to generate long and\nfactual captions. However, traditional measures for hallucination and\nfactuality are not well suited for evaluating longer, more diverse captions and\nin settings where ground-truth human-annotated captions are unavailable. We\nintroduce OV-Fact, a novel method for measuring caption factuality of long\ncaptions that leverages open-vocabulary visual grounding and tool-based\nverification without depending on human annotations. Our method improves\nagreement with human judgments and captures both caption descriptiveness\n(recall) and factual precision in the same metric. Furthermore, unlike previous\nmetrics, our reference-free method design enables new applications towards\nfactuality-based data filtering. We observe models trained on an\nOVFact-filtered (2.5-5x less) subset of a large-scale, noisy (VLM-generated)\npretraining set meaningfully improve factuality precision without sacrificing\ncaption descriptiveness across a range of downstream long caption benchmarks.",
        "url": "http://arxiv.org/abs/2507.19262v1",
        "published_date": "2025-07-25T13:38:06+00:00",
        "updated_date": "2025-07-25T13:38:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Monika Wysoczańska",
            "Shyamal Buch",
            "Anurag Arnab",
            "Cordelia Schmid"
        ],
        "tldr": "The paper introduces OV-Fact, a novel reference-free method for measuring and improving the factuality of long captions generated by VLMs, using open-vocabulary visual grounding and tool-based verification. It demonstrates improved factuality precision without sacrificing descriptiveness by filtering a noisy pretraining dataset.",
        "tldr_zh": "该论文介绍了一种名为OV-Fact的新颖的无需参考的方法，用于测量和提高VLM生成的长文本描述的真实性，该方法使用开放词汇的视觉定位和基于工具的验证。 它通过过滤嘈杂的预训练数据集，展示了在不牺牲描述性的情况下提高事实准确性的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for High-Resolution Multi-Attribute Point Prediction",
        "summary": "Visual selective attention, driven by individual preferences, regulates human\nprioritization of visual stimuli by bridging subjective cognitive mechanisms\nwith objective visual elements, thereby steering the semantic interpretation\nand hierarchical processing of dynamic visual scenes. However, existing models\nand datasets predominantly neglect the influence of subjective cognitive\ndiversity on fixation behavior. Conventional saliency prediction models,\ntypically employing segmentation approaches, rely on low-resolution imagery to\ngenerate saliency heatmaps, subsequently upscaled to native resolutions, which\nlimiting their capacity to capture personalized attention patterns.\nFurthermore, MLLMs are constrained by factors such as hallucinations, making it\nvery costly to strictly adhere to the expected format in tasks involving\nmultiple point predictions, and achieving precise point positioning is\nchallenging. To address these limitations, we present Subjective Personalized\nAttention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal\ndataset capturing gaze behaviors from over 4,500 participants varying in age\nand gender with 486 videos. Furthermore, we propose PRE-MAP, a novel\neye-tracking saliency model that characterizes Personalized visual disparities\nthrough Reinforcement learning-optimized Eye-tracking, built upon MLLMs and\nguided by Multi-Attribute user profiles to predict Points. To ensure MLLMs\nproduce prediction points that are both format-correct and spatially accurate,\nwe introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired\nby the variability in eye movement points and Multi-Attribute profiles.\nExtensive experiments on SPA-ADV and other benchmarks demonstrate the\neffectiveness of our approach. The code and dataset are available at\n\\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.",
        "url": "http://arxiv.org/abs/2507.19213v1",
        "published_date": "2025-07-25T12:32:29+00:00",
        "updated_date": "2025-07-25T12:32:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanbing Wu",
            "Ping Jiang",
            "Anyang Su",
            "Chenxu Zhao",
            "Tianyu Fu",
            "Minghui Wu",
            "Beiping Tan",
            "Huiying Li"
        ],
        "tldr": "The paper introduces PRE-MAP, a personalized eye-tracking saliency model leveraging reinforcement learning and MLLMs for high-resolution multi-attribute point prediction, along with a new large-scale multimodal dataset, SPA-ADV, for personalized attention in advertisement videos.",
        "tldr_zh": "该论文介绍了PRE-MAP，一种个性化的眼动追踪显著性模型，利用强化学习和多模态大型语言模型（MLLMs）进行高分辨率的多属性点预测。同时，论文还发布了一个新的大规模多模态数据集SPA-ADV，用于研究广告视频中的个性化注意力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?",
        "summary": "Computer-using agents have shown strong potential to boost human productivity\nand enable new application forms across platforms. While recent advances have\nled to usable applications, existing benchmarks fail to account for the\ninternal task heterogeneity and the corresponding agent capabilities, as well\nas their alignment with actual user demands-hindering both targeted capability\ndevelopment and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily\ncomputer-using automation that organizes its 416 realistic tasks across 15\napplications along two key dimensions: a five-level taxonomy of automation and\na generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with\nreal-world scenarios, OS-MAP evaluates agents along two dimensions: automation\nlevel across a five-level taxonomy, and generalization scope across a demand\nhierarchy. This design captures varying levels of required agent autonomy and\ngeneralization, forming a performance-generalization evaluation matrix for\nstructured and comprehensive assessment. Experiments show that even\nState-of-the-Art agents with VLM backbones struggle with higher-level tasks\ninvolving perception, reasoning, and coordination-highlighting the need for a\ndeeper understanding of current strengths and limitations to drive the future\nprogress in computer-using agents research and deployment. All code,\nenvironments, baselines, and data are publicly available at\nhttps://github.com/OS-Copilot/OS-Map.",
        "url": "http://arxiv.org/abs/2507.19132v1",
        "published_date": "2025-07-25T10:14:53+00:00",
        "updated_date": "2025-07-25T10:14:53+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Xuetian Chen",
            "Yinghao Chen",
            "Xinfeng Yuan",
            "Zhuo Peng",
            "Lu Chen",
            "Yuekeng Li",
            "Zhoujia Zhang",
            "Yingqian Huang",
            "Leyan Huang",
            "Jiaqing Liang",
            "Tianbao Xie",
            "Zhiyong Wu",
            "Qiushi Sun",
            "Biqing Qi",
            "Bowen Zhou"
        ],
        "tldr": "The paper introduces OS-MAP, a new benchmark for computer-using agents that evaluates their automation level and generalization scope across realistic daily tasks, revealing limitations in current state-of-the-art agents.",
        "tldr_zh": "该论文介绍了OS-MAP，一个新的计算机使用代理的基准测试，它评估了代理在现实日常任务中的自动化水平和泛化范围，揭示了当前最先进代理的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) excel in vision-language tasks such\nas image captioning but remain prone to object hallucinations, where they\ndescribe objects that do not appear in the image. To mitigate this, we propose\n\\textbf{LISA}, a \\textbf{L}ayer-wise \\textbf{I}ntegration and\n\\textbf{S}uppression \\textbf{A}pproach that enhances generation consistency\nthrough hierarchical modulation and multi-layer fusion. LISA leverages the\nfunctional hierarchy within MLLMs, where shallow layers provide visual\ngrounding, middle layers encode semantics, and deep layers tend to amplify\nspurious signals. First, zone-specific spectral modulation stabilizes attention\nby suppressing over-amplified activations in deeper layers while preserving\nalignment cues in earlier layers. Second, token-level logits from selected\nlayers are fused via anchor-based routing, with token-wise anchor selection and\nsoft logit fusion enabling adaptive integration during decoding. LISA is fully\n\\textbf{plug-and-play} and can be seamlessly integrated into existing MLLMs,\nincluding Qwen2.5-VL. Experiments on multiple benchmarks show that LISA reduces\nhallucinations by up to 53.6\\% in $\\mathrm{CHAIR}_I$ and improves POPE F1 by\n4.5\\%, demonstrating strong generalization across models and tasks.",
        "url": "http://arxiv.org/abs/2507.19110v1",
        "published_date": "2025-07-25T09:48:23+00:00",
        "updated_date": "2025-07-25T09:48:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhihui Guo",
            "Xin Man",
            "Hui Xu",
            "Jie Shao"
        ],
        "tldr": "The paper introduces LISA, a plug-and-play approach to mitigate object hallucinations in MLLMs by layer-wise integration and suppression, showing significant reduction in hallucination rates and improved performance on benchmarks.",
        "tldr_zh": "该论文介绍了LISA，一种即插即用的方法，通过分层集成和抑制来减轻多模态大型语言模型中的对象幻觉，并在基准测试中显示出幻觉率的显著降低和性能的提高。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Negation-Aware Test-Time Adaptation for Vision-Language Models",
        "summary": "In this paper, we study a practical but less-touched problem in\nVision-Language Models (VLMs), \\ie, negation understanding. Specifically, many\nreal-world applications require models to explicitly identify what is false or\nnon-existent, \\eg, radiologists may search for images that exclude specific\nconditions. Despite the impressive transferability of VLMs through large-scale\ntraining, they suffer from a critical limitation that fails to handle negation.\nTo address this challenge, existing methods attribute its root cause to the\nscarcity of negation training data and propose to fine-tune VLMs on massive\ndata containing explicit negation. Undoubtedly, such data-centric solutions\ndemand substantial data and computational resources, limiting their sustainable\nwidespread adoption. To tackle negation in a low-carbon manner, we empirically\nobserve that the key obstacle lies in the dual-concept shifts between the\naffirmation and negation distributions. Therefore, we propose a Negation-Aware\nTest-Time Adaptation (NEAT) method to efficiently adjust distribution-related\nparameters during inference. In brief, NEAT can reduce distribution shift in\nconsistent semantics while eliminating false distributional consistency in\nunrelated semantics. Extensive experiments on the various negation\nunderstanding tasks verify the effectiveness of the proposed method. The code\nis available at https://github.com/hhc1997/NEAT.",
        "url": "http://arxiv.org/abs/2507.19064v1",
        "published_date": "2025-07-25T08:25:48+00:00",
        "updated_date": "2025-07-25T08:25:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haochen Han",
            "Alex Jinpeng Wang",
            "Fangming Liu"
        ],
        "tldr": "This paper proposes a Negation-Aware Test-Time Adaptation (NEAT) method to improve the negation understanding ability of Vision-Language Models (VLMs) by addressing the distribution shifts between affirmation and negation during inference, requiring less data and computation.",
        "tldr_zh": "本文提出了一种 Negation-Aware Test-Time Adaptation (NEAT) 方法，通过解决推理过程中肯定和否定之间的分布偏移，从而提高视觉-语言模型 (VLM) 的否定理解能力，并且需要更少的数据和计算资源。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Closing the Modality Gap for Mixed Modality Search",
        "summary": "Mixed modality search -- retrieving information across a heterogeneous corpus\ncomposed of images, texts, and multimodal documents -- is an important yet\nunderexplored real-world application. In this work, we investigate how\ncontrastive vision-language models, such as CLIP, perform on the mixed modality\nsearch task. Our analysis reveals a critical limitation: these models exhibit a\npronounced modality gap in the embedding space, where image and text embeddings\nform distinct clusters, leading to intra-modal ranking bias and inter-modal\nfusion failure. To address this issue, we propose GR-CLIP, a lightweight\npost-hoc calibration method that removes the modality gap in CLIP's embedding\nspace. Evaluated on MixBench -- the first benchmark specifically designed for\nmixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points\nover CLIP, surpasses recent vision-language generative embedding models by 4\npercentage points, while using 75x less compute.",
        "url": "http://arxiv.org/abs/2507.19054v1",
        "published_date": "2025-07-25T08:15:28+00:00",
        "updated_date": "2025-07-25T08:15:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.IR",
            "cs.LG"
        ],
        "authors": [
            "Binxu Li",
            "Yuhui Zhang",
            "Xiaohan Wang",
            "Weixin Liang",
            "Ludwig Schmidt",
            "Serena Yeung-Levy"
        ],
        "tldr": "This paper identifies and addresses a modality gap in CLIP embeddings that hinders mixed modality search performance, proposing a post-hoc calibration method (GR-CLIP) that significantly improves search accuracy with minimal computational overhead.",
        "tldr_zh": "该论文发现并解决了CLIP嵌入中的模态差距，该差距阻碍了混合模态搜索的性能。论文提出了一种事后校准方法 (GR-CLIP)，该方法以极低的计算开销显著提高了搜索准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding",
        "summary": "Understanding another person's creative output requires a shared language of\nassociation. However, when training vision-language models such as CLIP, we\nrely on web-scraped datasets containing short, predominantly literal, alt-text.\nIn this work, we introduce a method for mining contextualized associations for\nsalient visual elements in an image that can scale to any unlabeled dataset.\nGiven an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we\nproduce a new dataset of visual associations and 1.7m creative captions for the\nimages in MSCOCO. Human evaluation confirms that these captions remain visually\ngrounded while exhibiting recognizably increasing abstraction. Moreover,\nfine-tuning a visual encoder on this dataset yields meaningful improvements in\nzero-shot image-text retrieval in two creative domains: poetry and metaphor\nvisualization. We release our dataset, our generation code and our models for\nuse by the broader community.",
        "url": "http://arxiv.org/abs/2507.18915v1",
        "published_date": "2025-07-25T03:15:16+00:00",
        "updated_date": "2025-07-25T03:15:16+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Ananya Sahu",
            "Amith Ananthram",
            "Kathleen McKeown"
        ],
        "tldr": "The paper introduces a method to mine contextualized visual associations from images to generate creative captions, creating a new dataset and improving zero-shot image-text retrieval in creative domains.",
        "tldr_zh": "该论文介绍了一种从图像中挖掘上下文视觉关联的方法，用于生成创意标题，创建了一个新的数据集，并提高了创意领域中零样本图像-文本检索的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning",
        "summary": "Vision Language Models (VLMs) have achieved remarkable breakthroughs in the\nfield of remote sensing in recent years. Synthetic Aperture Radar (SAR)\nimagery, with its all-weather capability, is essential in remote sensing, yet\nthe lack of large-scale, high-quality SAR image-text datasets hinders its\nsemantic understanding. In this paper, we construct SAR-Text, a large-scale and\nhigh-quality dataset consisting of over 130,000 SAR image-text pairs. To\nconstruct the SAR-Text dataset, we design the SAR-Narrator framework, which\ngenerates textual descriptions for SAR images through a multi-stage progressive\ntransfer learning strategy. To verify the effectiveness of the SAR-TEXT\ndataset, we conduct experiments on three typical vision-language tasks:\nimage-text retrieval, image captioning, and visual question answering (VQA).\nSpecifically, we construct three representative models on SAR-TEXT:\nSAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable\nimprovements in retrieval performance, boosting average recall by 16.43% and\n10.54% on the OSdataset-512 and HRSID test sets, respectively. In the\ncaptioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding\nthose of the original CoCa model by more than 8x, 4x, and 10x, respectively. In\nthe VQA task, SAR-GPT outperforms baseline and single-stage models on multiple\nSAR-VQA datasets, demonstrating stronger semantic understanding and reasoning\nability, as further confirmed by qualitative results. It is worth noting that,\nas a flexible captioning tool, SAR-Narrator can be readily adopted by the\ncommunity to construct larger-scale SAR image-text datasets.",
        "url": "http://arxiv.org/abs/2507.18743v1",
        "published_date": "2025-07-24T18:45:30+00:00",
        "updated_date": "2025-07-24T18:45:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinjun Cheng",
            "Yiguo He",
            "Junjie Zhu",
            "Chunping Qiu",
            "Jun Wang",
            "Qiangjuan Huang",
            "Ke Yang"
        ],
        "tldr": "The paper introduces SAR-Text, a large-scale SAR image-text dataset generated using a novel SAR-Narrator framework with progressive transfer learning, and demonstrates its effectiveness on several vision-language tasks.",
        "tldr_zh": "本文介绍了一个大规模SAR图像-文本数据集SAR-Text，该数据集通过一种新颖的SAR-Narrator框架和渐进式迁移学习生成，并在多个视觉语言任务中展示了其有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays",
        "summary": "Chest radiography (CXR) plays a crucial role in the diagnosis of various\ndiseases. However, the inherent class imbalance in the distribution of clinical\nfindings presents a significant challenge for current self-supervised deep\nlearning models. These models often fail to accurately classify long-tailed\nclasses. Current Vision-Language models such as Contrastive Language Image\nPre-training (CLIP) models effectively model the manifold distribution of the\nlatent space, enabling high zero-shot classification accuracies. Although CLIP\nperforms well on most of the primary classes in the dataset, our work reveals\nthat its effectiveness decreases significantly for classes with a long-tailed\ndistribution. Our approach employs a class-weighting mechanism that directly\naligns with the distribution of classes within the latent space. This method\nensures a substantial improvement in overall classification performance, with\nparticular emphasis on enhancing the recognition and accuracy of rarely\nobserved classes. We accomplish this by applying Gaussian Mixture Model (GMM)\nclustering to the latent space. The subsequent clusters are further refined by\nStudent t-distribution, followed by a metric loss that utilizes the altered\nembeddings. Our approach facilitates stable and adaptive clustering of the\nfeatures. This results in a notable average improvement of 7\\% points in\nzero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from\nprevious SOTA models.",
        "url": "http://arxiv.org/abs/2507.19398v1",
        "published_date": "2025-07-25T16:05:47+00:00",
        "updated_date": "2025-07-25T16:05:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Rajesh Madhipati",
            "Sheethal Bhat",
            "Lukas Buess",
            "Andreas Maier"
        ],
        "tldr": "The paper introduces a class-weighted approach using GMM and Student's t-distribution to improve zero-shot classification of long-tailed diseases in chest X-rays using CLIP, achieving a 7% AUC improvement on MIMIC-CXR-JPG.",
        "tldr_zh": "该论文提出了一种使用GMM和学生t分布的类别加权方法，以改进使用CLIP的胸部X光片中长尾疾病的零样本分类，在MIMIC-CXR-JPG数据集上实现了7%的AUC提升。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving",
        "summary": "Autonomous driving technology has the potential to transform transportation,\nbut its wide adoption depends on the development of interpretable and\ntransparent decision-making systems. Scene captioning, which generates natural\nlanguage descriptions of the driving environment, plays a crucial role in\nenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,\na lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM\nleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,\nincorporating a novel absolute positional encoding for view-specific scene\ndescriptions. Despite using a small 1B parameter base model, BEV-LLM achieves\ncompetitive performance on the nuCaption dataset, surpassing state-of-the-art\nby up to 5\\% in BLEU scores. Additionally, we release two new datasets - nuView\n(focused on environmental conditions and viewpoints) and GroundView (focused on\nobject grounding) - to better assess scene captioning across diverse driving\nscenarios and address gaps in current benchmarks, along with initial\nbenchmarking results demonstrating their effectiveness.",
        "url": "http://arxiv.org/abs/2507.19370v1",
        "published_date": "2025-07-25T15:22:56+00:00",
        "updated_date": "2025-07-25T15:22:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Felix Brandstaetter",
            "Erik Schuetz",
            "Katharina Winter",
            "Fabian Flohr"
        ],
        "tldr": "BEV-LLM is introduced for 3D scene captioning in autonomous driving, utilizing BEVFusion and a small 1B parameter model. They also release two new datasets to address gaps in current benchmarks.",
        "tldr_zh": "BEV-LLM被提出用于自动驾驶中的3D场景描述，利用BEVFusion和一个小型1B参数模型。他们还发布了两个新的数据集，以解决当前基准测试中的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow",
        "summary": "Remote sensing imagery presents vast, inherently unstructured spatial data,\ndemanding sophisticated reasoning to interpret complex user intents and\ncontextual relationships beyond simple recognition tasks. In this paper, we aim\nto construct an Earth observation workflow to handle complex queries by\nreasoning about spatial context and user intent. As a reasoning workflow, it\nshould be somewhat autonomous, where predefined ground-truth reasoning paths do\nnot constrain the learning process. Furthermore, its architecture ought to be\nunified yet flexible, enabling the model to perform diverse reasoning tasks\nwith distinct output formats through a single forward pass. Existing remote\nsensing approaches fail to address these requirements, as they rely on\nsupervised fine-tuning paradigms that constrain the autonomy of reasoning. To\nthis end, we propose RemoteReasoner, a flexible and robust workflow for remote\nsensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal\nlarge language model (MLLM) for interpreting user instructions and localizing\ntargets, together with task adaptation strategies that enable multi-granularity\noutput generation. In contrast to existing methods, our framework is trained\nwith reinforcement learning (RL) to endow the MLLM sufficient autonomy for\nprecise reasoning. At the inference stage, our adaptation strategies enable\ndiverse output formats at inference time without requiring task-specific\ndecoders or further fine-tuning. Preliminary experiments demonstrated that\nRemoteReasoner achieves remarkable performance across multi-granularity\nreasoning tasks, including region-level and pixel-level. Additionally, our\nframework enables novel capabilities such as the contour extraction task beyond\nthe reach of existing reasoning pipelines.",
        "url": "http://arxiv.org/abs/2507.19280v1",
        "published_date": "2025-07-25T13:58:11+00:00",
        "updated_date": "2025-07-25T13:58:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liang Yao",
            "Fan Liu",
            "Hongbo Lu",
            "Chuanyi Zhang",
            "Rui Min",
            "Shengxiang Xu",
            "Shimin Di",
            "Pai Peng"
        ],
        "tldr": "The paper introduces RemoteReasoner, a reinforcement learning-trained framework leveraging multi-modal large language models for complex geospatial reasoning tasks from remote sensing imagery, achieving multi-granularity output and novel capabilities without task-specific fine-tuning.",
        "tldr_zh": "该论文介绍了 RemoteReasoner，一个利用多模态大型语言模型进行复杂地理空间推理的强化学习训练框架，能够从遥感图像中实现多粒度输出和新颖的功能，而无需针对特定任务进行微调。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions",
        "summary": "Charts are a fundamental visualization format widely used in data analysis\nacross research and industry. While enabling users to edit charts based on\nhigh-level intentions is of great practical value, existing methods primarily\nrely on natural language instructions, which are often too ambiguous to support\nfine-grained editing. In this work, we introduce a novel paradigm for\nmultimodal chart editing, where user intent is expressed through a combination\nof natural language and visual indicators that explicitly highlight the\nelements to be modified. To support this paradigm, we present\nChart$\\text{M}^3$, a new benchmark for Multimodal chart editing with\nMulti-level complexity and Multi-perspective evaluation. Chart$\\text{M}^3$\ncontains 1,000 samples spanning four levels of editing difficulty. Each sample\nincludes triplets in the form of (chart, code, multimodal instructions). To\ncomprehensively evaluate chart editing models, Chart$\\text{M}^3$ provides\nmetrics that assess both visual appearance and code correctness. Our benchmark\nreveals significant limitations in current multimodal large language models\n(MLLMs), including GPT-4o, particularly in their ability to interpret and act\non visual indicators. To address this, we construct Chart$\\text{M}^3$-Train, a\nlarge-scale training set with 24,000 multimodal chart editing samples.\nFine-tuning MLLMs on this dataset leads to substantial improvements,\ndemonstrating the importance of multimodal supervision in building practical\nchart editing systems. Our datasets, codes, and evaluation tools are available\nat https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our\ndatasets, codes, and evaluation tools are available at\nhttps://github.com/yaolinli/VCE.",
        "url": "http://arxiv.org/abs/2507.21167v2",
        "published_date": "2025-07-25T13:30:14+00:00",
        "updated_date": "2025-07-30T05:05:10+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Donglu Yang",
            "Liang Zhang",
            "Zihao Yue",
            "Liangyu Chen",
            "Yichen Xu",
            "Wenxuan Wang",
            "Qin Jin"
        ],
        "tldr": "The paper introduces ChartM$^3$, a new benchmark for multimodal chart editing using natural language and visual indicators, and demonstrates the limitations of current MLLMs while providing a training dataset to improve their performance.",
        "tldr_zh": "本文介绍了ChartM$^3$，一个新的多模态图表编辑基准，它使用自然语言和视觉指示器，并展示了当前MLLM的局限性，同时提供了一个训练数据集来提高它们的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Querying GI Endoscopy Images: A VQA Approach",
        "summary": "VQA (Visual Question Answering) combines Natural Language Processing (NLP)\nwith image understanding to answer questions about a given image. It has\nenormous potential for the development of medical diagnostic AI systems. Such a\nsystem can help clinicians diagnose gastro-intestinal (GI) diseases accurately\nand efficiently. Although many of the multimodal LLMs available today have\nexcellent VQA capabilities in the general domain, they perform very poorly for\nVQA tasks in specialized domains such as medical imaging. This study is a\nsubmission for ImageCLEFmed-MEDVQA-GI 2025 subtask 1 that explores the\nadaptation of the Florence2 model to answer medical visual questions on GI\nendoscopy images. We also evaluate the model performance using standard metrics\nlike ROUGE, BLEU and METEOR",
        "url": "http://arxiv.org/abs/2507.21165v1",
        "published_date": "2025-07-25T13:03:46+00:00",
        "updated_date": "2025-07-25T13:03:46+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Gaurav Parajuli"
        ],
        "tldr": "This paper explores adapting the Florence2 model for Visual Question Answering (VQA) on gastro-intestinal (GI) endoscopy images, addressing the poor performance of general-purpose VQA models in specialized medical domains, and is a submission to ImageCLEFmed-MEDVQA-GI 2025.",
        "tldr_zh": "该论文探讨了如何调整 Florence2 模型，以在胃肠 (GI) 内窥镜图像上进行视觉问题解答 (VQA)，解决了通用 VQA 模型在专业医学领域表现不佳的问题，并且是 ImageCLEFmed-MEDVQA-GI 2025 的一个提交。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues",
        "summary": "Pedestrian intention prediction is essential for autonomous driving in\ncomplex urban environments. Conventional approaches depend on supervised\nlearning over frame sequences and require extensive retraining to adapt to new\nscenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention\nPrediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing\nintentions directly from short, continuous video clips enriched with structured\nJAAD metadata. In contrast to GPT-4V based methods that operate on discrete\nframes, BF-PIP processes uninterrupted temporal clips. It also incorporates\nbounding-box annotations and ego-vehicle speed via specialized multimodal\nprompts. Without any additional training, BF-PIP achieves 73% prediction\naccuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate\nthat combining temporal video inputs with contextual cues enhances\nspatiotemporal perception and improves intent inference under ambiguous\nconditions. This approach paves the way for agile, retraining-free perception\nmodule in intelligent transportation system.",
        "url": "http://arxiv.org/abs/2507.21161v1",
        "published_date": "2025-07-25T07:23:11+00:00",
        "updated_date": "2025-07-25T07:23:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Pallavi Zambare",
            "Venkata Nikhil Thanikella",
            "Ying Liu"
        ],
        "tldr": "The paper introduces BF-PIP, a zero-shot pedestrian intention prediction method using Gemini 2.5 Pro that processes raw temporal video with multimodal cues, outperforming GPT-4V in accuracy without retraining.",
        "tldr_zh": "该论文介绍了BF-PIP，一种使用Gemini 2.5 Pro的零样本行人意图预测方法，该方法处理原始时序视频和多模态线索，在准确性方面优于GPT-4V，且无需重新训练。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition",
        "summary": "Although pre-trained visual models with text have demonstrated strong\ncapabilities in visual feature extraction, sticker emotion understanding\nremains challenging due to its reliance on multi-view information, such as\nbackground knowledge and stylistic cues. To address this, we propose a novel\nmulti-granularity hierarchical fusion transformer (MGHFT), with a multi-view\nsticker interpreter based on Multimodal Large Language Models. Specifically,\ninspired by the human ability to interpret sticker emotions from multiple\nviews, we first use Multimodal Large Language Models to interpret stickers by\nproviding rich textual context via multi-view descriptions. Then, we design a\nhierarchical fusion strategy to fuse the textual context into visual\nunderstanding, which builds upon a pyramid visual transformer to extract both\nglobal and local sticker features at multiple stages. Through contrastive\nlearning and attention mechanisms, textual features are injected at different\nstages of the visual backbone, enhancing the fusion of global- and\nlocal-granularity visual semantics with textual guidance. Finally, we introduce\na text-guided fusion attention mechanism to effectively integrate the overall\nmultimodal features, enhancing semantic understanding. Extensive experiments on\n2 public sticker emotion datasets demonstrate that MGHFT significantly\noutperforms existing sticker emotion recognition approaches, achieving higher\naccuracy and more fine-grained emotion recognition. Compared to the best\npre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%\non F1 and 4.0% on accuracy. The code is released at\nhttps://github.com/cccccj-03/MGHFT_ACMMM2025.",
        "url": "http://arxiv.org/abs/2507.18929v1",
        "published_date": "2025-07-25T03:42:26+00:00",
        "updated_date": "2025-07-25T03:42:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jian Chen",
            "Yuxuan Hu",
            "Haifeng Lu",
            "Wei Wang",
            "Min Yang",
            "Chengming Li",
            "Xiping Hu"
        ],
        "tldr": "The paper introduces MGHFT, a multi-granularity hierarchical fusion transformer for sticker emotion recognition, using multi-modal large language models for contextual understanding and hierarchical fusion of textual and visual features. It achieves state-of-the-art results on public datasets.",
        "tldr_zh": "本文介绍了一种用于表情包情感识别的多粒度分层融合Transformer（MGHFT），它使用多模态大型语言模型进行上下文理解，并分层融合文本和视觉特征。在公共数据集上取得了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content",
        "summary": "Hateful memes aimed at LGBTQ\\,+ communities often evade detection by tweaking\neither the caption, the image, or both. We build the first robustness benchmark\nfor this setting, pairing four realistic caption attacks with three canonical\nimage corruptions and testing all combinations on the PrideMM dataset. Two\nstate-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and\nwe introduce a lightweight \\textbf{Text Denoising Adapter (TDA)} to enhance the\nlatter's resilience. Across the grid, MemeCLIP degrades more gently, while\nMemeBLIP2 is particularly sensitive to the caption edits that disrupt its\nlanguage processing. However, the addition of the TDA not only remedies this\nweakness but makes MemeBLIP2 the most robust model overall. Ablations reveal\nthat all systems lean heavily on text, but architectural choices and\npre-training data significantly impact robustness. Our benchmark exposes where\ncurrent multimodal safety models crack and demonstrates that targeted,\nlightweight modules like the TDA offer a powerful path towards stronger\ndefences.",
        "url": "http://arxiv.org/abs/2507.19551v1",
        "published_date": "2025-07-24T23:10:42+00:00",
        "updated_date": "2025-07-24T23:10:42+00:00",
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ran Tong",
            "Songtao Wei",
            "Jiaqi Liu",
            "Lanruo Wang"
        ],
        "tldr": "The paper introduces a benchmark to stress-test multimodal harmful-meme detectors on LGBTQ+ content using caption and image attacks, revealing vulnerabilities in current models and proposing a Text Denoising Adapter (TDA) to improve robustness.",
        "tldr_zh": "该论文引入了一个基准，用于通过标题和图像攻击对针对 LGBTQ+ 内容的多模态有害模因检测器进行压力测试，揭示了当前模型的漏洞，并提出了一种文本去噪适配器 (TDA) 以提高鲁棒性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]