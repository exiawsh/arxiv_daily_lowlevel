[
    {
        "title": "LRPO: Enhancing Blind Face Restoration through Online Reinforcement Learning",
        "summary": "Blind Face Restoration (BFR) encounters inherent challenges in exploring its\nlarge solution space, leading to common artifacts like missing details and\nidentity ambiguity in the restored images. To tackle these challenges, we\npropose a Likelihood-Regularized Policy Optimization (LRPO) framework, the\nfirst to apply online reinforcement learning (RL) to the BFR task. LRPO\nleverages rewards from sampled candidates to refine the policy network,\nincreasing the likelihood of high-quality outputs while improving restoration\nperformance on low-quality inputs. However, directly applying RL to BFR creates\nincompatibility issues, producing restoration results that deviate\nsignificantly from the ground truth. To balance perceptual quality and\nfidelity, we propose three key strategies: 1) a composite reward function\ntailored for face restoration assessment, 2) ground-truth guided likelihood\nregularization, and 3) noise-level advantage assignment. Extensive experiments\ndemonstrate that our proposed LRPO significantly improves the face restoration\nquality over baseline methods and achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2509.23339v1",
        "published_date": "2025-09-27T14:42:29+00:00",
        "updated_date": "2025-09-27T14:42:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bin Wu",
            "Yahui Liu",
            "Chi Zhang",
            "Yao Zhao",
            "Wei Wang"
        ],
        "tldr": "The paper introduces LRPO, a novel online reinforcement learning framework for blind face restoration, addressing artifacts like missing details and identity ambiguity. It achieves state-of-the-art performance by balancing perceptual quality and fidelity through a composite reward function, likelihood regularization, and noise-level advantage assignment.",
        "tldr_zh": "该论文介绍了LRPO，一种用于盲人脸修复的新型在线强化学习框架，旨在解决诸如缺失细节和身份模糊等问题。 它通过复合奖励函数、似然正则化和噪声水平优势分配来平衡感知质量和保真度，从而实现最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing the Unseen in Low-light Spike Streams",
        "summary": "Spike camera, a type of neuromorphic sensor with high-temporal resolution,\nshows great promise for high-speed visual tasks. Unlike traditional cameras,\nspike camera continuously accumulates photons and fires asynchronous spike\nstreams. Due to unique data modality, spike streams require reconstruction\nmethods to become perceptible to the human eye.\n  However, lots of methods struggle to handle spike streams in low-light\nhigh-speed scenarios due to severe noise and sparse information. In this work,\nwe propose Diff-SPK, the first diffusion-based reconstruction method for spike\ncamera. Diff-SPK effectively leverages generative priors to supplement texture\ninformation in low-light conditions. Specifically, it first employs an\n\\textbf{E}nhanced \\textbf{T}exture \\textbf{f}rom Inter-spike \\textbf{I}nterval\n(ETFI) to aggregate sparse information from low-light spike streams. Then, ETFI\nserves as a conditioning input for ControlNet to generate the high-speed\nscenes. To improve the quality of results, we introduce an ETFI-based feature\nfusion module during the generation process.\n  Moreover, we establish the first bona fide benchmark for the low-light spike\nstream reconstruction task. It significantly surpasses existing reconstruction\ndatasets in scale and provides quantitative illumination information. The\nperformance on real low-light spike streams demonstrates the superiority of\nDiff-SPK.",
        "url": "http://arxiv.org/abs/2509.23304v1",
        "published_date": "2025-09-27T13:33:03+00:00",
        "updated_date": "2025-09-27T13:33:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liwen Hu",
            "Yang Li",
            "Mianzhi Liu",
            "Yijia Guo",
            "Shenghao Xie",
            "Ziluo Ding",
            "Tiejun Huang",
            "Lei Ma"
        ],
        "tldr": "The paper introduces Diff-SPK, a diffusion-based reconstruction method for low-light spike streams from neuromorphic cameras, and establishes a new benchmark dataset for this task, demonstrating superior performance over existing methods.",
        "tldr_zh": "该论文介绍了一种基于扩散模型的重建方法Diff-SPK，用于从神经形态相机中重建低光照的spike stream，并建立了一个新的基准数据集，证明了其性能优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WeatherCycle: Unpaired Multi-Weather Restoration via Color Space Decoupled Cycle Learning",
        "summary": "Unsupervised image restoration under multi-weather conditions remains a\nfundamental yet underexplored challenge. While existing methods often rely on\ntask-specific physical priors, their narrow focus limits scalability and\ngeneralization to diverse real-world weather scenarios. In this work, we\npropose \\textbf{WeatherCycle}, a unified unpaired framework that reformulates\nweather restoration as a bidirectional degradation-content translation cycle,\nguided by degradation-aware curriculum regularization. At its core,\nWeatherCycle employs a \\textit{lumina-chroma decomposition} strategy to\ndecouple degradation from content without modeling complex weather, enabling\ndomain conversion between degraded and clean images. To model diverse and\ncomplex degradations, we propose a \\textit{Lumina Degradation Guidance Module}\n(LDGM), which learns luminance degradation priors from a degraded image pool\nand injects them into clean images via frequency-domain amplitude modulation,\nenabling controllable and realistic degradation modeling. Additionally, we\nincorporate a \\textit{Difficulty-Aware Contrastive Regularization (DACR)}\nmodule that identifies hard samples via a CLIP-based classifier and enforces\ncontrastive alignment between hard samples and restored features to enhance\nsemantic consistency and robustness. Extensive experiments across serve\nmulti-weather datasets, demonstrate that our method achieves state-of-the-art\nperformance among unsupervised approaches, with strong generalization to\ncomplex weather degradations.",
        "url": "http://arxiv.org/abs/2509.23150v1",
        "published_date": "2025-09-27T06:44:27+00:00",
        "updated_date": "2025-09-27T06:44:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenxuan Fang",
            "Jiangwei Weng",
            "Jianjun Qian",
            "Jian Yang",
            "Jun Li"
        ],
        "tldr": "The paper introduces WeatherCycle, an unsupervised image restoration framework for multi-weather conditions using lumina-chroma decomposition and degradation-aware curriculum regularization, achieving state-of-the-art performance and strong generalization.",
        "tldr_zh": "该论文提出了WeatherCycle，一个用于多天气条件下的无监督图像修复框架，它使用亮度-色度分解和感知退化的课程正则化，实现了最先进的性能和强大的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Stochastic Interpolants via Conditional Dependent Coupling",
        "summary": "Existing image generation models face critical challenges regarding the\ntrade-off between computation and fidelity. Specifically, models relying on a\npretrained Variational Autoencoder (VAE) suffer from information loss, limited\ndetail, and the inability to support end-to-end training. In contrast, models\noperating directly in the pixel space incur prohibitive computational cost.\nAlthough cascade models can mitigate computational cost, stage-wise separation\nprevents effective end-to-end optimization, hampers knowledge sharing, and\noften results in inaccurate distribution learning within each stage. To address\nthese challenges, we introduce a unified multistage generative framework based\non our proposed Conditional Dependent Coupling strategy. It decomposes the\ngenerative process into interpolant trajectories at multiple stages, ensuring\naccurate distribution learning while enabling end-to-end optimization.\nImportantly, the entire process is modeled as a single unified Diffusion\nTransformer, eliminating the need for disjoint modules and also enabling\nknowledge sharing. Extensive experiments demonstrate that our method achieves\nboth high fidelity and efficiency across multiple resolutions.",
        "url": "http://arxiv.org/abs/2509.23122v1",
        "published_date": "2025-09-27T05:03:08+00:00",
        "updated_date": "2025-09-27T05:03:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenrui Ma",
            "Xi Xiao",
            "Tianyang Wang",
            "Xiao Wang",
            "Yanning Shen"
        ],
        "tldr": "This paper introduces a novel multistage generative framework based on Conditional Dependent Coupling within a unified Diffusion Transformer to achieve high fidelity and efficiency in image generation by enabling end-to-end optimization and knowledge sharing.",
        "tldr_zh": "本文提出了一种基于条件依赖耦合的新型多阶段生成框架，该框架在一个统一的扩散Transformer中实现，通过端到端优化和知识共享，从而在图像生成中实现高保真度和高效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Soft-Di[M]O: Improving One-Step Discrete Image Generation with Soft Embeddings",
        "summary": "One-step generators distilled from Masked Diffusion Models (MDMs) compress\nmultiple sampling steps into a single forward pass, enabling efficient text and\nimage synthesis. However, they suffer two key limitations: they inherit\nmodeling bias from the teacher, and their discrete token outputs block gradient\nflow, preventing post-distillation refinements such as adversarial training,\nreward-based fine-tuning, and Test-Time Embedding Optimization (TTEO). In this\nwork, we introduce soft embeddings, a simple relaxation that replaces discrete\ntokens with the expected embeddings under the generator's output distribution.\nSoft embeddings preserve representation fidelity for one-step discrete\ngenerator while providing a fully differentiable continuous surrogate that is\ncompatible with teacher backbones and tokenizer decoders. Integrating soft\nembeddings into the Di[M]O distillation framework (denoted Soft-Di[M]O) makes\none-step generators end-to-end trainable and enables straightforward\napplication of GAN-based refinement, differentiable reward fine-tuning, and\nTTEO. Empirically, across multiple MDM teachers (e.g., MaskBit, MaskGen),\nSoft-Di[M]O achieves state-of-the-art one-step results: improved class-to-image\nperformance, a one-step FID of 1.56 on ImageNet-256 with GAN-based refinement,\nalong with higher GenEval and HPS scores on text-to-image with reward\nfine-tuning, and further gains from TTEO.",
        "url": "http://arxiv.org/abs/2509.22925v1",
        "published_date": "2025-09-26T20:51:20+00:00",
        "updated_date": "2025-09-26T20:51:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yuanzhi Zhu",
            "Xi Wang",
            "Stéphane Lathuilière",
            "Vicky Kalogeiton"
        ],
        "tldr": "The paper introduces Soft-Di[M]O, a method to improve one-step image generators distilled from Masked Diffusion Models by using soft embeddings for differentiability, enabling post-distillation refinement techniques and achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了Soft-Di[M]O，一种通过使用软嵌入来提高从Masked Diffusion Models中提取的单步图像生成器的方法，从而实现可微性，并支持后蒸馏优化技术，最终取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]