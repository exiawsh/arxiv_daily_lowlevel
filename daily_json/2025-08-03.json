[
    {
        "title": "SpatioTemporal Difference Network for Video Depth Super-Resolution",
        "summary": "Depth super-resolution has achieved impressive performance, and the\nincorporation of multi-frame information further enhances reconstruction\nquality. Nevertheless, statistical analyses reveal that video depth\nsuper-resolution remains affected by pronounced long-tailed distributions, with\nthe long-tailed effects primarily manifesting in spatial non-smooth regions and\ntemporal variation zones. To address these challenges, we propose a novel\nSpatioTemporal Difference Network (STDNet) comprising two core branches: a\nspatial difference branch and a temporal difference branch. In the spatial\ndifference branch, we introduce a spatial difference mechanism to mitigate the\nlong-tailed issues in spatial non-smooth regions. This mechanism dynamically\naligns RGB features with learned spatial difference representations, enabling\nintra-frame RGB-D aggregation for depth calibration. In the temporal difference\nbranch, we further design a temporal difference strategy that preferentially\npropagates temporal variation information from adjacent RGB and depth frames to\nthe current depth frame, leveraging temporal difference representations to\nachieve precise motion compensation in temporal long-tailed areas. Extensive\nexperimental results across multiple datasets demonstrate the effectiveness of\nour STDNet, outperforming existing approaches.",
        "url": "http://arxiv.org/abs/2508.01259v1",
        "published_date": "2025-08-02T08:18:38+00:00",
        "updated_date": "2025-08-02T08:18:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengxue Wang",
            "Yuan Wu",
            "Xiang Li",
            "Zhiqiang Yan",
            "Jian Yang"
        ],
        "tldr": "The paper introduces a SpatioTemporal Difference Network (STDNet) for video depth super-resolution, addressing long-tailed distributions in spatial non-smooth regions and temporal variation zones through spatial and temporal difference branches.",
        "tldr_zh": "该论文提出了一种用于视频深度超分辨率的时空差异网络 (STDNet)，通过空间和时间差异分支解决空间非平滑区域和时间变化区域中的长尾分布问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Hyperspectral Image Recovery Constrained by Multi-Granularity Non-Local Self-Similarity Priors",
        "summary": "Hyperspectral image (HSI) recovery, as an upstream image processing task,\n  holds significant importance for downstream tasks such as classification,\n  segmentation, and detection. In recent years, HSI recovery methods based on\n  non-local prior representations have demonstrated outstanding performance.\nHowever,\n  these methods employ a fixed-format factor to represent the non-local\nself-similarity\n  tensor groups, making them unable to adapt to diverse missing scenarios. To\naddress\n  this issue, we introduce the concept of granularity in tensor decomposition\nfor the first\n  time and propose an HSI recovery model constrained by multi-granularity\nnon-local\n  self-similarity priors. Specifically, the proposed model alternately performs\n  coarse-grained decomposition and fine-grained decomposition on the non-local\n  self-similarity tensor groups. Among them, the coarse-grained decomposition\nbuilds\n  upon Tucker tensor decomposition, which extracts global structural\ninformation of the\n  image by performing singular value shrinkage on the mode-unfolded matrices.\nThe\n  fine-grained decomposition employs the FCTN decomposition, capturing local\ndetail\n  information through modeling pairwise correlations among factor tensors. This\n  architectural approach achieves a unified representation of global, local,\nand non-local\n  priors for HSIs. Experimental results demonstrate that the model has strong\n  applicability and exhibits outstanding recovery effects in various types of\nmissing\n  scenes such as pixels and stripes.",
        "url": "http://arxiv.org/abs/2508.01435v1",
        "published_date": "2025-08-02T16:51:07+00:00",
        "updated_date": "2025-08-02T16:51:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuoran Peng",
            "Yiqing Shen"
        ],
        "tldr": "This paper introduces a novel hyperspectral image recovery method using multi-granularity non-local self-similarity priors, addressing limitations of fixed-format tensor decomposition by alternating coarse-grained and fine-grained decomposition to capture both global and local information.",
        "tldr_zh": "本文提出了一种新的高光谱图像恢复方法，该方法使用多粒度非局部自相似先验，通过交替粗粒度和细粒度分解来捕获全局和局部信息，从而解决了固定格式张量分解的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Effective Damage Data Generation by Fusing Imagery with Human Knowledge Using Vision-Language Models",
        "summary": "It is of crucial importance to assess damages promptly and accurately in\nhumanitarian assistance and disaster response (HADR). Current deep learning\napproaches struggle to generalize effectively due to the imbalance of data\nclasses, scarcity of moderate damage examples, and human inaccuracy in pixel\nlabeling during HADR situations. To accommodate for these limitations and\nexploit state-of-the-art techniques in vision-language models (VLMs) to fuse\nimagery with human knowledge understanding, there is an opportunity to generate\na diversified set of image-based damage data effectively. Our initial\nexperimental results suggest encouraging data generation quality, which\ndemonstrates an improvement in classifying scenes with different levels of\nstructural damage to buildings, roads, and infrastructures.",
        "url": "http://arxiv.org/abs/2508.01380v1",
        "published_date": "2025-08-02T14:22:25+00:00",
        "updated_date": "2025-08-02T14:22:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jie Wei",
            "Erika Ardiles-Cruz",
            "Aleksey Panasyuk",
            "Erik Blasch"
        ],
        "tldr": "This paper explores using Vision-Language Models (VLMs) to generate more balanced and accurate damage assessment data for humanitarian aid, addressing data scarcity and labeling inaccuracies in disaster response scenarios. Initial results show improved damage classification across different infrastructure types.",
        "tldr_zh": "本文探索使用视觉-语言模型（VLMs）生成更平衡和准确的灾害评估数据，以解决人道主义援助中数据稀缺和标注不准确的问题。初步结果表明，不同类型基础设施的灾害分类有所改善。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis",
        "summary": "Synthesizing amyloid PET scans from the more widely available and accessible\nstructural MRI modality offers a promising, cost-effective approach for\nlarge-scale Alzheimer's Disease (AD) screening. This is motivated by evidence\nthat, while MRI does not directly detect amyloid pathology, it may nonetheless\nencode information correlated with amyloid deposition that can be uncovered\nthrough advanced modeling. However, the high dimensionality and structural\ncomplexity of 3D neuroimaging data pose significant challenges for existing\nMRI-to-PET translation methods. Modeling the cross-modality relationship in a\nlower-dimensional latent space can simplify the learning task and enable more\neffective translation. As such, we present CoCoLIT (ControlNet-Conditioned\nLatent Image Translation), a diffusion-based latent generative framework that\nincorporates three main innovations: (1) a novel Weighted Image Space Loss\n(WISL) that improves latent representation learning and synthesis quality; (2)\na theoretical and empirical analysis of Latent Average Stabilization (LAS), an\nexisting technique used in similar generative models to enhance inference\nconsistency; and (3) the introduction of ControlNet-based conditioning for\nMRI-to-PET translation. We evaluate CoCoLIT's performance on publicly available\ndatasets and find that our model significantly outperforms state-of-the-art\nmethods on both image-based and amyloid-related metrics. Notably, in\namyloid-positivity classification, CoCoLIT outperforms the second-best method\nwith improvements of +10.5% on the internal dataset and +23.7% on the external\ndataset. The code and models of our approach are available at\nhttps://github.com/brAIn-science/CoCoLIT.",
        "url": "http://arxiv.org/abs/2508.01292v1",
        "published_date": "2025-08-02T09:58:30+00:00",
        "updated_date": "2025-08-02T09:58:30+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Alec Sargood",
            "Lemuel Puglisi",
            "James H. Cole",
            "Neil P. Oxtoby",
            "Daniele Ravì",
            "Daniel C. Alexander"
        ],
        "tldr": "This paper introduces CoCoLIT, a ControlNet-conditioned diffusion model for synthesizing amyloid PET scans from MRI images, demonstrating improved performance in amyloid-positivity classification compared to state-of-the-art methods.",
        "tldr_zh": "本文介绍了CoCoLIT，一个ControlNet条件扩散模型，用于从MRI图像合成淀粉样蛋白PET扫描，与最先进的方法相比，在淀粉样蛋白阳性分类方面表现出更高的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]