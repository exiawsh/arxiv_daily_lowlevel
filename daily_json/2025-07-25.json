[
    {
        "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs",
        "summary": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.",
        "url": "http://arxiv.org/abs/2507.18043v1",
        "published_date": "2025-07-24T02:34:13+00:00",
        "updated_date": "2025-07-24T02:34:13+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Duy Nguyen",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "tldr": "The paper introduces GrAInS, a gradient-based inference-time steering method for LLMs and VLMs that uses contrastive attribution to identify influential tokens and steer model behavior, outperforming fine-tuning and existing steering baselines.",
        "tldr_zh": "该论文介绍了GrAInS，一种基于梯度的LLM和VLM推理时引导方法，它使用对比归因来识别有影响力的tokens并引导模型行为，优于微调和现有的引导基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning",
        "summary": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC.",
        "url": "http://arxiv.org/abs/2507.18616v1",
        "published_date": "2025-07-24T17:53:26+00:00",
        "updated_date": "2025-07-24T17:53:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Si-Woo Kim",
            "MinJu Jeon",
            "Ye-Chan Kim",
            "Soeun Lee",
            "Taewhan Kim",
            "Dong-Jin Kim"
        ],
        "tldr": "The paper introduces SynC, a framework for refining synthetic image-caption datasets for zero-shot image captioning by reassigning captions to semantically aligned images using a one-to-many mapping and cycle-consistency-inspired scoring, achieving SOTA results.",
        "tldr_zh": "该论文介绍了SynC，一个用于改进零样本图像描述的合成图像-标题数据集的框架。该框架通过使用一对多映射和循环一致性评分，将标题重新分配给语义对齐的图像，从而实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning",
        "summary": "Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet.",
        "url": "http://arxiv.org/abs/2507.18531v1",
        "published_date": "2025-07-24T15:58:36+00:00",
        "updated_date": "2025-07-24T15:58:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianheng Qiu",
            "Jingchun Gao",
            "Jingyu Li",
            "Huiyi Leong",
            "Xuan Huang",
            "Xi Wang",
            "Xiaocheng Zhang",
            "Kele Xu",
            "Lan Zhang"
        ],
        "tldr": "The paper proposes IntentVCNet, a novel approach to improve intention-oriented controllable video captioning by bridging spatio-temporal gaps in Large Visual Language Models (LVLMs) using prompt combination and a parameter-efficient box adapter. It achieved state-of-the-art results and was the runner-up in the IntentVC challenge.",
        "tldr_zh": "该论文提出了IntentVCNet，一种通过结合提示组合和参数高效的box adapter来弥合大型视觉语言模型（LVLM）中的时空差距，从而改进面向意图的可控视频字幕生成的新方法。该方法取得了最先进的结果，并在IntentVC挑战赛中获得了亚军。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior",
        "summary": "Understanding a driver's behavior and intentions is important for potential\nrisk assessment and early accident prevention. Safety and driver assistance\nsystems can be tailored to individual drivers' behavior, significantly\nenhancing their effectiveness. However, existing datasets are limited in\ndescribing and explaining general vehicle movements based on external visual\nevidence. This paper introduces a benchmark, PDB-Eval, for a detailed\nunderstanding of Personalized Driver Behavior, and aligning Large Multimodal\nModels (MLLMs) with driving comprehension and reasoning. Our benchmark consists\nof two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'\nunderstanding of temporal driving scenes. Our dataset is designed to find valid\nvisual evidence from the external view to explain the driver's behavior from\nthe internal view. To align MLLMs' reasoning abilities with driving tasks, we\npropose PDB-QA as a visual explanation question-answering task for MLLM\ninstruction fine-tuning. As a generic learning task for generative models like\nMLLMs, PDB-QA can bridge the domain gap without harming MLLMs'\ngeneralizability. Our evaluation indicates that fine-tuning MLLMs on\nfine-grained descriptions and explanations can effectively bridge the gap\nbetween MLLMs and the driving domain, which improves zero-shot performance on\nquestion-answering tasks by up to 73.2%. We further evaluate the MLLMs\nfine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition\ntasks. We observe up to 12.5% performance improvements on the turn intention\nprediction task in Brain4Cars, and consistent performance improvements up to\n11.0% on all tasks in AIDE.",
        "url": "http://arxiv.org/abs/2507.18447v1",
        "published_date": "2025-07-24T14:33:06+00:00",
        "updated_date": "2025-07-24T14:33:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junda Wu",
            "Jessica Echterhoff",
            "Kyungtae Han",
            "Amr Abdelraouf",
            "Rohit Gupta",
            "Julian McAuley"
        ],
        "tldr": "The paper introduces PDB-Eval, a benchmark for evaluating Large Multimodal Models (MLLMs) in understanding and explaining personalized driving behavior, demonstrating improved performance after fine-tuning on their dataset.",
        "tldr_zh": "该论文介绍了PDB-Eval，一个用于评估大型多模态模型（MLLM）在理解和解释个性化驾驶行为方面的基准，并展示了在他们的数据集上进行微调后性能的提高。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiagR1: A Vision-Language Model Trained via Reinforcement Learning for Digestive Pathology Diagnosis",
        "summary": "Multimodal large models have shown great potential in automating pathology\nimage analysis. However, current multimodal models for gastrointestinal\npathology are constrained by both data quality and reasoning transparency:\npervasive noise and incomplete annotations in public datasets predispose vision\nlanguage models to factual hallucinations when generating diagnostic text,\nwhile the absence of explicit intermediate reasoning chains renders the outputs\ndifficult to audit and thus less trustworthy in clinical practice. To address\nthese issues, we construct a large scale gastrointestinal pathology dataset\ncontaining both microscopic descriptions and diagnostic conclusions, and\npropose a prompt argumentation strategy that incorporates lesion classification\nand anatomical site information. This design guides the model to better capture\nimage specific features and maintain semantic consistency in generation.\nFurthermore, we employ a post training pipeline that combines supervised fine\ntuning with Group Relative Policy Optimization (GRPO) to improve reasoning\nquality and output structure. Experimental results on real world pathology\nreport generation tasks demonstrate that our approach significantly outperforms\nstate of the art open source and proprietary baselines in terms of generation\nquality, structural completeness, and clinical relevance. Our solution\noutperforms state of the art models with 18.7% higher clinical relevance, 32.4%\nimproved structural completeness, and 41.2% fewer diagnostic errors,\ndemonstrating superior accuracy and clinical utility compared to existing\nsolutions.",
        "url": "http://arxiv.org/abs/2507.18433v1",
        "published_date": "2025-07-24T14:12:20+00:00",
        "updated_date": "2025-07-24T14:12:20+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Minxi Ouyang",
            "Lianghui Zhu",
            "Yaqing Bao",
            "Qiang Huang",
            "Jingli Ouyang",
            "Tian Guan",
            "Xitong Ling",
            "Jiawen Li",
            "Song Duan",
            "Wenbin Dai",
            "Li Zheng",
            "Xuemei Zhang",
            "Yonghong He"
        ],
        "tldr": "The paper introduces DiagR1, a Vision-Language Model for digestive pathology diagnosis, trained with reinforcement learning on a new dataset with a prompt argumentation strategy and a post-training pipeline for improved reasoning and accuracy.",
        "tldr_zh": "该论文介绍了DiagR1，一种用于消化病理诊断的视觉-语言模型，它使用强化学习在一个新的数据集上进行训练，该数据集具有提示论证策略和后训练流程，以提高推理和准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs",
        "summary": "Transferring and integrating knowledge across first-person (egocentric) and\nthird-person (exocentric) viewpoints is intrinsic to human intelligence,\nenabling humans to learn from others and convey insights from their own\nexperiences. Despite rapid progress in multimodal large language models\n(MLLMs), their ability to perform such cross-view reasoning remains unexplored.\nTo address this, we introduce EgoExoBench, the first benchmark for\negocentric-exocentric video understanding and reasoning. Built from publicly\navailable datasets, EgoExoBench comprises over 7,300 question-answer pairs\nspanning eleven sub-tasks organized into three core challenges: semantic\nalignment, viewpoint association, and temporal reasoning. We evaluate 13\nstate-of-the-art MLLMs and find that while these models excel on single-view\ntasks, they struggle to align semantics across perspectives, accurately\nassociate views, and infer temporal dynamics in the ego-exo context. We hope\nEgoExoBench can serve as a valuable resource for research on embodied agents\nand intelligent assistants seeking human-like cross-view intelligence.",
        "url": "http://arxiv.org/abs/2507.18342v1",
        "published_date": "2025-07-24T12:14:49+00:00",
        "updated_date": "2025-07-24T12:14:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuping He",
            "Yifei Huang",
            "Guo Chen",
            "Baoqi Pei",
            "Jilan Xu",
            "Tong Lu",
            "Jiangmiao Pang"
        ],
        "tldr": "EgoExoBench is introduced as the first benchmark for evaluating MLLMs on egocentric-exocentric video understanding, revealing their limitations in cross-view reasoning.",
        "tldr_zh": "EgoExoBench被提出，作为第一个评估MLLM在以自我为中心和以外部为中心的视频理解能力的基准，揭示了它们在跨视角推理方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow",
        "summary": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic\ndiagnosis, holding significant potential to address vision-threatening\ndiseases. However, their accuracy is constrained by hallucinations stemming\nfrom limited ophthalmic knowledge, insufficient visual localization and\nreasoning capabilities, and a scarcity of multimodal ophthalmic data, which\ncollectively impede precise lesion detection and disease diagnosis.\nFurthermore, existing medical benchmarks fail to effectively evaluate various\ntypes of hallucinations or provide actionable solutions to mitigate them. To\naddress the above challenges, we introduce EH-Benchmark, a novel ophthalmology\nbenchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'\nhallucinations based on specific tasks and error types into two primary\nclasses: Visual Understanding and Logical Composition, each comprising multiple\nsubclasses. Given that MLLMs predominantly rely on language-based reasoning\nrather than visual processing, we propose an agent-centric, three-phase\nframework, including the Knowledge-Level Retrieval stage, the Task-Level Case\nStudies stage, and the Result-Level Validation stage. Experimental results show\nthat our multi-agent framework significantly mitigates both types of\nhallucinations, enhancing accuracy, interpretability, and reliability. Our\nproject is available at https://github.com/ppxy1/EH-Benchmark.",
        "url": "http://arxiv.org/abs/2507.22929v1",
        "published_date": "2025-07-24T12:07:36+00:00",
        "updated_date": "2025-07-24T12:07:36+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Xiaoyu Pan",
            "Yang Bai",
            "Ke Zou",
            "Yang Zhou",
            "Jun Zhou",
            "Huazhu Fu",
            "Yih-Chung Tham",
            "Yong Liu"
        ],
        "tldr": "The paper introduces EH-Benchmark, a new ophthalmology benchmark for evaluating and mitigating hallucinations in medical LLMs, using a novel agent-centric framework. Results show that the framework reduces hallucinations and improves accuracy.",
        "tldr_zh": "该论文介绍了EH-Benchmark，一个新的眼科基准，用于评估和缓解医学大型语言模型中的幻觉，使用了一种新颖的以代理为中心的框架。结果表明，该框架减少了幻觉并提高了准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Large Vision-Language Models' Understanding for Field Data",
        "summary": "Large Vision-Language Models (LVLMs) have shown impressive capabilities\nacross a range of tasks that integrate visual and textual understanding, such\nas image captioning and visual question answering. These models are trained on\nlarge-scale image and video datasets paired with text, enabling them to bridge\nvisual perception and natural language processing. However, their application\nto scientific domains, especially in interpreting complex field data commonly\nused in the natural sciences, remains underexplored. In this work, we introduce\nFieldLVLM, a novel framework designed to improve large vision-language models'\nunderstanding of field data. FieldLVLM consists of two main components: a\nfield-aware language generation strategy and a data-compressed multimodal model\ntuning. The field-aware language generation strategy leverages a\nspecial-purpose machine learning pipeline to extract key physical features from\nfield data, such as flow classification, Reynolds number, and vortex patterns.\nThis information is then converted into structured textual descriptions that\nserve as a dataset. The data-compressed multimodal model tuning focuses on\nLVLMs with these generated datasets, using a data compression strategy to\nreduce the complexity of field inputs and retain only the most informative\nvalues. This ensures compatibility with the models language decoder and guides\nits learning more effectively. Experimental results on newly proposed benchmark\ndatasets demonstrate that FieldLVLM significantly outperforms existing methods\nin tasks involving scientific field data. Our findings suggest that this\napproach opens up new possibilities for applying large vision-language models\nto scientific research, helping bridge the gap between large models and\ndomain-specific discovery.",
        "url": "http://arxiv.org/abs/2507.18311v1",
        "published_date": "2025-07-24T11:28:53+00:00",
        "updated_date": "2025-07-24T11:28:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaomei Zhang",
            "Hanyu Zheng",
            "Xiangyu Zhu",
            "Jinghuan Wei",
            "Junhong Zou",
            "Zhen Lei",
            "Zhaoxiang Zhang"
        ],
        "tldr": "The paper introduces FieldLVLM, a framework that enhances large vision-language models for understanding complex field data by using field-aware language generation and data-compressed multimodal model tuning, achieving state-of-the-art results on proposed benchmarks.",
        "tldr_zh": "该论文介绍了FieldLVLM，一个通过使用领域感知语言生成和数据压缩多模态模型调优来增强大型视觉语言模型理解复杂领域数据的框架，并在提出的基准测试中取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LMM-Det: Make Large Multimodal Models Excel in Object Detection",
        "summary": "Large multimodal models (LMMs) have garnered wide-spread attention and\ninterest within the artificial intelligence research and industrial\ncommunities, owing to their remarkable capability in multimodal understanding,\nreasoning, and in-context learning, among others. While LMMs have demonstrated\npromising results in tackling multimodal tasks like image captioning, visual\nquestion answering, and visual grounding, the object detection capabilities of\nLMMs exhibit a significant gap compared to specialist detectors. To bridge the\ngap, we depart from the conventional methods of integrating heavy detectors\nwith LMMs and propose LMM-Det, a simple yet effective approach that leverages a\nLarge Multimodal Model for vanilla object Detection without relying on\nspecialized detection modules. Specifically, we conduct a comprehensive\nexploratory analysis when a large multimodal model meets with object detection,\nrevealing that the recall rate degrades significantly compared with specialist\ndetection models. To mitigate this, we propose to increase the recall rate by\nintroducing data distribution adjustment and inference optimization tailored\nfor object detection. We re-organize the instruction conversations to enhance\nthe object detection capabilities of large multimodal models. We claim that a\nlarge multimodal model possesses detection capability without any extra\ndetection modules. Extensive experiments support our claim and show the\neffectiveness of the versatile LMM-Det. The datasets, models, and codes are\navailable at https://github.com/360CVGroup/LMM-Det.",
        "url": "http://arxiv.org/abs/2507.18300v1",
        "published_date": "2025-07-24T11:05:24+00:00",
        "updated_date": "2025-07-24T11:05:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jincheng Li",
            "Chunyu Xie",
            "Ji Ao",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "tldr": "The paper introduces LMM-Det, an approach to enhance object detection capabilities of large multimodal models (LMMs) without relying on specialized detection modules, by focusing on data distribution adjustment and inference optimization.",
        "tldr_zh": "该论文介绍了 LMM-Det，一种通过专注于数据分布调整和推理优化来增强大型多模态模型 (LMM) 的目标检测能力的方法，而无需依赖专门的检测模块。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement",
        "summary": "Most existing low-light image enhancement (LLIE) methods rely on pre-trained\nmodel priors, low-light inputs, or both, while neglecting the semantic guidance\navailable from normal-light images. This limitation hinders their effectiveness\nin complex lighting conditions. In this paper, we propose VLM-IMI, a novel\nframework that leverages large vision-language models (VLMs) with iterative and\nmanual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions\nof the desired normal-light content as enhancement cues, enabling semantically\ninformed restoration. To effectively integrate cross-modal priors, we introduce\nan instruction prior fusion module, which dynamically aligns and fuses image\nand text features, promoting the generation of detailed and semantically\ncoherent outputs. During inference, we adopt an iterative and manual\ninstruction strategy to refine textual instructions, progressively improving\nvisual quality. This refinement enhances structural fidelity, semantic\nalignment, and the recovery of fine details under extremely low-light\nconditions. Extensive experiments across diverse scenarios demonstrate that\nVLM-IMI outperforms state-of-the-art methods in both quantitative metrics and\nperceptual quality. The source code is available at\nhttps://github.com/sunxiaoran01/VLM-IMI.",
        "url": "http://arxiv.org/abs/2507.18064v1",
        "published_date": "2025-07-24T03:35:20+00:00",
        "updated_date": "2025-07-24T03:35:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoran Sun",
            "Liyan Wang",
            "Cong Wang",
            "Yeying Jin",
            "Kin-man Lam",
            "Zhixun Su",
            "Yang Yang",
            "Jinshan Pan"
        ],
        "tldr": "This paper introduces VLM-IMI, a framework that utilizes large vision-language models with iterative and manual instructions for low-light image enhancement, achieving state-of-the-art results by incorporating semantic guidance from textual descriptions of normal-light content.",
        "tldr_zh": "该论文介绍了 VLM-IMI，一个利用大型视觉语言模型和迭代及手动指令进行低光图像增强的框架。通过结合正常光照内容的文本描述，实现语义引导，并取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks",
        "summary": "The rapid rise of deepfake technology, which produces realistic but\nfraudulent digital content, threatens the authenticity of media. Traditional\ndeepfake detection approaches often struggle with sophisticated, customized\ndeepfakes, especially in terms of generalization and robustness against\nmalicious attacks. This paper introduces ViGText, a novel approach that\nintegrates images with Vision Large Language Model (VLLM) Text explanations\nwithin a Graph-based framework to improve deepfake detection. The novelty of\nViGText lies in its integration of detailed explanations with visual data, as\nit provides a more context-aware analysis than captions, which often lack\nspecificity and fail to reveal subtle inconsistencies. ViGText systematically\ndivides images into patches, constructs image and text graphs, and integrates\nthem for analysis using Graph Neural Networks (GNNs) to identify deepfakes.\nThrough the use of multi-level feature extraction across spatial and frequency\ndomains, ViGText captures details that enhance its robustness and accuracy to\ndetect sophisticated deepfakes. Extensive experiments demonstrate that ViGText\nsignificantly enhances generalization and achieves a notable performance boost\nwhen it detects user-customized deepfakes. Specifically, average F1 scores rise\nfrom 72.45% to 98.32% under generalization evaluation, and reflects the model's\nsuperior ability to generalize to unseen, fine-tuned variations of stable\ndiffusion models. As for robustness, ViGText achieves an increase of 11.1% in\nrecall compared to other deepfake detection approaches. When facing targeted\nattacks that exploit its graph-based architecture, ViGText limits\nclassification performance degradation to less than 4%. ViGText uses detailed\nvisual and textual analysis to set a new standard for detecting deepfakes,\nhelping ensure media authenticity and information integrity.",
        "url": "http://arxiv.org/abs/2507.18031v1",
        "published_date": "2025-07-24T02:04:58+00:00",
        "updated_date": "2025-07-24T02:04:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ahmad ALBarqawi",
            "Mahmoud Nazzal",
            "Issa Khalil",
            "Abdallah Khreishah",
            "NhatHai Phan"
        ],
        "tldr": "ViGText uses Vision Language Models (VLMs) and Graph Neural Networks (GNNs) to enhance deepfake detection by integrating visual data with detailed textual explanations, achieving significant improvements in generalization and robustness.",
        "tldr_zh": "ViGText 利用视觉语言模型 (VLM) 和图神经网络 (GNN)，通过将视觉数据与详细的文本解释相结合，来增强深度伪造检测，并在泛化性和鲁棒性方面取得了显著改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures",
        "summary": "State-of-the-art (SOTA) image and text generation models are multimodal\nmodels that have many similarities to large language models (LLMs). Despite\nachieving strong performances, leading foundational multimodal model\narchitectures frequently lag behind the architectural sophistication of\ncontemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner\n(CoCa) model that incorporates Gaussian error gated linear units, root mean\nsquared normalization, and rotary positional embedding into the textual\ndecoders and the vision transformer (ViT) encoder. Each architectural\nmodification has been shown to improve model performance in LLMs, but has yet\nto be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model\nwith the same modified textual decoders but with CoCa's original ViT encoder.\nWe used standard pretraining and fine-tuning workflows to benchmark the models\non contrastive and generative tasks. Our GRR-CoCa significantly outperformed\nBaseline CoCa on the pretraining dataset and three diverse fine-tuning\ndatasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in\nperplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were\n13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We\nshow that GRR-CoCa's modified architecture improves performance and\ngeneralization across vision-language domains.",
        "url": "http://arxiv.org/abs/2507.18009v1",
        "published_date": "2025-07-24T00:54:31+00:00",
        "updated_date": "2025-07-24T00:54:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Jake R. Patock",
            "Nicole Catherine Lewis",
            "Kevin McCoy",
            "Christina Gomez",
            "Canling Chen",
            "Lorenzo Luzi"
        ],
        "tldr": "The paper introduces GRR-CoCa, an improved CoCa model incorporating LLM architectural advancements (Gaussian error gated linear units, RMSNorm, Rotary embeddings) into the ViT encoder and textual decoders, demonstrating significant performance gains in pretraining and fine-tuning across various vision-language tasks.",
        "tldr_zh": "本文介绍了GRR-CoCa，一种改进的CoCa模型，它将LLM架构的进步（高斯误差门控线性单元、RMSNorm、旋转嵌入）融入到ViT编码器和文本解码器中，在各种视觉-语言任务的预训练和微调中表现出显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation",
        "summary": "Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.",
        "url": "http://arxiv.org/abs/2507.18632v1",
        "published_date": "2025-07-24T17:59:36+00:00",
        "updated_date": "2025-07-24T17:59:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Ye-Chan Kim",
            "SeungJu Cha",
            "Si-Woo Kim",
            "Taewhan Kim",
            "Dong-Jin Kim"
        ],
        "tldr": "The paper introduces SIDA, a novel zero-shot domain adaptation method leveraging synthetic images generated via image translation to capture complex real-world variations for improved performance and efficiency, outperforming text-driven approaches.",
        "tldr_zh": "该论文介绍了一种名为SIDA的新型零样本领域自适应方法，该方法利用图像翻译生成的合成图像来捕获复杂的真实世界变化，从而提高性能和效率，优于文本驱动的方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Explaining How Visual, Textual and Multimodal Encoders Share Concepts",
        "summary": "Sparse autoencoders (SAEs) have emerged as a powerful technique for\nextracting human-interpretable features from neural networks activations.\nPrevious works compared different models based on SAE-derived features but\nthose comparisons have been restricted to models within the same modality. We\npropose a novel indicator allowing quantitative comparison of models across SAE\nfeatures, and use it to conduct a comparative study of visual, textual and\nmultimodal encoders. We also propose to quantify the Comparative Sharedness of\nindividual features between different classes of models. With these two new\ntools, we conduct several studies on 21 encoders of the three types, with two\nsignificantly different sizes, and considering generalist and domain specific\ndatasets. The results allow to revisit previous studies at the light of\nencoders trained in a multimodal context and to quantify to which extent all\nthese models share some representations or features. They also suggest that\nvisual features that are specific to VLMs among vision encoders are shared with\ntext encoders, highlighting the impact of text pretraining. The code is\navailable at https://github.com/CEA-LIST/SAEshareConcepts",
        "url": "http://arxiv.org/abs/2507.18512v1",
        "published_date": "2025-07-24T15:33:31+00:00",
        "updated_date": "2025-07-24T15:33:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Clément Cornet",
            "Romaric Besançon",
            "Hervé Le Borgne"
        ],
        "tldr": "This paper introduces a novel indicator to compare visual, textual, and multimodal encoders based on features extracted using sparse autoencoders, quantifying the sharedness of representations across different model types and datasets.",
        "tldr_zh": "本文提出了一种新指标，通过稀疏自编码器提取的特征来比较视觉、文本和多模态编码器，量化不同模型类型和数据集之间表征的共享程度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks",
        "summary": "Human action recognition plays a critical role in healthcare and medicine,\nsupporting applications such as patient behavior monitoring, fall detection,\nsurgical robot supervision, and procedural skill assessment. While traditional\nmodels like CNNs and RNNs have achieved moderate success, they often struggle\nto generalize across diverse and complex actions. Recent advancements in\nvision-language models, especially the transformer-based CLIP model, offer\npromising capabilities for generalizing action recognition from video data. In\nthis work, we evaluate CLIP on the UCF-101 dataset and systematically analyze\nits performance under three masking strategies: (1) percentage-based and\nshape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to\nsuppress bias-inducing elements, and (3) isolation masking that retains only\nclass-specific regions. Our results reveal that CLIP exhibits inconsistent\nbehavior and frequent misclassifications, particularly when essential visual\ncues are obscured. To overcome these limitations, we propose incorporating\nclass-specific noise, learned via a custom loss function, to reinforce\nattention to class-defining features. This enhancement improves classification\naccuracy and model confidence while reducing bias. We conclude with a\ndiscussion on the challenges of applying such models in clinical domains and\noutline directions for future work to improve generalizability across\ndomain-independent healthcare scenarios.",
        "url": "http://arxiv.org/abs/2507.18675v2",
        "published_date": "2025-07-24T13:13:28+00:00",
        "updated_date": "2025-07-30T20:14:41+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Utkarsh Shandilya",
            "Marsha Mariya Kappan",
            "Sanyam Jain",
            "Vijeta Sharma"
        ],
        "tldr": "This paper explores the generalization ability of CLIP for human action recognition, identifying limitations under masking and proposing a class-specific noise enhancement to improve performance and reduce bias, with a focus on healthcare applications.",
        "tldr_zh": "该论文探索了CLIP在人体动作识别中的泛化能力，发现其在掩蔽下的局限性，并提出了一种类特定的噪声增强方法，以提高性能和减少偏差，重点关注医疗保健应用。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation",
        "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos are available at\nhttps://github.com/scy-v/ReSem3D and https://resem3d.github.io.",
        "url": "http://arxiv.org/abs/2507.18262v2",
        "published_date": "2025-07-24T10:07:31+00:00",
        "updated_date": "2025-07-25T17:54:43+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Chenyu Su",
            "Weiwei Shang",
            "Chen Qian",
            "Fei Zhang",
            "Shuang Cong"
        ],
        "tldr": "ReSem3D is a robotic manipulation framework using MLLMs and VFMs for fine-grained semantic understanding and real-time 3D spatial constraint generation, enabling zero-shot adaptability in diverse environments.",
        "tldr_zh": "ReSem3D是一个机器人操作框架，利用MLLM和VFM进行细粒度的语义理解和实时的3D空间约束生成，从而在不同的环境中实现零样本适应性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
        "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (Text Embeddings\nFusion), a novel and efficient distillation method that directly incorporates\nthe guidance magnitude into the text embeddings and distills the teacher\nmodel's complex sampling strategy. By simply fusing conditional and\nunconditional text embeddings using linear operations, TeEFusion reconstructs\nthe desired guidance without adding extra parameters, simultaneously enabling\nthe student model to learn from the teacher's output produced via its\nsophisticated sampling approach. Extensive experiments on state-of-the-art\nmodels such as SD3 demonstrate that our method allows the student to closely\nmimic the teacher's performance with a far simpler and more efficient sampling\nstrategy. Consequently, the student model achieves inference speeds up to\n6$\\times$ faster than the teacher model, while maintaining image quality at\nlevels comparable to those obtained through the teacher's complex sampling\napproach. The code is publicly available at\nhttps://github.com/AIDC-AI/TeEFusion.",
        "url": "http://arxiv.org/abs/2507.18192v2",
        "published_date": "2025-07-24T08:45:40+00:00",
        "updated_date": "2025-07-25T03:17:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minghao Fu",
            "Guo-Hua Wang",
            "Xiaohao Chen",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "tldr": "TeEFusion is introduced as a novel distillation method that incorporates guidance magnitude into text embeddings by fusing conditional and unconditional embeddings to accelerate text-to-image generation inference while maintaining image quality.",
        "tldr_zh": "TeEFusion 是一种新颖的蒸馏方法，通过融合条件和非条件嵌入，将引导幅度整合到文本嵌入中，从而加速文本到图像生成推理，同时保持图像质量。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning",
        "summary": "Video Temporal Grounding (VTG) aims to localize relevant temporal segments in\nvideos given natural language queries. Despite recent progress with large\nvision-language models (LVLMs) and instruction-tuning, existing approaches\noften suffer from limited temporal awareness and poor generalization. In this\nwork, we introduce a two-stage training framework that integrates supervised\nfine-tuning with reinforcement learning (RL) to improve both the accuracy and\nrobustness of VTG models. Our approach first leverages high-quality curated\ncold start data for SFT initialization, followed by difficulty-controlled RL to\nfurther enhance temporal localization and reasoning abilities. Comprehensive\nexperiments on multiple VTG benchmarks demonstrate that our method consistently\noutperforms existing models, particularly in challenging and open-domain\nscenarios. We conduct an in-depth analysis of training strategies and dataset\ncuration, highlighting the importance of both high-quality cold start data and\ndifficulty-controlled RL. To facilitate further research and industrial\nadoption, we release all intermediate datasets, models, and code to the\ncommunity.",
        "url": "http://arxiv.org/abs/2507.18100v1",
        "published_date": "2025-07-24T05:24:01+00:00",
        "updated_date": "2025-07-24T05:24:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ruizhe Chen",
            "Zhiting Fan",
            "Tianze Luo",
            "Heqing Zou",
            "Zhaopeng Feng",
            "Guiyang Xie",
            "Hansheng Zhang",
            "Zhuochen Wang",
            "Zuozhu Liu",
            "Huaijian Zhang"
        ],
        "tldr": "This paper introduces a two-stage (SFT + RL) framework for Video Temporal Grounding (VTG) that improves accuracy and robustness, particularly in challenging scenarios, and releases datasets, models, and code.",
        "tldr_zh": "本文介绍了一种用于视频时间定位(VTG)的两阶段(SFT + RL)框架，提高了准确性和鲁棒性，尤其是在具有挑战性的场景中，并发布了数据集、模型和代码。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound",
        "summary": "Pancreatic cancer carries a poor prognosis and relies on endoscopic\nultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle\nnoise, low contrast, and unintuitive appearance of EUS make segmentation of\npancreatic tumors with fully supervised deep learning (DL) models both\nerror-prone and dependent on large, expert-curated annotation datasets. To\naddress these challenges, we present TextSAM-EUS, a novel, lightweight,\ntext-driven adaptation of the Segment Anything Model (SAM) that requires no\nmanual geometric prompts at inference. Our approach leverages text prompt\nlearning (context optimization) through the BiomedCLIP text encoder in\nconjunction with a LoRA-based adaptation of SAM's architecture to enable\nautomatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total\nparameters. On the public Endoscopic Ultrasound Database of the Pancreas,\nTextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized\nsurface distance (NSD), and with manual geometric prompts reaches 83.10% Dice\nand 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised\nDL models and foundation models (e.g., SAM and its variants). As the first\nattempt to incorporate prompt learning in SAM-based medical image segmentation,\nTextSAM-EUS offers a practical option for efficient and robust automatic EUS\nsegmentation. Code is available at https://github.com/HealthX-Lab/TextSAM-EUS .",
        "url": "http://arxiv.org/abs/2507.18082v3",
        "published_date": "2025-07-24T04:17:06+00:00",
        "updated_date": "2025-07-30T17:39:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Pascal Spiegler",
            "Taha Koleilat",
            "Arash Harirpoush",
            "Corey S. Miller",
            "Hassan Rivaz",
            "Marta Kersten-Oertel",
            "Yiming Xiao"
        ],
        "tldr": "TextSAM-EUS adapts the Segment Anything Model (SAM) with text prompt learning for accurate pancreatic tumor segmentation in endoscopic ultrasound images, achieving state-of-the-art results with minimal parameter tuning.",
        "tldr_zh": "TextSAM-EUS通过文本提示学习调整了Segment Anything Model (SAM)，以在内窥镜超声图像中实现准确的胰腺肿瘤分割，并以最小的参数调整实现了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
        "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI.",
        "url": "http://arxiv.org/abs/2507.18576v2",
        "published_date": "2025-07-24T16:49:19+00:00",
        "updated_date": "2025-07-28T05:33:59+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Shanghai AI Lab",
            ":",
            "Yicheng Bao",
            "Guanxu Chen",
            "Mingkang Chen",
            "Yunhao Chen",
            "Chiyu Chen",
            "Lingjie Chen",
            "Sirui Chen",
            "Xinquan Chen",
            "Jie Cheng",
            "Yu Cheng",
            "Dengke Deng",
            "Yizhuo Ding",
            "Dan Ding",
            "Xiaoshan Ding",
            "Yi Ding",
            "Zhichen Dong",
            "Lingxiao Du",
            "Yuyu Fan",
            "Xinshun Feng",
            "Yanwei Fu",
            "Yuxuan Gao",
            "Ruijun Ge",
            "Tianle Gu",
            "Lujun Gui",
            "Jiaxuan Guo",
            "Qianxi He",
            "Yuenan Hou",
            "Xuhao Hu",
            "Hong Huang",
            "Kaichen Huang",
            "Shiyang Huang",
            "Yuxian Jiang",
            "Shanzhe Lei",
            "Jie Li",
            "Lijun Li",
            "Hao Li",
            "Juncheng Li",
            "Xiangtian Li",
            "Yafu Li",
            "Lingyu Li",
            "Xueyan Li",
            "Haotian Liang",
            "Dongrui Liu",
            "Qihua Liu",
            "Zhixuan Liu",
            "Bangwei Liu",
            "Huacan Liu",
            "Yuexiao Liu",
            "Zongkai Liu",
            "Chaochao Lu",
            "Yudong Lu",
            "Xiaoya Lu",
            "Zhenghao Lu",
            "Qitan Lv",
            "Caoyuan Ma",
            "Jiachen Ma",
            "Xiaoya Ma",
            "Zhongtian Ma",
            "Lingyu Meng",
            "Ziqi Miao",
            "Yazhe Niu",
            "Yuezhang Peng",
            "Yuan Pu",
            "Han Qi",
            "Chen Qian",
            "Xingge Qiao",
            "Jingjing Qu",
            "Jiashu Qu",
            "Wanying Qu",
            "Wenwen Qu",
            "Xiaoye Qu",
            "Qihan Ren",
            "Qingnan Ren",
            "Qingyu Ren",
            "Jing Shao",
            "Wenqi Shao",
            "Shuai Shao",
            "Dongxing Shi",
            "Xin Song",
            "Xinhao Song",
            "Yan Teng",
            "Xuan Tong",
            "Yingchun Wang",
            "Xuhong Wang",
            "Shujie Wang",
            "Xin Wang",
            "Yige Wang",
            "Yixu Wang",
            "Yuanfu Wang",
            "Futing Wang",
            "Ruofan Wang",
            "Wenjie Wang",
            "Yajie Wang",
            "Muhao Wei",
            "Xiaoyu Wen",
            "Fenghua Weng",
            "Yuqi Wu",
            "Yingtong Xiong",
            "Xingcheng Xu",
            "Chao Yang",
            "Yue Yang",
            "Yang Yao",
            "Yulei Ye",
            "Zhenyun Yin",
            "Yi Yu",
            "Bo Zhang",
            "Qiaosheng Zhang",
            "Jinxuan Zhang",
            "Yexin Zhang",
            "Yinqiang Zheng",
            "Hefeng Zhou",
            "Zhanhui Zhou",
            "Pengyu Zhu",
            "Qingzi Zhu",
            "Yubo Zhu",
            "Bowen Zhou"
        ],
        "tldr": "The paper introduces SafeWork-R1, a multimodal reasoning model trained with the SafeLadder framework that achieves state-of-the-art safety performance while maintaining general capabilities by co-evolving safety and intelligence.",
        "tldr_zh": "该论文介绍了SafeWork-R1，一个通过SafeLadder框架训练的多模态推理模型，通过安全性和智能的协同进化，在保持通用能力的同时实现了最先进的安全性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs",
        "summary": "In this work, we address the problem of semantic object segmentation using\nfoundation models. We investigate whether foundation models, trained on a large\nnumber and variety of objects, can perform object segmentation without\nfine-tuning on specific images containing everyday objects, but in highly\ncluttered visual scenes. The ''in the wild'' context is driven by the target\napplication of vision guided upper limb neuroprostheses. We propose a method\nfor generating prompts based on gaze fixations to guide the Segment Anything\nModel (SAM) in our segmentation scenario, and fine-tune it on egocentric visual\ndata. Evaluation results of our approach show an improvement of the IoU\nsegmentation quality metric by up to 0.51 points on real-world challenging data\nof Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform\n(https://universe.roboflow.com/iwrist/grasping-in-the-wild)",
        "url": "http://arxiv.org/abs/2507.18517v1",
        "published_date": "2025-07-24T15:40:44+00:00",
        "updated_date": "2025-07-24T15:40:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bolutife Atoki",
            "Jenny Benois-Pineau",
            "Renaud Péteri",
            "Fabien Baldacci",
            "Aymar de Rugy"
        ],
        "tldr": "This paper explores the use of foundation models, specifically SAM, for object segmentation in cluttered real-world scenes relevant to vision-guided neuroprostheses, using gaze-fixation-based prompts and fine-tuning to improve performance.",
        "tldr_zh": "本文探讨了使用基础模型，特别是SAM，在与视觉引导神经假肢相关的混乱现实场景中进行目标分割，使用基于注视的提示和微调来提高性能。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Identifying Prompted Artist Names from Generated Images",
        "summary": "A common and controversial use of text-to-image models is to generate\npictures by explicitly naming artists, such as \"in the style of Greg\nRutkowski\". We introduce a benchmark for prompted-artist recognition:\npredicting which artist names were invoked in the prompt from the image alone.\nThe dataset contains 1.95M images covering 110 artists and spans four\ngeneralization settings: held-out artists, increasing prompt complexity,\nmultiple-artist prompts, and different text-to-image models. We evaluate\nfeature similarity baselines, contrastive style descriptors, data attribution\nmethods, supervised classifiers, and few-shot prototypical networks.\nGeneralization patterns vary: supervised and few-shot models excel on seen\nartists and complex prompts, whereas style descriptors transfer better when the\nartist's style is pronounced; multi-artist prompts remain the most challenging.\nOur benchmark reveals substantial headroom and provides a public testbed to\nadvance the responsible moderation of text-to-image models. We release the\ndataset and benchmark to foster further research:\nhttps://graceduansu.github.io/IdentifyingPromptedArtists/",
        "url": "http://arxiv.org/abs/2507.18633v1",
        "published_date": "2025-07-24T17:59:44+00:00",
        "updated_date": "2025-07-24T17:59:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Grace Su",
            "Sheng-Yu Wang",
            "Aaron Hertzmann",
            "Eli Shechtman",
            "Jun-Yan Zhu",
            "Richard Zhang"
        ],
        "tldr": "This paper introduces a benchmark dataset and evaluation framework for identifying which artists' styles were prompted in generated images, exploring various methods and generalization challenges. It highlights the need for responsible moderation of text-to-image models.",
        "tldr_zh": "该论文介绍了一个基准数据集和评估框架，用于识别生成图像中提示的艺术家风格，探讨了各种方法和泛化挑战。它强调了对文本到图像模型进行负责任的审核的必要性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    },
    {
        "title": "Gen-AI Police Sketches with Stable Diffusion",
        "summary": "This project investigates the use of multimodal AI-driven approaches to\nautomate and enhance suspect sketching. Three pipelines were developed and\nevaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model\nintegrated with a pre-trained CLIP model for text-image alignment, and (3)\nnovel approach incorporating LoRA fine-tuning of the CLIP model, applied to\nself-attention and cross-attention layers, and integrated with Stable\nDiffusion. An ablation study confirmed that fine-tuning both self- and\ncross-attention layers yielded the best alignment between text descriptions and\nsketches. Performance testing revealed that Model 1 achieved the highest\nstructural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of\n25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced\nperceptual similarity (LPIPS), with Model 3 showing improvement over Model 2\nbut still trailing Model 1. Qualitatively, sketches generated by Model 1\ndemonstrated the clearest facial features, highlighting its robustness as a\nbaseline despite its simplicity.",
        "url": "http://arxiv.org/abs/2507.18667v1",
        "published_date": "2025-07-24T04:41:58+00:00",
        "updated_date": "2025-07-24T04:41:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nicholas Fidalgo",
            "Aaron Contreras",
            "Katherine Harvey",
            "Johnny Ni"
        ],
        "tldr": "The paper explores using Stable Diffusion and CLIP models, including a LoRA fine-tuned version, to generate police sketches from text descriptions, finding that the baseline Stable Diffusion model produced the best structural similarity and clearest facial features.",
        "tldr_zh": "该论文探索了使用Stable Diffusion和CLIP模型（包括LoRA微调版本）从文本描述生成警察素描的方法。研究发现，基线Stable Diffusion模型产生了最佳的结构相似性和最清晰的面部特征。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4
    }
]