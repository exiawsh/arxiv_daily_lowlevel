[
    {
        "title": "2D Representation for Unguided Single-View 3D Super-Resolution in Real-Time",
        "summary": "We introduce 2Dto3D-SR, a versatile framework for real-time single-view 3D super-resolution that eliminates the need for high-resolution RGB guidance. Our framework encodes 3D data from a single viewpoint into a structured 2D representation, enabling the direct application of existing 2D image super-resolution architectures. We utilize the Projected Normalized Coordinate Code (PNCC) to represent 3D geometry from a visible surface as a regular image, thereby circumventing the complexities of 3D point-based or RGB-guided methods. This design supports lightweight and fast models adaptable to various deployment environments. We evaluate 2Dto3D-SR with two implementations: one using Swin Transformers for high accuracy, and another using Vision Mamba for high efficiency. Experiments show the Swin Transformer model achieves state-of-the-art accuracy on standard benchmarks, while the Vision Mamba model delivers competitive results at real-time speeds. This establishes our geometry-guided pipeline as a surprisingly simple yet viable and practical solution for real-world scenarios, especially where high-resolution RGB data is inaccessible.",
        "url": "http://arxiv.org/abs/2511.08224v1",
        "published_date": "2025-11-11T13:27:10+00:00",
        "updated_date": "2025-11-12T01:47:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ignasi Mas",
            "Ivan Huerta",
            "Ramon Morros",
            "Javier Ruiz-Hidalgo"
        ],
        "tldr": "The paper introduces 2Dto3D-SR, a real-time single-view 3D super-resolution framework that uses a 2D representation (PNCC) of 3D geometry to apply existing 2D image super-resolution techniques, achieving state-of-the-art accuracy or real-time speed with Swin Transformers and Vision Mamba respectively.",
        "tldr_zh": "该论文提出了2Dto3D-SR，一个实时单视图3D超分辨率框架，它使用3D几何体的2D表示（PNCC）来应用现有的2D图像超分辨率技术，分别使用Swin Transformers和Vision Mamba实现了最先进的精度或实时速度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StableMorph: High-Quality Face Morph Generation with Stable Diffusion",
        "summary": "Face morphing attacks threaten the integrity of biometric identity systems by enabling multiple individuals to share a single identity. To develop and evaluate effective morphing attack detection (MAD) systems, we need access to high-quality, realistic morphed images that reflect the challenges posed in real-world scenarios. However, existing morph generation methods often produce images that are blurry, riddled with artifacts, or poorly constructed making them easy to detect and not representative of the most dangerous attacks. In this work, we introduce StableMorph, a novel approach that generates highly realistic, artifact-free morphed face images using modern diffusion-based image synthesis. Unlike prior methods, StableMorph produces full-head images with sharp details, avoids common visual flaws, and offers unmatched control over visual attributes. Through extensive evaluation, we show that StableMorph images not only rival or exceed the quality of genuine face images but also maintain a strong ability to fool face recognition systems posing a greater challenge to existing MAD solutions and setting a new standard for morph quality in research and operational testing. StableMorph improves the evaluation of biometric security by creating more realistic and effective attacks and supports the development of more robust detection systems.",
        "url": "http://arxiv.org/abs/2511.08090v1",
        "published_date": "2025-11-11T10:44:41+00:00",
        "updated_date": "2025-11-12T01:39:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wassim Kabbani",
            "Kiran Raja",
            "Raghavendra Ramachandra",
            "Christoph Busch"
        ],
        "tldr": "StableMorph uses stable diffusion to generate high-quality, realistic face morphs for biometric security testing, improving upon existing methods that produce easily detectable artifacts and biases.",
        "tldr_zh": "StableMorph利用stable diffusion生成高质量、逼真的面部融合图像，用于生物识别安全测试，改进了现有方法中容易检测到的伪影和偏差。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion",
        "summary": "LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems.",
        "url": "http://arxiv.org/abs/2511.07377v1",
        "published_date": "2025-11-10T18:38:15+00:00",
        "updated_date": "2025-11-11T02:52:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "June Moh Goo",
            "Zichao Zeng",
            "Jan Boehm"
        ],
        "tldr": "The paper introduces FLASH, a novel LiDAR super-resolution framework utilizing frequency-aware attention and adaptive multi-scale fusion to achieve state-of-the-art performance with real-time efficiency, outperforming previous methods like TULIP.",
        "tldr_zh": "该论文介绍了FLASH，一种新颖的LiDAR超分辨率框架，它利用频率感知注意力和自适应多尺度融合来实现最先进的性能，并具有实时效率，优于以前的方法，如TULIP。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier",
        "summary": "Diffusion models have achieved remarkable success in conditional image generation, yet their outputs often remain misaligned with human preferences. To address this, recent work has applied Direct Preference Optimization (DPO) to diffusion models, yielding significant improvements.~However, DPO-like methods exhibit two key limitations: 1) High computational cost,due to the entire model fine-tuning; 2) Sensitivity to reference model quality}, due to its tendency to introduce instability and bias. To overcome these limitations, we propose a novel framework for human preference alignment in diffusion models (PC-Diffusion), using a lightweight, trainable Preference Classifier that directly models the relative preference between samples. By restricting preference learning to this classifier, PC-Diffusion decouples preference alignment from the generative model, eliminating the need for entire model fine-tuning and reference model reliance.~We further provide theoretical guarantees for PC-Diffusion:1) PC-Diffusion ensures that the preference-guided distributions are consistently propagated across timesteps. 2)The training objective of the preference classifier is equivalent to DPO, but does not require a reference model.3) The proposed preference-guided correction can progressively steer generation toward preference-aligned regions.~Empirical results show that PC-Diffusion achieves comparable preference consistency to DPO while significantly reducing training costs and enabling efficient and stable preference-guided generation.",
        "url": "http://arxiv.org/abs/2511.07806v1",
        "published_date": "2025-11-11T03:53:06+00:00",
        "updated_date": "2025-11-12T01:19:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shaomeng Wang",
            "He Wang",
            "Xiaolu Wei",
            "Longquan Dai",
            "Jinhui Tang"
        ],
        "tldr": "The paper introduces PC-Diffusion, a novel framework that uses a lightweight preference classifier to align diffusion models with human preferences, addressing the limitations of DPO-like methods by reducing computational cost and reliance on reference model quality.",
        "tldr_zh": "该论文介绍了PC-Diffusion，一种新型框架，它使用轻量级的偏好分类器来使扩散模型与人类偏好对齐，通过降低计算成本和对参考模型质量的依赖性，解决了类DPO方法的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics",
        "summary": "We introduce VectorSynth, a diffusion-based framework for pixel-accurate satellite image synthesis conditioned on polygonal geographic annotations with semantic attributes. Unlike prior text- or layout-conditioned models, VectorSynth learns dense cross-modal correspondences that align imagery and semantic vector geometry, enabling fine-grained, spatially grounded edits. A vision language alignment module produces pixel-level embeddings from polygon semantics; these embeddings guide a conditional image generation framework to respect both spatial extents and semantic cues. VectorSynth supports interactive workflows that mix language prompts with geometry-aware conditioning, allowing rapid what-if simulations, spatial edits, and map-informed content generation. For training and evaluation, we assemble a collection of satellite scenes paired with pixel-registered polygon annotations spanning diverse urban scenes with both built and natural features. We observe strong improvements over prior methods in semantic fidelity and structural realism, and show that our trained vision language model demonstrates fine-grained spatial grounding. The code and data are available at https://github.com/mvrl/VectorSynth.",
        "url": "http://arxiv.org/abs/2511.07744v1",
        "published_date": "2025-11-11T01:54:45+00:00",
        "updated_date": "2025-11-12T01:14:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Cher",
            "Brian Wei",
            "Srikumar Sastry",
            "Nathan Jacobs"
        ],
        "tldr": "VectorSynth is a diffusion-based framework that synthesizes pixel-accurate satellite images conditioned on polygonal geographic annotations with semantic attributes, enabling fine-grained, spatially grounded edits and outperforming prior methods.",
        "tldr_zh": "VectorSynth是一个基于扩散模型的框架，它能够根据带有语义属性的多边形地理注释合成像素精确的卫星图像，从而实现精细的、空间定位的编辑，并且优于先前的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Inference-Time Scaling of Diffusion Models for Infrared Data Generation",
        "summary": "Infrared imagery enables temperature-based scene understanding using passive sensors, particularly under conditions of low visibility where traditional RGB imaging fails. Yet, developing downstream vision models for infrared applications is hindered by the scarcity of high-quality annotated data, due to the specialized expertise required for infrared annotation. While synthetic infrared image generation has the potential to accelerate model development by providing large-scale, diverse training data, training foundation-level generative diffusion models in the infrared domain has remained elusive due to limited datasets. In light of such data constraints, we explore an inference-time scaling approach using a domain-adapted CLIP-based verifier for enhanced infrared image generation quality. We adapt FLUX.1-dev, a state-of-the-art text-to-image diffusion model, to the infrared domain by finetuning it on a small sample of infrared images using parameter-efficient techniques. The trained verifier is then employed during inference to guide the diffusion sampling process toward higher quality infrared generations that better align with input text prompts. Empirically, we find that our approach leads to consistent improvements in generation quality, reducing FID scores on the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared to unguided baseline samples. Our results suggest that inference-time guidance offers a promising direction for bridging the domain gap in low-data infrared settings.",
        "url": "http://arxiv.org/abs/2511.07362v1",
        "published_date": "2025-11-10T18:18:38+00:00",
        "updated_date": "2025-11-11T02:52:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Kai A. Horstmann",
            "Maxim Clouser",
            "Kia Khezeli"
        ],
        "tldr": "This paper explores inference-time scaling of diffusion models for generating infrared images, addressing the scarcity of infrared data by fine-tuning a text-to-image model and using a CLIP-based verifier for improved quality. It shows promising results in reducing FID scores on a benchmark dataset.",
        "tldr_zh": "本文探讨了扩散模型在推理时缩放以生成红外图像，通过微调文本到图像模型并使用基于CLIP的验证器来提高质量，从而解决了红外数据稀缺的问题。它在基准数据集上降低FID分数方面显示出很有希望的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Generating Sketches in a Hierarchical Auto-Regressive Process for Flexible Sketch Drawing Manipulation at Stroke-Level",
        "summary": "Generating sketches with specific patterns as expected, i.e., manipulating sketches in a controllable way, is a popular task. Recent studies control sketch features at stroke-level by editing values of stroke embeddings as conditions. However, in order to provide generator a global view about what a sketch is going to be drawn, all these edited conditions should be collected and fed into generator simultaneously before generation starts, i.e., no further manipulation is allowed during sketch generating process. In order to realize sketch drawing manipulation more flexibly, we propose a hierarchical auto-regressive sketch generating process. Instead of generating an entire sketch at once, each stroke in a sketch is generated in a three-staged hierarchy: 1) predicting a stroke embedding to represent which stroke is going to be drawn, and 2) anchoring the predicted stroke on the canvas, and 3) translating the embedding to a sequence of drawing actions to form the full sketch. Moreover, the stroke prediction, anchoring and translation are proceeded auto-regressively, i.e., both the recently generated strokes and their positions are considered to predict the current one, guiding model to produce an appropriate stroke at a suitable position to benefit the full sketch generation. It is flexible to manipulate stroke-level sketch drawing at any time during generation by adjusting the exposed editable stroke embeddings.",
        "url": "http://arxiv.org/abs/2511.07889v1",
        "published_date": "2025-11-11T06:40:12+00:00",
        "updated_date": "2025-11-12T01:26:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sicong Zang",
            "Shuhui Gao",
            "Zhijun Fang"
        ],
        "tldr": "This paper introduces a hierarchical auto-regressive model for generating sketches, enabling flexible stroke-level manipulation during the generation process by adjusting stroke embeddings.",
        "tldr_zh": "本文提出了一种用于生成草图的分层自回归模型，该模型通过调整笔画嵌入，能够在生成过程中灵活地进行笔画级操作。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]