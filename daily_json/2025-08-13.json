[
    {
        "title": "Unlocking the Potential of Diffusion Priors in Blind Face Restoration",
        "summary": "Although diffusion prior is rising as a powerful solution for blind face\nrestoration (BFR), the inherent gap between the vanilla diffusion model and BFR\nsettings hinders its seamless adaptation. The gap mainly stems from the\ndiscrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2)\nsynthesized and real-world images. The vanilla diffusion model is trained on\nimages with no or less degradations, whereas BFR handles moderately to severely\ndegraded images. Additionally, LQ images used for training are synthesized by a\nnaive degradation model with limited degradation patterns, which fails to\nsimulate complex and unknown degradations in real-world scenarios. In this\nwork, we use a unified network FLIPNET that switches between two modes to\nresolve specific gaps. In Restoration mode, the model gradually integrates\nBFR-oriented features and face embeddings from LQ images to achieve authentic\nand faithful face restoration. In Degradation mode, the model synthesizes\nreal-world like degraded images based on the knowledge learned from real-world\ndegradation datasets. Extensive evaluations on benchmark datasets show that our\nmodel 1) outperforms previous diffusion prior based BFR methods in terms of\nauthenticity and fidelity, and 2) outperforms the naive degradation model in\nmodeling the real-world degradations.",
        "url": "http://arxiv.org/abs/2508.08556v1",
        "published_date": "2025-08-12T01:50:55+00:00",
        "updated_date": "2025-08-12T01:50:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunqi Miao",
            "Zhiyu Qu",
            "Mingqi Gao",
            "Changrui Chen",
            "Jifei Song",
            "Jungong Han",
            "Jiankang Deng"
        ],
        "tldr": "This paper addresses the gap between vanilla diffusion models and blind face restoration by proposing a unified network, FLIPNET, that switches between restoration and degradation modes to handle real-world degraded images and improve authenticity and fidelity.",
        "tldr_zh": "该论文通过提出一个统一的网络FLIPNET，解决了 vanilla 扩散模型和盲脸修复之间的差距。FLIPNET 在修复和降解模式之间切换，以处理真实世界的退化图像，并提高真实性和保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SharpXR: Structure-Aware Denoising for Pediatric Chest X-Rays",
        "summary": "Pediatric chest X-ray imaging is essential for early diagnosis, particularly\nin low-resource settings where advanced imaging modalities are often\ninaccessible. Low-dose protocols reduce radiation exposure in children but\nintroduce substantial noise that can obscure critical anatomical details.\nConventional denoising methods often degrade fine details, compromising\ndiagnostic accuracy. In this paper, we present SharpXR, a structure-aware\ndual-decoder U-Net designed to denoise low-dose pediatric X-rays while\npreserving diagnostically relevant features. SharpXR combines a\nLaplacian-guided edge-preserving decoder with a learnable fusion module that\nadaptively balances noise suppression and structural detail retention. To\naddress the scarcity of paired training data, we simulate realistic\nPoisson-Gaussian noise on the Pediatric Pneumonia Chest X-ray dataset. SharpXR\noutperforms state-of-the-art baselines across all evaluation metrics while\nmaintaining computational efficiency suitable for resource-constrained\nsettings. SharpXR-denoised images improved downstream pneumonia classification\naccuracy from 88.8% to 92.5%, underscoring its diagnostic value in low-resource\npediatric care.",
        "url": "http://arxiv.org/abs/2508.08518v1",
        "published_date": "2025-08-11T23:07:20+00:00",
        "updated_date": "2025-08-11T23:07:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ilerioluwakiiye Abolade",
            "Emmanuel Idoko",
            "Solomon Odelola",
            "Promise Omoigui",
            "Adetola Adebanwo",
            "Aondana Iorumbur",
            "Udunna Anazodo",
            "Alessandro Crimi",
            "Raymond Confidence"
        ],
        "tldr": "The paper introduces SharpXR, a structure-aware U-Net for denoising low-dose pediatric chest X-rays, which improves downstream pneumonia classification accuracy, especially beneficial in low-resource environments.",
        "tldr_zh": "该论文介绍了 SharpXR，一种结构感知的 U-Net，用于对低剂量儿科胸部 X 射线进行去噪，提高了下游肺炎分类的准确性，尤其是在资源匮乏的环境中具有优势。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Per-Query Visual Concept Learning",
        "summary": "Visual concept learning, also known as Text-to-image personalization, is the\nprocess of teaching new concepts to a pretrained model. This has numerous\napplications from product placement to entertainment and personalized design.\nHere we show that many existing methods can be substantially augmented by\nadding a personalization step that is (1) specific to the prompt and noise\nseed, and (2) using two loss terms based on the self- and cross- attention,\ncapturing the identity of the personalized concept. Specifically, we leverage\nPDM features - previously designed to capture identity - and show how they can\nbe used to improve personalized semantic similarity. We evaluate the benefit\nthat our method gains on top of six different personalization methods, and\nseveral base text-to-image models (both UNet- and DiT-based). We find\nsignificant improvements even over previous per-query personalization methods.",
        "url": "http://arxiv.org/abs/2508.09045v1",
        "published_date": "2025-08-12T16:07:27+00:00",
        "updated_date": "2025-08-12T16:07:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ori Malca",
            "Dvir Samuel",
            "Gal Chechik"
        ],
        "tldr": "This paper introduces a per-query personalization method for text-to-image generation that improves semantic similarity by using self- and cross-attention based losses, and outperforms existing personalization methods.",
        "tldr_zh": "本文提出了一种用于文本到图像生成的逐查询个性化方法，该方法通过使用基于自注意力和交叉注意力的损失来提高语义相似性，并且优于现有的个性化方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SafeFix: Targeted Model Repair via Controlled Image Generation",
        "summary": "Deep learning models for visual recognition often exhibit systematic errors\ndue to underrepresented semantic subpopulations. Although existing debugging\nframeworks can pinpoint these failures by identifying key failure attributes,\nrepairing the model effectively remains difficult. Current solutions often rely\non manually designed prompts to generate synthetic training images -- an\napproach prone to distribution shift and semantic errors. To overcome these\nchallenges, we introduce a model repair module that builds on an interpretable\nfailure attribution pipeline. Our approach uses a conditional text-to-image\nmodel to generate semantically faithful and targeted images for failure cases.\nTo preserve the quality and relevance of the generated samples, we further\nemploy a large vision-language model (LVLM) to filter the outputs, enforcing\nalignment with the original data distribution and maintaining semantic\nconsistency. By retraining vision models with this rare-case-augmented\nsynthetic dataset, we significantly reduce errors associated with rare cases.\nOur experiments demonstrate that this targeted repair strategy improves model\nrobustness without introducing new bugs. Code is available at\nhttps://github.com/oxu2/SafeFix",
        "url": "http://arxiv.org/abs/2508.08701v1",
        "published_date": "2025-08-12T07:45:25+00:00",
        "updated_date": "2025-08-12T07:45:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Ouyang Xu",
            "Baoming Zhang",
            "Ruiyu Mao",
            "Yunhui Guo"
        ],
        "tldr": "SafeFix introduces a method for repairing deep learning models by generating targeted, semantically faithful images for rare failure cases using a conditional text-to-image model, filtered by an LVLM, to improve model robustness without introducing new errors.",
        "tldr_zh": "SafeFix 提出了一种修复深度学习模型的方法，通过使用条件文本到图像模型生成针对罕见失败案例的有针对性的、语义上忠实的图像，并由 LVLM 过滤，以提高模型的鲁棒性，而不会引入新的错误。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CObL: Toward Zero-Shot Ordinal Layering without User Prompting",
        "summary": "Vision benefits from grouping pixels into objects and understanding their\nspatial relationships, both laterally and in depth. We capture this with a\nscene representation comprising an occlusion-ordered stack of \"object layers,\"\neach containing an isolated and amodally-completed object. To infer this\nrepresentation from an image, we introduce a diffusion-based architecture named\nConcurrent Object Layers (CObL). CObL generates a stack of object layers in\nparallel, using Stable Diffusion as a prior for natural objects and\ninference-time guidance to ensure the inferred layers composite back to the\ninput image. We train CObL using a few thousand synthetically-generated images\nof multi-object tabletop scenes, and we find that it zero-shot generalizes to\nphotographs of real-world tabletops with varying numbers of novel objects. In\ncontrast to recent models for amodal object completion, CObL reconstructs\nmultiple occluded objects without user prompting and without knowing the number\nof objects beforehand. Unlike previous models for unsupervised object-centric\nrepresentation learning, CObL is not limited to the world it was trained in.",
        "url": "http://arxiv.org/abs/2508.08498v1",
        "published_date": "2025-08-11T22:08:57+00:00",
        "updated_date": "2025-08-11T22:08:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aneel Damaraju",
            "Dean Hazineh",
            "Todd Zickler"
        ],
        "tldr": "The paper introduces CObL, a diffusion-based architecture that infers a scene representation as an occlusion-ordered stack of object layers from an image, generalizing to real-world tabletop scenes without user prompting.",
        "tldr_zh": "该论文介绍了一种名为CObL的基于扩散的架构，它可以从图像中推断出场景的表示，该表示为一个遮挡排序的物体层堆栈，并且可以推广到现实世界的桌面场景，无需用户提示。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]