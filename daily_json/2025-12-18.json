[
    {
        "title": "MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance",
        "summary": "Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.",
        "url": "http://arxiv.org/abs/2512.15048v1",
        "published_date": "2025-12-17T03:23:12+00:00",
        "updated_date": "2025-12-17T03:23:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaizhe Zhang",
            "Shinan Chen",
            "Qian Zhao",
            "Weizhan Zhang",
            "Caixia Yan",
            "Yudeng Xin"
        ],
        "tldr": "This paper introduces MVGSR, a multi-view consistent super-resolution method for 3D Gaussian Splatting that leverages epipolar constraints and auxiliary view selection to improve detail fidelity and geometric consistency in unstructured multi-view datasets.",
        "tldr_zh": "本文介绍了MVGSR，一种多视角一致的3D高斯溅射超分辨率方法，利用极线约束和辅助视图选择来提高非结构化多视角数据集中细节的保真度和几何一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
        "summary": "Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($Λ_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.",
        "url": "http://arxiv.org/abs/2512.14697v1",
        "published_date": "2025-12-16T18:59:57+00:00",
        "updated_date": "2025-12-16T18:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.SP"
        ],
        "authors": [
            "Yue Zhao",
            "Hanwen Jiang",
            "Zhenlin Xu",
            "Chutong Yang",
            "Ehsan Adeli",
            "Philipp Krähenbühl"
        ],
        "tldr": "The paper introduces Spherical Leech Quantization ($Λ_{24}$-SQ), a novel quantization method based on the Leech lattice, for improved image tokenization, compression, and generation, achieving better performance than BSQ with fewer bits.",
        "tldr_zh": "该论文介绍了球形李奇量化 ($Λ_{24}$-SQ)，一种基于李奇格子的新型量化方法，用于改进图像标记化、压缩和生成，与 BSQ 相比，以更少的比特实现了更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
        "summary": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \\textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \\textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \\href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}",
        "url": "http://arxiv.org/abs/2512.15603v1",
        "published_date": "2025-12-17T17:12:42+00:00",
        "updated_date": "2025-12-17T17:12:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengming Yin",
            "Zekai Zhang",
            "Zecheng Tang",
            "Kaiyuan Gao",
            "Xiao Xu",
            "Kun Yan",
            "Jiahao Li",
            "Yilei Chen",
            "Yuxiang Chen",
            "Heung-Yeung Shum",
            "Lionel M. Ni",
            "Jingren Zhou",
            "Junyang Lin",
            "Chenfei Wu"
        ],
        "tldr": "The paper introduces Qwen-Image-Layered, a diffusion model that decomposes images into semantically disentangled RGBA layers for inherent editability, addressing consistency issues in image editing. They also provide a method for generating multilayer training data.",
        "tldr_zh": "该论文介绍了 Qwen-Image-Layered，一种将图像分解为语义分离的 RGBA 图层的扩散模型，以实现固有的可编辑性，从而解决图像编辑中的一致性问题。他们还提供了一种生成多层训练数据的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
        "summary": "The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \\textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.",
        "url": "http://arxiv.org/abs/2512.15110v1",
        "published_date": "2025-12-17T06:02:25+00:00",
        "updated_date": "2025-12-17T06:02:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jialong Zuo",
            "Haoyou Deng",
            "Hanyu Zhou",
            "Jiaxin Zhu",
            "Yicheng Zhang",
            "Yiwei Zhang",
            "Yongxin Yan",
            "Kaixing Huang",
            "Weisen Chen",
            "Yongtai Deng",
            "Rui Jin",
            "Nong Sang",
            "Changxin Gao"
        ],
        "tldr": "The paper evaluates Nano Banana Pro, a text-to-image model, on low-level vision tasks, finding it excels in subjective visual quality but lags in quantitative metrics due to inherent stochasticity.",
        "tldr_zh": "该论文评估了文本到图像模型Nano Banana Pro在低级视觉任务中的表现，发现它在主观视觉质量方面表现出色，但由于固有的随机性，在定量指标方面表现落后。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    }
]