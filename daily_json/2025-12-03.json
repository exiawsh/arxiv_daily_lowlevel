[
    {
        "title": "Glance: Accelerating Diffusion Models with 1 Sample",
        "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
        "url": "http://arxiv.org/abs/2512.02899v1",
        "published_date": "2025-12-02T16:05:21+00:00",
        "updated_date": "2025-12-02T16:05:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuobai Dong",
            "Rui Zhao",
            "Songjie Wu",
            "Junchao Yi",
            "Linjie Li",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Alex Jinpeng Wang"
        ],
        "tldr": "The paper introduces Glance, a method to accelerate diffusion models by strategically applying different speedups to different denoising phases using lightweight LoRA adapters, achieving significant acceleration with minimal training and strong generalization.",
        "tldr_zh": "该论文介绍了 Glance，一种加速扩散模型的方法，通过使用轻量级 LoRA 适配器有策略地将不同的加速应用于不同的去噪阶段，从而以最少的训练和强大的泛化能力实现显着的加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Progressive Image Restoration via Text-Conditioned Video Generation",
        "summary": "Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.",
        "url": "http://arxiv.org/abs/2512.02273v1",
        "published_date": "2025-12-01T23:37:51+00:00",
        "updated_date": "2025-12-01T23:37:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Peng Kang",
            "Xijun Wang",
            "Yu Yuan"
        ],
        "tldr": "This paper repurposes a text-to-video model (CogVideo) for progressive image restoration by fine-tuning it on synthetic datasets and using text prompts to guide restoration trajectories, achieving good results in super-resolution, deblurring, and low-light enhancement.",
        "tldr_zh": "本文通过微调CogVideo文本到视频模型，并利用文本提示指导恢复轨迹，将其重新用于渐进式图像恢复任务。该方法在超分辨率、去模糊和低光增强方面取得了良好的效果。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution",
        "summary": "Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.",
        "url": "http://arxiv.org/abs/2512.02681v1",
        "published_date": "2025-12-02T12:06:39+00:00",
        "updated_date": "2025-12-02T12:06:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongbao Yang",
            "Jiangxin Dong",
            "Yazhou Yao",
            "Jinhui Tang",
            "Jinshan Pan"
        ],
        "tldr": "PGP-DiffSR introduces a phase-guided progressive pruning method to reduce the computational cost of diffusion-based image super-resolution models, achieving competitive performance with significantly reduced resources.",
        "tldr_zh": "PGP-DiffSR提出了一种相位引导的渐进式剪枝方法，以降低基于扩散的图像超分辨率模型的计算成本，并在显著减少资源的情况下实现具有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OmniPerson: Unified Identity-Preserving Pedestrian Generation",
        "summary": "Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.",
        "url": "http://arxiv.org/abs/2512.02554v1",
        "published_date": "2025-12-02T09:24:34+00:00",
        "updated_date": "2025-12-02T09:24:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changxiao Ma",
            "Chao Yuan",
            "Xincheng Shi",
            "Yuzhuo Ma",
            "Yongfei Zhang",
            "Longkun Zhou",
            "Yujia Zhang",
            "Shangze Li",
            "Yifan Xu"
        ],
        "tldr": "The paper introduces OmniPerson, a unified pedestrian generation pipeline that preserves identity and offers fine-grained control over pedestrian attributes, addressing the lack of high-quality training data for person re-identification tasks. They also introduce a new large-scale dataset, PersonSyn.",
        "tldr_zh": "该论文介绍了OmniPerson，一个统一的行人生成流程，可以保持身份并对行人属性进行细粒度控制，解决了行人重识别任务中缺乏高质量训练数据的问题。他们还引入了一个新的大规模数据集PersonSyn。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Two-Stage Vision Transformer for Image Restoration: Colorization Pretraining + Residual Upsampling",
        "summary": "In computer vision, Single Image Super-Resolution (SISR) is still a difficult problem. We present ViT-SR, a new technique to improve the performance of a Vision Transformer (ViT) employing a two-stage training strategy. In our method, the model learns rich, generalizable visual representations from the data itself through a self-supervised pretraining phase on a colourization task. The pre-trained model is then adjusted for 4x super-resolution. By predicting the addition of a high-frequency residual image to an initial bicubic interpolation, this design simplifies residual learning. ViT-SR, trained and evaluated on the DIV2K benchmark dataset, achieves an impressive SSIM of 0.712 and PSNR of 22.90 dB. These results demonstrate the efficacy of our two-stage approach and highlight the potential of self-supervised pre-training for complex image restoration tasks. Further improvements may be possible with larger ViT architectures or alternative pretext tasks.",
        "url": "http://arxiv.org/abs/2512.02512v1",
        "published_date": "2025-12-02T08:10:55+00:00",
        "updated_date": "2025-12-02T08:10:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aditya Chaudhary",
            "Prachet Dev Singh",
            "Ankit Jha"
        ],
        "tldr": "This paper presents ViT-SR, a two-stage Vision Transformer approach for single image super-resolution (SISR). It uses colorization pretraining for better feature extraction and residual learning with upsampling to achieve promising results on the DIV2K dataset.",
        "tldr_zh": "本文提出了一种用于单图像超分辨率(SISR)的两阶段视觉Transformer方法ViT-SR。它采用着色预训练来提取更好的特征，并使用残差学习和上采样在DIV2K数据集上取得了有希望的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting",
        "summary": "3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.",
        "url": "http://arxiv.org/abs/2512.02172v1",
        "published_date": "2025-12-01T20:08:39+00:00",
        "updated_date": "2025-12-01T20:08:39+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Pranav Asthana",
            "Alex Hanson",
            "Allen Tu",
            "Tom Goldstein",
            "Matthias Zwicker",
            "Amitabh Varshney"
        ],
        "tldr": "The paper introduces SplatSuRe, a selective super-resolution method for 3D Gaussian Splatting that applies SR only in undersampled regions to improve multi-view consistency and rendering quality, especially in localized foreground regions.",
        "tldr_zh": "该论文提出了SplatSuRe，一种用于3D高斯溅射的选择性超分辨率方法，它仅在欠采样区域应用超分辨率，以提高多视角一致性和渲染质量，尤其是在局部前景区域。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation",
        "summary": "In this paper, we investigate the underexplored challenge of sample diversity in autoregressive (AR) generative models with bitwise visual tokenizers. We first analyze the factors that limit diversity in bitwise AR models and identify two key issues: (1) the binary classification nature of bitwise modeling, which restricts the prediction space, and (2) the overly sharp logits distribution, which causes sampling collapse and reduces diversity. Building on these insights, we propose DiverseAR, a principled and effective method that enhances image diversity without sacrificing visual quality. Specifically, we introduce an adaptive logits distribution scaling mechanism that dynamically adjusts the sharpness of the binary output distribution during sampling, resulting in smoother predictions and greater diversity. To mitigate potential fidelity loss caused by distribution smoothing, we further develop an energy-based generation path search algorithm that avoids sampling low-confidence tokens, thereby preserving high visual quality. Extensive experiments demonstrate that DiverseAR substantially improves sample diversity in bitwise autoregressive image generation.",
        "url": "http://arxiv.org/abs/2512.02931v1",
        "published_date": "2025-12-02T16:54:36+00:00",
        "updated_date": "2025-12-02T16:54:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying Yang",
            "Zhengyao Lv",
            "Tianlin Pan",
            "Haofan Wang",
            "Binxin Yang",
            "Hubery Yin",
            "Chen Li",
            "Chenyang Si"
        ],
        "tldr": "The paper proposes DiverseAR, a method to improve sample diversity in bitwise autoregressive image generation by addressing issues of limited prediction space and overly sharp logits distributions. It introduces adaptive logits scaling and an energy-based generation path search to enhance diversity without sacrificing image quality.",
        "tldr_zh": "该论文提出了DiverseAR，一种通过解决预测空间有限和logits分布过于尖锐的问题来提高逐位自回归图像生成中样本多样性的方法。它引入了自适应logits缩放和基于能量的生成路径搜索，以增强多样性而不牺牲图像质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation",
        "summary": "Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.",
        "url": "http://arxiv.org/abs/2512.01960v1",
        "published_date": "2025-12-01T18:13:40+00:00",
        "updated_date": "2025-12-01T18:13:40+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Zisu Li",
            "Hengye Lyu",
            "Jiaxin Shi",
            "Yufeng Zeng",
            "Mingming Fan",
            "Hanwang Zhang",
            "Chen Liang"
        ],
        "tldr": "SpriteHand introduces an autoregressive video generation framework for real-time synthesis of hand-object interaction videos, addressing limitations of traditional physics engines with deformable objects and achieving real-time performance with high visual quality.",
        "tldr_zh": "SpriteHand 提出了一种自回归视频生成框架，用于实时合成手部与物体交互的视频，解决了传统物理引擎在处理可变形物体方面的局限性，并以高质量的视觉效果实现了实时性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]