[
    {
        "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
        "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.",
        "url": "http://arxiv.org/abs/2511.22699v1",
        "published_date": "2025-11-27T18:52:07+00:00",
        "updated_date": "2025-11-27T18:52:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Z-Image Team",
            "Huanqia Cai",
            "Sihan Cao",
            "Ruoyi Du",
            "Peng Gao",
            "Steven Hoi",
            "Shijie Huang",
            "Zhaohui Hou",
            "Dengyang Jiang",
            "Xin Jin",
            "Liangchen Li",
            "Zhen Li",
            "Zhong-Yu Li",
            "David Liu",
            "Dongyang Liu",
            "Junhan Shi",
            "Qilong Wu",
            "Feng Yu",
            "Chi Zhang",
            "Shifeng Zhang",
            "Shilin Zhou"
        ],
        "tldr": "Z-Image is a 6B-parameter efficient image generation model that achieves comparable or superior performance to larger models while significantly reducing computational costs and enabling fine-tuning on consumer hardware.",
        "tldr_zh": "Z-Image是一个60亿参数的高效图像生成模型，它在显著降低计算成本并在消费级硬件上进行微调的同时，实现了与更大模型相当甚至更优越的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Markovian Scale Prediction: A New Era of Visual Autoregressive Generation",
        "summary": "Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 $\\times$ 256) and decreases peak memory consumption by 83.8% (1024 $\\times$ 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.",
        "url": "http://arxiv.org/abs/2511.23334v1",
        "published_date": "2025-11-28T16:42:18+00:00",
        "updated_date": "2025-11-28T16:42:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Zhang",
            "Jingyi Liu",
            "Yiwei Shi",
            "Qi Zhang",
            "Duoqian Miao",
            "Changwei Wang",
            "Longbing Cao"
        ],
        "tldr": "The paper introduces Markov-VAR, a more efficient Visual AutoRegressive model using Markovian Scale Prediction to reduce computational overhead and memory consumption compared to standard VAR, while achieving better performance on ImageNet.",
        "tldr_zh": "该论文介绍了 Markov-VAR，这是一种更高效的视觉自回归模型，它使用马尔可夫尺度预测来减少计算开销和内存消耗，与标准 VAR 相比，在 ImageNet 上实现了更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI",
        "summary": "MR data are acquired in the frequency domain, known as k-space. Acquiring high-quality and high-resolution MR images can be time-consuming, posing a significant challenge when multiple sequences providing complementary contrast information are needed or when the patient is unable to remain in the scanner for an extended period of time. Reducing k-space measurements is a strategy to speed up acquisition, but often leads to reduced quality in reconstructed images. Additionally, in real-world MRI, both under-sampled and full-sampled images are prone to artefacts, and correcting these artefacts is crucial for maintaining diagnostic accuracy. Deep learning methods have been proposed to restore image quality from under-sampled data, while others focused on the correction of artefacts that result from the noise or motion. No approach has however been proposed so far that addresses both acceleration and artefacts correction, limiting the performance of these models when these degradation factors occur simultaneously. To address this gap, we present a method for recovering high-quality images from under-sampled data with simultaneously correction for noise and motion artefact called USArt (Under-Sampling and Artifact correction model). Customized for 2D brain anatomical images acquired with Cartesian sampling, USArt employs a dual sub-model approach. The results demonstrate remarkable increase of signal-to-noise ratio (SNR) and contrast in the images restored. Various under-sampling strategies and degradation levels were explored, with the gradient under-sampling strategy yielding the best outcomes. We achieved up to 5x acceleration and simultaneously artefacts correction without significant degradation, showcasing the model's robustness in real-world settings.",
        "url": "http://arxiv.org/abs/2511.23274v1",
        "published_date": "2025-11-28T15:25:24+00:00",
        "updated_date": "2025-11-28T15:25:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "physics.med-ph"
        ],
        "authors": [
            "Georgia Kanli",
            "Daniele Perlo",
            "Selma Boudissa",
            "Radovan Jirik",
            "Olivier Keunen"
        ],
        "tldr": "This paper introduces USArt, a deep learning model that simultaneously improves image quality and corrects artifacts in accelerated MRI, specifically tailored for 2D brain anatomical images with Cartesian sampling, achieving up to 5x acceleration.",
        "tldr_zh": "本文介绍了一种名为USArt的深度学习模型，该模型可以同时提高加速MRI的图像质量并校正伪影，特别是针对采用笛卡尔采样的2D脑部解剖图像，实现了高达5倍的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism",
        "summary": "Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.",
        "url": "http://arxiv.org/abs/2511.23113v1",
        "published_date": "2025-11-28T11:55:46+00:00",
        "updated_date": "2025-11-28T11:55:46+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Siqi Chen",
            "Ke Hong",
            "Tianchen Zhao",
            "Ruiqi Xie",
            "Zhenhua Zhu",
            "Xudong Zhang",
            "Yu Wang"
        ],
        "tldr": "This paper introduces db-SP, a sparsity-aware sequence parallelism technique for accelerating sparse attention in visual generative models, achieving improved performance compared to existing methods.",
        "tldr_zh": "该论文介绍了db-SP，一种稀疏感知序列并行技术，用于加速视觉生成模型中的稀疏注意力机制，并实现了比现有方法更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DenoiseGS: Gaussian Reconstruction Model for Burst Denoising",
        "summary": "Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs. In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising. Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details. To this end, we propose a Gaussian self-consistency (GSC) loss, which regularizes the geometry predicted from noisy inputs with high-quality Gaussian point clouds. These point clouds are generated from clean inputs by the same model that we are training, thereby alleviating potential bias or domain gaps. Additionally, we introduce a log-weighted frequency (LWF) loss to strengthen supervision within the spectral domain, effectively preserving fine-grained details. The LWF loss adaptively weights frequency discrepancies in a logarithmic manner, emphasizing challenging high-frequency details. Extensive experiments demonstrate that DenoiseGS significantly exceeds the state-of-the-art NeRF-based methods on both burst denoising and novel view synthesis under noisy conditions, while achieving \\textbf{250$\\times$} faster inference speed. Code and models are released at https://github.com/yscheng04/DenoiseGS.",
        "url": "http://arxiv.org/abs/2511.22939v1",
        "published_date": "2025-11-28T07:29:54+00:00",
        "updated_date": "2025-11-28T07:29:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongsen Cheng",
            "Yuanhao Cai",
            "Yulun Zhang"
        ],
        "tldr": "DenoiseGS introduces a 3D Gaussian Splatting-based framework for burst denoising, achieving significant speed improvements and state-of-the-art performance by using a Gaussian self-consistency loss and a log-weighted frequency loss.",
        "tldr_zh": "DenoiseGS 提出了一个基于 3D Gaussian Splatting 的 burst 降噪框架，通过使用 Gaussian 自洽性损失和对数加权频率损失，显著提高了速度并实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MammoRGB: Dual-View Mammogram Synthesis Using Denoising Diffusion Probabilistic Models",
        "summary": "Purpose: This study aims to develop and evaluate a three channel denoising diffusion probabilistic model (DDPM) for synthesizing single breast dual view mammograms and to assess the impact of channel representations on image fidelity and cross view consistency. Materials and Methods: A pretrained three channel DDPM, sourced from Hugging Face, was fine tuned on a private dataset of 11020 screening mammograms to generate paired craniocaudal (CC) and mediolateral oblique (MLO) views. Three third channel encodings of the CC and MLO views were evaluated: sum, absolute difference, and zero channel. Each model produced 500 synthetic image pairs. Quantitative assessment involved breast mask segmentation using Intersection over Union (IoU) and Dice Similarity Coefficient (DSC), with distributional comparisons against 2500 real pairs using Earth Movers Distance (EMD) and Kolmogorov Smirnov (KS) tests. Qualitative evaluation included a visual Turing test by a non expert radiologist to assess cross view consistency and artifacts. Results: Synthetic mammograms showed IoU and DSC distributions comparable to real images, with EMD and KS values (0.020 and 0.077 respectively). Models using sum or absolute difference encodings outperformed others in IoU and DSC (p < 0.001), though distributions remained broadly similar. Generated CC and MLO views maintained cross view consistency, with 6 to 8 percent of synthetic images exhibiting artifacts consistent with those in the training data. Conclusion: Three channel DDPMs can generate realistic and anatomically consistent dual view mammograms with promising applications in dataset augmentation.",
        "url": "http://arxiv.org/abs/2511.22759v1",
        "published_date": "2025-11-27T21:10:36+00:00",
        "updated_date": "2025-11-27T21:10:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jorge Alberto Garza-Abdala",
            "Gerardo A. Fumagal-González",
            "Daly Avendano",
            "Servando Cardona",
            "Sadam Hussain",
            "Eduardo de Avila-Armenta",
            "Jasiel H. Toscano-Martínez",
            "Diana S. M. Rosales Gurmendi",
            "Alma A. Pedro-Pérez",
            "Jose Gerardo Tamez-Pena"
        ],
        "tldr": "This paper explores the use of a three-channel denoising diffusion probabilistic model (DDPM) to synthesize dual-view mammograms, demonstrating realistic and anatomically consistent results for potential dataset augmentation.",
        "tldr_zh": "本文探讨了使用三通道去噪扩散概率模型 (DDPM) 来合成双视图乳房X光片，展示了逼真且解剖学上一致的结果，可用于潜在的数据集增强。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
        "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
        "url": "http://arxiv.org/abs/2511.23386v1",
        "published_date": "2025-11-28T17:26:34+00:00",
        "updated_date": "2025-11-28T17:26:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sinan Du",
            "Jiahao Guo",
            "Bo Li",
            "Shuhao Cui",
            "Zhengzhuo Xu",
            "Yifu Luo",
            "Yongxian Wei",
            "Kun Gai",
            "Xinggang Wang",
            "Kai Wu",
            "Chun Yuan"
        ],
        "tldr": "The paper introduces VQRAE, a novel Vector Quantization Representation Autoencoder that unifies multimodal understanding, generation, and reconstruction within a single tokenizer, achieving competitive performance on various benchmarks.",
        "tldr_zh": "该论文介绍了VQRAE，一种新型的向量量化表示自编码器，它在单个tokenizer中统一了多模态理解、生成和重建，并在各种基准测试中取得了具有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories",
        "summary": "Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.",
        "url": "http://arxiv.org/abs/2511.23342v1",
        "published_date": "2025-11-28T16:50:08+00:00",
        "updated_date": "2025-11-28T16:50:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinxi Zhang",
            "Shiwei Tan",
            "Quang Nguyen",
            "Quan Dao",
            "Ligong Han",
            "Xiaoxiao He",
            "Tunyu Zhang",
            "Alen Mrdovic",
            "Dimitris Metaxas"
        ],
        "tldr": "The paper introduces Rectified MeanFlow, a novel approach to one-step generative modeling that combines the benefits of Rectified Flow and MeanFlow, achieving improved sample quality and training efficiency, particularly on high-resolution ImageNet datasets.",
        "tldr_zh": "该论文介绍了一种名为 Rectified MeanFlow 的新颖的单步生成模型方法，它结合了 Rectified Flow 和 MeanFlow 的优点，从而提高了样本质量和训练效率，尤其是在高分辨率 ImageNet 数据集上。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DNA-Prior: Unsupervised Denoise Anything via Dual-Domain Prior",
        "summary": "Medical imaging pipelines critically rely on robust denoising to stabilise downstream tasks such as segmentation and reconstruction. However, many existing denoisers depend on large annotated datasets or supervised learning, which restricts their usability in clinical environments with heterogeneous modalities and limited ground-truth data. To address this limitation, we introduce DNA-Prior, a universal unsupervised denoising framework that reconstructs clean images directly from corrupted observations through a mathematically principled hybrid prior. DNA-Prior integrates (i) an implicit architectural prior, enforced through a deep network parameterisation, with (ii) an explicit spectral-spatial prior composed of a frequency-domain fidelity term and a spatial regularisation functional. This dual-domain formulation yields a well-structured optimisation problem that jointly preserves global frequency characteristics and local anatomical structure, without requiring any external training data or modality-specific tuning. Experiments across multiple modalities show that DNA achieves consistent noise suppression and structural preservation under diverse noise conditions.",
        "url": "http://arxiv.org/abs/2511.23124v1",
        "published_date": "2025-11-28T12:15:27+00:00",
        "updated_date": "2025-11-28T12:15:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanqi Cheng",
            "Chun-Wun Cheng",
            "Jim Denholm",
            "Thiago Lima",
            "Javier A. Montoya-Zegarra",
            "Richard Goodwin",
            "Carola-Bibiane Schönlieb",
            "Angelica I Aviles-Rivero"
        ],
        "tldr": "The paper introduces DNA-Prior, an unsupervised denoising framework for medical images that uses a hybrid prior combining implicit architectural and explicit spectral-spatial constraints, achieving denoising without training data or modality-specific tuning.",
        "tldr_zh": "该论文介绍了一种名为DNA-Prior的无监督医学图像去噪框架，该框架使用混合先验，结合了隐式架构先验和显式频谱-空间约束，无需训练数据或特定模态调整即可实现去噪。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Guiding Visual Autoregressive Models through Spectrum Weakening",
        "summary": "Classifier-free guidance (CFG) has become a widely adopted and practical approach for enhancing generation quality and improving condition alignment. Recent studies have explored guidance mechanisms for unconditional generation, yet these approaches remain fundamentally tied to assumptions specific to diffusion models. In this work, we propose a spectrum-weakening framework for visual autoregressive (AR) models. This method works without the need for re-training, specific conditions, or any architectural modifications. It achieves this by constructing a controllable weak model in the spectral domain. We theoretically show that invertible spectral transformations preserve information, while selectively retaining only a subset of spectrum introduces controlled information reduction. Based on this insight, we perform spectrum selection along the channel dimension of internal representations, which avoids the structural constraints imposed by diffusion models. We further introduce two spectrum renormalization strategies that ensures numerical stability during the weakening process. Extensive experiments were conducted on both discrete and continuous AR models, with text or class conditioning. The results demonstrate that our method enables high-quality unconditional generation while maintaining strong prompt alignment for conditional generation.",
        "url": "http://arxiv.org/abs/2511.22991v1",
        "published_date": "2025-11-28T08:52:50+00:00",
        "updated_date": "2025-11-28T08:52:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaoyang Wang",
            "Tianmeng Yang",
            "Jingdong Wang",
            "Yunhai Tong"
        ],
        "tldr": "This paper introduces a spectrum-weakening framework for visual autoregressive models that enhances generation quality and condition alignment without retraining or architectural modifications by controlling information reduction in the spectral domain.",
        "tldr_zh": "本文提出了一种用于视觉自回归模型的频谱弱化框架，通过在频谱域中控制信息减少，无需重新训练或修改架构即可提高生成质量和条件对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]