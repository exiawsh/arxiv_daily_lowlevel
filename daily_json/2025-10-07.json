[
    {
        "title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery",
        "summary": "This paper presents a novel approach for enabling robust robotic perception\nin dark environments using infrared (IR) stream. IR stream is less susceptible\nto noise than RGB in low-light conditions. However, it is dominated by active\nemitter patterns that hinder high-level tasks such as object detection,\ntracking and localisation. To address this, a U-Net-based architecture is\nproposed that reconstructs clean IR images from emitter-populated input,\nimproving both image quality and downstream robotic performance. This approach\noutperforms existing enhancement techniques and enables reliable operation of\nvision-driven robotic systems across illumination conditions from well-lit to\nextreme low-light scenes.",
        "url": "http://arxiv.org/abs/2510.04883v1",
        "published_date": "2025-10-06T15:04:56+00:00",
        "updated_date": "2025-10-06T15:04:56+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Nathan Shankar",
            "Pawel Ladosz",
            "Hujun Yin"
        ],
        "tldr": "The paper introduces a U-Net based method to reconstruct clean infrared (IR) images from noisy, emitter-populated IR streams, enhancing robotic perception in low-light conditions.",
        "tldr_zh": "该论文提出了一种基于U-Net的方法，用于从嘈杂的、充满发射器的红外(IR)流中重建干净的红外图像，从而增强了机器人在弱光条件下的感知能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization",
        "summary": "Visual autoregressive (AR) generation offers a promising path toward unifying\nvision and language models, yet its performance remains suboptimal against\ndiffusion models. Prior work often attributes this gap to tokenizer limitations\nand rasterization ordering. In this work, we identify a core bottleneck from\nthe perspective of generator-tokenizer inconsistency, i.e., the AR-generated\ntokens may not be well-decoded by the tokenizer. To address this, we propose\nreAR, a simple training strategy introducing a token-wise regularization\nobjective: when predicting the next token, the causal transformer is also\ntrained to recover the visual embedding of the current token and predict the\nembedding of the target token under a noisy context. It requires no changes to\nthe tokenizer, generation order, inference pipeline, or external models.\nDespite its simplicity, reAR substantially improves performance. On ImageNet,\nit reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard\nrasterization-based tokenizer. When applied to advanced tokenizers, it achieves\na gFID of 1.42 with only 177M parameters, matching the performance with larger\nstate-of-the-art diffusion models (675M).",
        "url": "http://arxiv.org/abs/2510.04450v1",
        "published_date": "2025-10-06T02:48:13+00:00",
        "updated_date": "2025-10-06T02:48:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiyuan He",
            "Yicong Li",
            "Haotian Ye",
            "Jinghao Wang",
            "Xinyao Liao",
            "Pheng-Ann Heng",
            "Stefano Ermon",
            "James Zou",
            "Angela Yao"
        ],
        "tldr": "The paper proposes a novel training strategy, reAR, for visual autoregressive models that improves generator-tokenizer consistency, leading to significant performance gains in image generation without modifying the tokenizer or inference pipeline.",
        "tldr_zh": "该论文提出了一种新的视觉自回归模型训练策略 reAR，通过提高生成器-分词器的一致性，在不修改分词器或推理流程的情况下，显著提高了图像生成性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning",
        "summary": "Blind face restoration (BFR) has attracted increasing attention with the rise\nof generative methods. Most existing approaches integrate generative priors\ninto the restoration pro- cess, aiming to jointly address facial detail\ngeneration and identity preservation. However, these methods often suffer from\na trade-off between visual quality and identity fidelity, leading to either\nidentity distortion or suboptimal degradation removal. In this paper, we\npresent CodeFormer++, a novel framework that maximizes the utility of\ngenerative priors for high-quality face restoration while preserving identity.\nWe decompose BFR into three sub-tasks: (i) identity- preserving face\nrestoration, (ii) high-quality face generation, and (iii) dynamic fusion of\nidentity features with realistic texture details. Our method makes three key\ncontributions: (1) a learning-based deformable face registration module that\nsemantically aligns generated and restored faces; (2) a texture guided\nrestoration network to dynamically extract and transfer the texture of\ngenerated face to boost the quality of identity-preserving restored face; and\n(3) the integration of deep metric learning for BFR with the generation of\ninformative positive and hard negative samples to better fuse identity-\npreserving and generative features. Extensive experiments on real-world and\nsynthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves\nsuperior performance in terms of both visual fidelity and identity consistency.",
        "url": "http://arxiv.org/abs/2510.04410v1",
        "published_date": "2025-10-06T00:53:50+00:00",
        "updated_date": "2025-10-06T00:53:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Venkata Bharath Reddy Reddem",
            "Akshay P Sarashetti",
            "Ranjith Merugu",
            "Amit Satish Unde"
        ],
        "tldr": "CodeFormer++ introduces a novel framework for blind face restoration that balances visual quality and identity preservation by using deformable registration, texture-guided restoration, and deep metric learning to fuse generative priors with identity-preserving features.",
        "tldr_zh": "CodeFormer++ 提出了一种新颖的盲人脸部修复框架，通过使用可变形配准、纹理引导修复和深度度量学习来融合生成先验与身份保持特征，从而平衡了视觉质量和身份保持。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis",
        "summary": "Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in\nenabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment\nprecision while reducing patient radiation exposure. To address this task, we\nadopt a fully 3D Flow Matching (FM) framework, motivated by recent work\ndemonstrating FM's efficiency in producing high-quality images. In our\napproach, a Gaussian noise volume is transformed into an sCT image by\nintegrating a learned FM velocity field, conditioned on features extracted from\nthe input MRI or CBCT using a lightweight 3D encoder. We evaluated the method\non the SynthRAD2025 Challenge benchmark, training separate models for MRI\n$\\rightarrow$ sCT and CBCT $\\rightarrow$ sCT across three anatomical regions:\nabdomen, head and neck, and thorax. Validation and testing were performed\nthrough the challenge submission system. The results indicate that the method\naccurately reconstructs global anatomical structures; however, preservation of\nfine details was limited, primarily due to the relatively low training\nresolution imposed by memory and runtime constraints. Future work will explore\npatch-based training and latent-space flow models to improve resolution and\nlocal structural fidelity.",
        "url": "http://arxiv.org/abs/2510.04823v1",
        "published_date": "2025-10-06T14:07:03+00:00",
        "updated_date": "2025-10-06T14:07:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Arnela Hadzic",
            "Simon Johannes Joham",
            "Martin Urschler"
        ],
        "tldr": "This paper proposes a 3D Flow Matching framework for generating synthetic CT images from MRI or CBCT scans, achieving accurate reconstruction of global anatomical structures but with limited fine detail preservation due to resolution constraints.",
        "tldr_zh": "本文提出了一种基于 3D Flow Matching 的框架，用于从 MRI 或 CBCT 扫描生成合成 CT 图像。该方法能够准确重建全局解剖结构，但由于分辨率限制，对细节的保留有限。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling",
        "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.",
        "url": "http://arxiv.org/abs/2510.04533v1",
        "published_date": "2025-10-06T06:53:29+00:00",
        "updated_date": "2025-10-06T06:53:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyunmin Cho",
            "Donghoon Ahn",
            "Susung Hong",
            "Jee Eun Kim",
            "Seungryong Kim",
            "Kyong Hwan Jin"
        ],
        "tldr": "The paper introduces Tangential Amplifying Guidance (TAG), a computationally efficient, plug-and-play method that directly guides diffusion model sampling trajectories to reduce hallucinations without architectural modifications.",
        "tldr_zh": "本文介绍了一种名为切向放大引导 (TAG) 的方法，它是一种计算效率高、即插即用的方法，可以直接引导扩散模型采样轨迹，以减少幻觉，而无需修改架构。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model",
        "summary": "We propose a new image denoising model based on a variable-growth total\nvariation regularization of double-phase type with adaptive weight. It is\ndesigned to reduce staircasing with respect to the classical\nRudin--Osher--Fatemi model, while preserving the edges of the image in a\nsimilar fashion. We implement the model and test its performance on synthetic\nand natural images in 1D and 2D over a range of noise levels.",
        "url": "http://arxiv.org/abs/2510.04382v1",
        "published_date": "2025-10-05T22:26:06+00:00",
        "updated_date": "2025-10-05T22:26:06+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.NA",
            "math.NA"
        ],
        "authors": [
            "Wojciech Górny",
            "Michał Łasica",
            "Alexandros Matsoukas"
        ],
        "tldr": "This paper introduces a new image denoising model based on adaptive double-phase total variation regularization, aiming to reduce staircasing artifacts while preserving edges, and its performance is validated on synthetic and natural images.",
        "tldr_zh": "本文提出了一种新的图像去噪模型，该模型基于自适应双相全变分正则化，旨在减少阶梯效应，同时保留边缘，并在合成图像和自然图像上验证了其性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]