[
    {
        "title": "FedVLM: Scalable Personalized Vision-Language Models through Federated Learning",
        "summary": "Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot\nlearning capabilities, making them essential for several downstream tasks.\nHowever, fine-tuning these models at scale remains challenging, particularly in\nfederated environments where data is decentralized and non-iid across clients.\nExisting parameter-efficient tuning methods like LoRA (Low-Rank Adaptation)\nreduce computational overhead but struggle with heterogeneous client data,\nleading to suboptimal generalization. To address these challenges, we propose\nFedVLM, a federated LoRA fine-tuning framework that enables decentralized\nadaptation of VLMs while preserving model privacy and reducing reliance on\ncentralized training. To further tackle data heterogeneity, we introduce\npersonalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each\nclient's unique data distribution, significantly improving local adaptation\nwhile maintaining global model aggregation. Experiments on the RLAIF-V dataset\nshow that pLoRA improves client-specific performance by 24.5% over standard\nLoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a\nscalable and efficient solution for fine-tuning VLMs in federated settings,\nadvancing personalized adaptation in distributed learning scenarios.",
        "url": "http://arxiv.org/abs/2507.17088v1",
        "published_date": "2025-07-23T00:05:02+00:00",
        "updated_date": "2025-07-23T00:05:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Arkajyoti Mitra",
            "Afia Anjum",
            "Paul Agbaje",
            "Mert Pesé",
            "Habeeb Olufowobi"
        ],
        "tldr": "The paper introduces FedVLM, a federated learning framework with personalized LoRA (pLoRA) to fine-tune VLMs in decentralized, non-iid settings, achieving significant performance gains over standard LoRA.",
        "tldr_zh": "该论文介绍了FedVLM，一个具有个性化LoRA（pLoRA）的联邦学习框架，用于在分散的非独立同分布环境中微调VLM，与标准LoRA相比，实现了显著的性能提升。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings",
        "summary": "Multimodal learning plays a critical role in e-commerce recommendation\nplatforms today, enabling accurate recommendations and product understanding.\nHowever, existing vision-language models, such as CLIP, face key challenges in\ne-commerce recommendation systems: 1) Weak object-level alignment, where global\nimage embeddings fail to capture fine-grained product attributes, leading to\nsuboptimal retrieval performance; 2) Ambiguous textual representations, where\nproduct descriptions often lack contextual clarity, affecting cross-modal\nmatching; and 3) Domain mismatch, as generic vision-language models may not\ngeneralize well to e-commerce-specific data. To address these limitations, we\npropose a framework, VL-CLIP, that enhances CLIP embeddings by integrating\nVisual Grounding for fine-grained visual understanding and an LLM-based agent\nfor generating enriched text embeddings. Visual Grounding refines image\nrepresentations by localizing key products, while the LLM agent enhances\ntextual features by disambiguating product descriptions. Our approach\nsignificantly improves retrieval accuracy, multimodal retrieval effectiveness,\nand recommendation quality across tens of millions of items on one of the\nlargest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by\n15.5%, and GMV by 4.0%. Additional experimental results show that our framework\noutperforms vision-language models, including CLIP, FashionCLIP, and GCL, in\nboth precision and semantic alignment, demonstrating the potential of combining\nobject-aware visual grounding and LLM-enhanced text representation for robust\nmultimodal recommendations.",
        "url": "http://arxiv.org/abs/2507.17080v1",
        "published_date": "2025-07-22T23:45:43+00:00",
        "updated_date": "2025-07-22T23:45:43+00:00",
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ramin Giahi",
            "Kehui Yao",
            "Sriram Kollipara",
            "Kai Zhao",
            "Vahid Mirjalili",
            "Jianpeng Xu",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "tldr": "The paper introduces VL-CLIP, a framework that enhances CLIP embeddings for e-commerce recommendations by integrating visual grounding and LLM-augmented text embeddings, resulting in significant improvements in retrieval accuracy and recommendation quality on a large e-commerce platform.",
        "tldr_zh": "该论文介绍了VL-CLIP，一个通过整合视觉定位和LLM增强的文本嵌入来增强CLIP嵌入的框架，用于电子商务推荐，并在大型电子商务平台上显著提高了检索准确率和推荐质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems",
        "summary": "Large language models (LLMs) are growingly extended to process multimodal\ndata such as text and video simultaneously. Their remarkable performance in\nunderstanding what is shown in images is surpassing specialized neural networks\n(NNs) such as Yolo that is supporting only a well-formed but very limited\nvocabulary, ie., objects that they are able to detect. When being\nnon-restricted, LLMs and in particular state-of-the-art vision language models\n(VLMs) show impressive performance to describe even complex traffic situations.\nThis is making them potentially suitable components for automotive perception\nsystems to support the understanding of complex traffic situations or edge case\nsituation. However, LLMs and VLMs are prone to hallucination, which mean to\neither potentially not seeing traffic agents such as vulnerable road users who\nare present in a situation, or to seeing traffic agents who are not there in\nreality. While the latter is unwanted making an ADAS or autonomous driving\nsystems (ADS) to unnecessarily slow down, the former could lead to disastrous\ndecisions from an ADS. In our work, we are systematically assessing the\nperformance of 3 state-of-the-art VLMs on a diverse subset of traffic\nsituations sampled from the Waymo Open Dataset to support safety guardrails for\ncapturing such hallucinations in VLM-supported perception systems. We observe\nthat both, proprietary and open VLMs exhibit remarkable image understanding\ncapabilities even paying thorough attention to fine details sometimes difficult\nto spot for us humans. However, they are also still prone to making up elements\nin their descriptions to date requiring hallucination detection strategies such\nas BetterCheck that we propose in our work.",
        "url": "http://arxiv.org/abs/2507.17722v1",
        "published_date": "2025-07-23T17:32:17+00:00",
        "updated_date": "2025-07-23T17:32:17+00:00",
        "categories": [
            "cs.CV",
            "I.4.m"
        ],
        "authors": [
            "Malsha Ashani Mahawatta Dona",
            "Beatriz Cabrero-Daniel",
            "Yinan Yu",
            "Christian Berger"
        ],
        "tldr": "This paper evaluates the performance of state-of-the-art Vision Language Models (VLMs) on traffic situations from the Waymo Open Dataset, identifying and addressing hallucination issues to ensure safety in automotive perception systems.",
        "tldr_zh": "本文评估了最先进的视觉语言模型（VLM）在 Waymo 开放数据集中交通场景的表现，识别并解决了幻觉问题，以确保汽车感知系统的安全性。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back",
        "summary": "Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches.",
        "url": "http://arxiv.org/abs/2507.18661v2",
        "published_date": "2025-07-23T16:58:44+00:00",
        "updated_date": "2025-07-28T04:30:19+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ruixing Zhang",
            "Yang Zhang",
            "Tongyu Zhu",
            "Leilei Sun",
            "Weifeng Lv"
        ],
        "tldr": "This paper proposes a VLM-based next location prediction model that incorporates visual map reasoning through supervised fine-tuning and reinforcement learning, achieving state-of-the-art performance and superior cross-city generalization.",
        "tldr_zh": "本文提出了一种基于视觉语言模型的下一位置预测模型，该模型通过监督微调和强化学习结合了视觉地图推理，实现了最先进的性能和卓越的跨城市泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering",
        "summary": "Multimodal Large Language Models (MLLMs) have pushed the frontiers of\nKnowledge-Based Visual Question Answering (KBVQA), yet their reasoning is\nfundamentally bottlenecked by a reliance on uni-dimensional evidence. This\n\"seeing only the trees, but not the forest\" approach prevents robust,\nmulti-faceted understanding. Inspired by the principle of seeing both the\nforest and trees, we propose Synergos-VQA, a novel synergistic reasoning\nframework. At its core, Synergos-VQA concurrently generates and fuses three\ncomplementary evidence streams at inference time: (1) Holistic Evidence to\nperceive the entire scene (the \"forest\"), (2) Structural Evidence from a\nprototype-driven module to identify key objects (the \"trees\"), and (3) Causal\nEvidence from a counterfactual probe to ensure the reasoning is robustly\ngrounded. By synergistically fusing this multi-faceted evidence, our framework\nachieves a more comprehensive and reliable reasoning process. Extensive\nexperiments show that Synergos-VQA decisively establishes a new\nstate-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.\nFurthermore, our approach demonstrates strong plug-and-play capabilities,\nsignificantly boosting various open-source MLLMs and proving that superior\nmethodological design can outperform sheer model scale.",
        "url": "http://arxiv.org/abs/2507.17659v1",
        "published_date": "2025-07-23T16:24:57+00:00",
        "updated_date": "2025-07-23T16:24:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junjie Wang",
            "Yunhan Tang",
            "Yijie Wang",
            "Zhihao Yuan",
            "Huan Wang",
            "Yangfan He",
            "Bin Li"
        ],
        "tldr": "The paper introduces Synergos-VQA, a synergistic reasoning framework for Knowledge-Based Visual Question Answering that fuses holistic, structural, and causal evidence streams, achieving state-of-the-art results and demonstrating plug-and-play capabilities with various MLLMs.",
        "tldr_zh": "该论文介绍了 Synergos-VQA，一个用于基于知识的视觉问题回答的协同推理框架，它融合了整体、结构和因果证据流，实现了最先进的结果，并展示了与各种 MLLM 的即插即用能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning",
        "summary": "Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.",
        "url": "http://arxiv.org/abs/2507.17539v1",
        "published_date": "2025-07-23T14:19:30+00:00",
        "updated_date": "2025-07-23T14:19:30+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Xinyao Liu",
            "Diping Song"
        ],
        "tldr": "This paper introduces FundusExpert, an ophthalmology-specific MLLM, along with the FundusGen dataset, demonstrating state-of-the-art performance in ophthalmic question-answering and zero-shot report generation by integrating positioning-diagnosis reasoning through clinical cognitive chain.",
        "tldr_zh": "该论文介绍了FundusExpert，一种眼科专用多模态大语言模型，以及FundusGen数据集。通过临床认知链整合定位-诊断推理，在眼科问答和零样本报告生成方面表现出最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation",
        "summary": "To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.",
        "url": "http://arxiv.org/abs/2507.17520v1",
        "published_date": "2025-07-23T13:57:06+00:00",
        "updated_date": "2025-07-23T13:57:06+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Shuai Yang",
            "Hao Li",
            "Yilun Chen",
            "Bin Wang",
            "Yang Tian",
            "Tai Wang",
            "Hanqing Wang",
            "Feng Zhao",
            "Yiyi Liao",
            "Jiangmiao Pang"
        ],
        "tldr": "The paper introduces InstructVLA, a vision-language-action model trained with a novel instruction tuning paradigm, achieving state-of-the-art manipulation performance while preserving VLM reasoning capabilities and demonstrating strong generalization and real-world applicability.",
        "tldr_zh": "该论文介绍了InstructVLA，一个通过新的指令调整范式训练的视觉-语言-动作模型，在保持VLM推理能力的同时，实现了最先进的操作性能，并展示了强大的泛化能力和现实世界的适用性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection",
        "summary": "Human-Object Interaction (HOI) detection aims to identify humans and objects\nwithin images and interpret their interactions. Existing HOI methods rely\nheavily on large datasets with manual annotations to learn interactions from\nvisual cues. These annotations are labor-intensive to create, prone to\ninconsistency, and limit scalability to new domains and rare interactions. We\nargue that recent advances in Vision-Language Models (VLMs) offer untapped\npotential, particularly in enhancing interaction representation. While prior\nwork has injected such potential and even proposed training-free methods, there\nremain key gaps. Consequently, we propose a novel training-free HOI detection\nframework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively\nutilizes textual and visual interaction representations within a multimodal\nregistry, enabling robust and nuanced interaction understanding. This registry\nincorporates a small set of visual cues and uses innovative interaction\nsignatures to improve the semantic alignment of verbs, facilitating effective\ngeneralization to rare interactions. Additionally, we propose a unique\nmulti-head attention mechanism that adaptively weights the contributions of the\nvisual and textual features. Experimental results demonstrate that our DYSCO\nsurpasses training-free state-of-the-art models and is competitive with\ntraining-based approaches, particularly excelling in rare interactions. Code is\navailable at https://github.com/francescotonini/dysco.",
        "url": "http://arxiv.org/abs/2507.17456v1",
        "published_date": "2025-07-23T12:30:19+00:00",
        "updated_date": "2025-07-23T12:30:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Francesco Tonini",
            "Lorenzo Vaquero",
            "Alessandro Conti",
            "Cigdem Beyan",
            "Elisa Ricci"
        ],
        "tldr": "This paper introduces DYSCO, a training-free HOI detection framework leveraging VLMs and a multimodal registry with interaction signatures and a multi-head attention mechanism, achieving competitive performance, especially on rare interactions.",
        "tldr_zh": "本文介绍了一种名为DYSCO的无需训练的HOI检测框架，该框架利用视觉语言模型（VLM）和一个多模态注册表，其中包含交互签名和一个多头注意力机制，实现了具有竞争力的性能，尤其是在罕见交互方面。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs",
        "summary": "Video Anomaly Detection (VAD) aims to identify and locate deviations from\nnormal patterns in video sequences. Traditional methods often struggle with\nsubstantial computational demands and a reliance on extensive labeled datasets,\nthereby restricting their practical applicability. To address these\nconstraints, we propose HiProbe-VAD, a novel framework that leverages\npre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring\nfine-tuning. In this paper, we discover that the intermediate hidden states of\nMLLMs contain information-rich representations, exhibiting higher sensitivity\nand linear separability for anomalies compared to the output layer. To\ncapitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)\nmechanism that intelligently identifies and extracts the most informative\nhidden states from the optimal intermediate layer during the MLLMs reasoning.\nThen a lightweight anomaly scorer and temporal localization module efficiently\ndetects anomalies using these extracted hidden states and finally generate\nexplanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate\nthat HiProbe-VAD outperforms existing training-free and most traditional\napproaches. Furthermore, our framework exhibits remarkable cross-model\ngeneralization capabilities in different MLLMs without any tuning, unlocking\nthe potential of pre-trained MLLMs for video anomaly detection and paving the\nway for more practical and scalable solutions.",
        "url": "http://arxiv.org/abs/2507.17394v1",
        "published_date": "2025-07-23T10:41:46+00:00",
        "updated_date": "2025-07-23T10:41:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhaolin Cai",
            "Fan Li",
            "Ziwei Zheng",
            "Yanjun Qin"
        ],
        "tldr": "This paper introduces HiProbe-VAD, a novel tuning-free framework for video anomaly detection using MLLMs by probing intermediate hidden states, achieving superior performance and cross-model generalization.",
        "tldr_zh": "本文介绍了一种名为HiProbe-VAD的新型免调优视频异常检测框架，该框架利用MLLM通过探测中间隐藏状态来进行异常检测，实现了卓越的性能和跨模型泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model",
        "summary": "Multimodal large language models (MLLMs) have emerged as powerful tools for\ncomputational pathology, offering unprecedented opportunities to integrate\npathological images with language context for comprehensive diagnostic\nanalysis. These models hold particular promise for automating complex tasks\nthat traditionally require expert interpretation of pathologists. However,\ncurrent MLLM approaches in pathology demonstrate significantly constrained\nreasoning capabilities, primarily due to their reliance on expensive\nchain-of-thought annotations. Additionally, existing methods remain limited to\nsimplex application of visual question answering (VQA) at region-of-interest\n(ROI) level, failing to address the full spectrum of diagnostic needs such as\nROI classification, detection, segmentation, whole-slide-image (WSI)\nclassification and VQA in clinical practice. In this study, we present\nSmartPath-R1, a versatile MLLM capable of simultaneously addressing both\nROI-level and WSI-level tasks while demonstrating robust pathological reasoning\ncapability. Our framework combines scale-dependent supervised fine-tuning and\ntask-aware reinforcement fine-tuning, which circumvents the requirement for\nchain-of-thought supervision by leveraging the intrinsic knowledge within MLLM.\nFurthermore, SmartPath-R1 integrates multiscale and multitask analysis through\na mixture-of-experts mechanism, enabling dynamic processing for diverse tasks.\nWe curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI\nsamples for training and evaluation. Extensive experiments across 72 tasks\nvalidate the effectiveness and superiority of the proposed approach. This work\nrepresents a significant step toward developing versatile, reasoning-enhanced\nAI systems for precision pathology.",
        "url": "http://arxiv.org/abs/2507.17303v1",
        "published_date": "2025-07-23T08:09:42+00:00",
        "updated_date": "2025-07-23T08:09:42+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zhe Xu",
            "Ziyi Liu",
            "Junlin Hou",
            "Jiabo Ma",
            "Cheng Jin",
            "Yihui Wang",
            "Zhixuan Chen",
            "Zhengyu Zhang",
            "Zhengrui Guo",
            "Fengtao Zhou",
            "Yingxue Xu",
            "Xi Wang",
            "Ronald Cheong Kin Chan",
            "Li Liang",
            "Hao Chen"
        ],
        "tldr": "The paper introduces SmartPath-R1, a versatile multimodal large language model (MLLM) for pathology that addresses both ROI-level and WSI-level tasks with enhanced reasoning, using scale-dependent and task-aware reinforcement fine-tuning. It demonstrates strong performance across numerous pathology tasks.",
        "tldr_zh": "该论文介绍了一种名为SmartPath-R1的多功能多模态大型语言模型（MLLM），用于病理学，通过规模依赖和任务感知的强化微调，解决了ROI级别和WSI级别的任务，并具有增强的推理能力。该模型在众多病理学任务中表现出强大的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training",
        "summary": "Foundation models have recently gained tremendous popularity in medical image\nanalysis. State-of-the-art methods leverage either paired image-text data via\nvision-language pre-training or unpaired image data via self-supervised\npre-training to learn foundation models with generalizable image features to\nboost downstream task performance. However, learning foundation models\nexclusively on either paired or unpaired image data limits their ability to\nlearn richer and more comprehensive image features. In this paper, we\ninvestigate a novel task termed semi-supervised vision-language pre-training,\naiming to fully harness the potential of both paired and unpaired image data\nfor foundation model learning. To this end, we propose MaskedCLIP, a\nsynergistic masked image modeling and contrastive language-image pre-training\nframework for semi-supervised vision-language pre-training. The key challenge\nin combining paired and unpaired image data for learning a foundation model\nlies in the incompatible feature spaces derived from these two types of data.\nTo address this issue, we propose to connect the masked feature space with the\nCLIP feature space with a bridge transformer. In this way, the more semantic\nspecific CLIP features can benefit from the more general masked features for\nsemantic feature extraction. We further propose a masked knowledge distillation\nloss to distill semantic knowledge of original image features in CLIP feature\nspace back to the predicted masked image features in masked feature space. With\nthis mutually interactive design, our framework effectively leverages both\npaired and unpaired image data to learn more generalizable image features for\ndownstream tasks. Extensive experiments on retinal image analysis demonstrate\nthe effectiveness and data efficiency of our method.",
        "url": "http://arxiv.org/abs/2507.17239v1",
        "published_date": "2025-07-23T06:15:54+00:00",
        "updated_date": "2025-07-23T06:15:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lei Zhu",
            "Jun Zhou",
            "Rick Siow Mong Goh",
            "Yong Liu"
        ],
        "tldr": "The paper proposes MaskedCLIP, a semi-supervised vision-language pre-training framework that combines masked image modeling and contrastive language-image pre-training to leverage both paired and unpaired medical image data by bridging incompatible feature spaces with a transformer and knowledge distillation.",
        "tldr_zh": "该论文提出了MaskedCLIP，一种半监督视觉语言预训练框架，它结合了掩码图像建模和对比语言-图像预训练，通过使用Transformer和知识蒸馏桥接不兼容的特征空间，从而利用配对和未配对的医学图像数据。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models",
        "summary": "In this paper, we introduce VideoNarrator, a novel training-free pipeline\ndesigned to generate dense video captions that offer a structured snapshot of\nvideo content. These captions offer detailed narrations with precise\ntimestamps, capturing the nuances present in each segment of the video. Despite\nadvancements in multimodal large language models (MLLMs) for video\ncomprehension, these models often struggle with temporally aligned narrations\nand tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator\naddresses these challenges by leveraging a flexible pipeline where\noff-the-shelf MLLMs and visual-language models (VLMs) can function as caption\ngenerators, context providers, or caption verifiers. Our experimental results\ndemonstrate that the synergistic interaction of these components significantly\nenhances the quality and accuracy of video narrations, effectively reducing\nhallucinations and improving temporal alignment. This structured approach not\nonly enhances video understanding but also facilitates downstream tasks such as\nvideo summarization and video question answering, and can be potentially\nextended for advertising and marketing applications.",
        "url": "http://arxiv.org/abs/2507.17050v1",
        "published_date": "2025-07-22T22:16:37+00:00",
        "updated_date": "2025-07-22T22:16:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tz-Ying Wu",
            "Tahani Trigui",
            "Sharath Nittur Sridhar",
            "Anand Bodas",
            "Subarna Tripathi"
        ],
        "tldr": "The paper introduces VideoNarrator, a training-free pipeline leveraging multimodal LLMs and VLMs for generating accurate and temporally aligned video narrations, which reduces hallucinations and enhances downstream tasks like video summarization and question answering.",
        "tldr_zh": "本文介绍了一种名为VideoNarrator的免训练流程，该流程利用多模态LLM和VLM生成准确且时间对齐的视频叙述，减少了幻觉并增强了视频摘要和问答等下游任务。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding",
        "summary": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.",
        "url": "http://arxiv.org/abs/2507.17047v1",
        "published_date": "2025-07-22T22:09:00+00:00",
        "updated_date": "2025-07-22T22:09:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kuleen Sasse",
            "Efsun Sarioglu Kayi",
            "Arun Reddy"
        ],
        "tldr": "This paper presents a controllable hybrid captioner that combines LaViLa and LLaVA to generate improved long-form video summaries by incorporating both action and scene descriptions, enhancing question answering capabilities with LLMs.",
        "tldr_zh": "本文提出了一种可控的混合字幕生成器，结合了 LaViLa 和 LLaVA，通过整合动作和场景描述来生成改进的长篇视频摘要，从而增强了 LLM 的问答能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dual-branch Prompting for Multimodal Machine Translation",
        "summary": "Multimodal Machine Translation (MMT) typically enhances text-only translation\nby incorporating aligned visual features. Despite the remarkable progress,\nstate-of-the-art MMT approaches often rely on paired image-text inputs at\ninference and are sensitive to irrelevant visual noise, which limits their\nrobustness and practical applicability. To address these issues, we propose\nD2P-MMT, a diffusion-based dual-branch prompting framework for robust\nvision-guided translation. Specifically, D2P-MMT requires only the source text\nand a reconstructed image generated by a pre-trained diffusion model, which\nnaturally filters out distracting visual details while preserving semantic\ncues. During training, the model jointly learns from both authentic and\nreconstructed images using a dual-branch prompting strategy, encouraging rich\ncross-modal interactions. To bridge the modality gap and mitigate\ntraining-inference discrepancies, we introduce a distributional alignment loss\nthat enforces consistency between the output distributions of the two branches.\nExtensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves\nsuperior translation performance compared to existing state-of-the-art\napproaches.",
        "url": "http://arxiv.org/abs/2507.17588v1",
        "published_date": "2025-07-23T15:22:51+00:00",
        "updated_date": "2025-07-23T15:22:51+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Jie Wang",
            "Zhendong Yang",
            "Liansong Zong",
            "Xiaobo Zhang",
            "Dexian Wang",
            "Ji Zhang"
        ],
        "tldr": "This paper introduces D2P-MMT, a diffusion-based dual-branch prompting framework for robust multimodal machine translation that uses reconstructed images from a diffusion model to filter out irrelevant visual noise, improving translation performance.",
        "tldr_zh": "该论文介绍了D2P-MMT，一个基于扩散模型的双分支提示框架，用于鲁棒的多模态机器翻译。该框架使用从扩散模型重建的图像来过滤掉不相关的视觉噪声，从而提高翻译性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls",
        "summary": "This study investigates the extent to which the Visual Entailment (VE) task\nserves as a reliable probe of vision-language understanding in multimodal\nlanguage models, using the LLaMA 3.2 11B Vision model as a test case. Beyond\nreporting performance metrics, we aim to interpret what these results reveal\nabout the underlying possibilities and limitations of the VE task. We conduct a\nseries of experiments across zero-shot, few-shot, and fine-tuning settings,\nexploring how factors such as prompt design, the number and order of in-context\nexamples and access to visual information might affect VE performance. To\nfurther probe the reasoning processes of the model, we used explanation-based\nevaluations. Results indicate that three-shot inference outperforms the\nzero-shot baselines. However, additional examples introduce more noise than\nthey provide benefits. Additionally, the order of the labels in the prompt is a\ncritical factor that influences the predictions. In the absence of visual\ninformation, the model has a strong tendency to hallucinate and imagine\ncontent, raising questions about the model's over-reliance on linguistic\npriors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on\nthe e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.\nAdditionally, the explanation evaluation demonstrates that the fine-tuned model\nprovides semantically meaningful explanations similar to those of humans, with\na BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore\nresults in experiments with limited vision, questioning the visual grounding of\nthis task. Overall, our results highlight both the utility and limitations of\nVE as a diagnostic task for vision-language understanding and point to\ndirections for refining multimodal evaluation methods.",
        "url": "http://arxiv.org/abs/2507.17467v1",
        "published_date": "2025-07-23T12:46:51+00:00",
        "updated_date": "2025-07-23T12:46:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Elena Pitta",
            "Tom Kouwenhoven",
            "Tessa Verhoef"
        ],
        "tldr": "This paper investigates the Visual Entailment (VE) task as a probe for vision-language understanding using LLaMA 3.2 11B Vision, revealing both its utility and limitations with respect to model reliance on linguistic priors and visual grounding.",
        "tldr_zh": "本文以 LLaMA 3.2 11B Vision 模型为例，研究了视觉蕴含 (VE) 任务作为视觉语言理解探针的有效性，揭示了其在模型对语言先验和视觉基础的依赖方面的效用和局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization",
        "summary": "Geo-localization from a single image at planet scale (essentially an advanced\nor extreme version of the kidnapped robot problem) is a fundamental and\nchallenging task in applications such as navigation, autonomous driving and\ndisaster response due to the vast diversity of locations, environmental\nconditions, and scene variations. Traditional retrieval-based methods for\ngeo-localization struggle with scalability and perceptual aliasing, while\nclassification-based approaches lack generalization and require extensive\ntraining data. Recent advances in vision-language models (VLMs) offer a\npromising alternative by leveraging contextual understanding and reasoning.\nHowever, while VLMs achieve high accuracy, they are often prone to\nhallucinations and lack interpretability, making them unreliable as standalone\nsolutions. In this work, we propose a novel hybrid geo-localization framework\nthat combines the strengths of VLMs with retrieval-based visual place\nrecognition (VPR) methods. Our approach first leverages a VLM to generate a\nprior, effectively guiding and constraining the retrieval search space. We then\nemploy a retrieval step, followed by a re-ranking mechanism that selects the\nmost geographically plausible matches based on feature similarity and proximity\nto the initially estimated coordinates. We evaluate our approach on multiple\ngeo-localization benchmarks and show that it consistently outperforms prior\nstate-of-the-art methods, particularly at street (up to 4.51%) and city level\n(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in\ncombination with VPR lead to scalable, robust, and accurate geo-localization\nsystems.",
        "url": "http://arxiv.org/abs/2507.17455v1",
        "published_date": "2025-07-23T12:23:03+00:00",
        "updated_date": "2025-07-23T12:23:03+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Sania Waheed",
            "Na Min An",
            "Michael Milford",
            "Sarvapali D. Ramchurn",
            "Shoaib Ehsan"
        ],
        "tldr": "This paper presents a hybrid geo-localization framework that combines Vision-Language Models (VLMs) with retrieval-based Visual Place Recognition (VPR) to improve accuracy and robustness, achieving state-of-the-art performance on geo-localization benchmarks.",
        "tldr_zh": "本文提出了一种混合地理定位框架，该框架结合了视觉语言模型（VLM）和基于检索的视觉地点识别（VPR），以提高准确性和鲁棒性，并在地理定位基准测试中实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection",
        "summary": "The Mixture of Experts (MoE) architecture has excelled in Large\nVision-Language Models (LVLMs), yet its potential in real-time open-vocabulary\nobject detectors, which also leverage large-scale vision-language datasets but\nsmaller models, remains unexplored. This work investigates this domain,\nrevealing intriguing insights. In the shallow layers, experts tend to cooperate\nwith diverse peers to expand the search space. While in the deeper layers,\nfixed collaborative structures emerge, where each expert maintains 2-3 fixed\npartners and distinct expert combinations are specialized in processing\nspecific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding\nDINO 1.5 Edge from a dense model to a dynamic inference framework via an\nefficient MoE-Tuning strategy. Additionally, we design a granularity\ndecomposition mechanism to decompose the Feed-Forward Network (FFN) of base\nmodel into multiple smaller expert networks, expanding the subnet search space.\nTo prevent performance degradation at the start of fine-tuning, we further\npropose a pre-trained weight allocation strategy for the experts, coupled with\na specific router initialization. During inference, only the input-relevant\nexperts are activated to form a compact subnet. Experiments show that,\npretrained with merely 1.56M open-source data, Dynamic-DINO outperforms\nGrounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.",
        "url": "http://arxiv.org/abs/2507.17436v1",
        "published_date": "2025-07-23T11:51:06+00:00",
        "updated_date": "2025-07-23T11:51:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yehao Lu",
            "Minghe Weng",
            "Zekang Xiao",
            "Rui Jiang",
            "Wei Su",
            "Guangcong Zheng",
            "Ping Lu",
            "Xi Li"
        ],
        "tldr": "The paper proposes Dynamic-DINO, a Mixture of Experts-based fine-tuning approach for real-time open-vocabulary object detection, showing improved performance over Grounding DINO 1.5 Edge with less training data.",
        "tldr_zh": "该论文提出了Dynamic-DINO，一种基于混合专家模型的微调方法，用于实时开放词汇的目标检测。实验表明，相比于Grounding DINO 1.5 Edge，Dynamic-DINO使用更少的训练数据获得了更好的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VisionTrap: Unanswerable Questions On Visual Data",
        "summary": "Visual Question Answering (VQA) has been a widely studied topic, with\nextensive research focusing on how VLMs respond to answerable questions based\non real-world images. However, there has been limited exploration of how these\nmodels handle unanswerable questions, particularly in cases where they should\nabstain from providing a response. This research investigates VQA performance\non unrealistically generated images or asking unanswerable questions, assessing\nwhether models recognize the limitations of their knowledge or attempt to\ngenerate incorrect answers. We introduced a dataset, VisionTrap, comprising\nthree categories of unanswerable questions across diverse image types: (1)\nhybrid entities that fuse objects and animals, (2) objects depicted in\nunconventional or impossible scenarios, and (3) fictional or non-existent\nfigures. The questions posed are logically structured yet inherently\nunanswerable, testing whether models can correctly recognize their limitations.\nOur findings highlight the importance of incorporating such questions into VQA\nbenchmarks to evaluate whether models tend to answer, even when they should\nabstain.",
        "url": "http://arxiv.org/abs/2507.17262v1",
        "published_date": "2025-07-23T07:00:19+00:00",
        "updated_date": "2025-07-23T07:00:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Asir Saadat",
            "Syem Aziz",
            "Shahriar Mahmud",
            "Abdullah Ibne Masud Mahi",
            "Sabbir Ahmed"
        ],
        "tldr": "This paper introduces the VisionTrap dataset to evaluate VQA models' ability to abstain from answering unanswerable questions involving unrealistically generated images and logically structured yet inherently unanswerable queries, highlighting the need to assess model limitations in VQA benchmarks.",
        "tldr_zh": "本文介绍了一个名为VisionTrap的数据集，用于评估VQA模型在面对包含不真实的生成图像和逻辑结构但本质上无法回答的查询时，是否能避免回答无法回答的问题，强调需要在VQA基准测试中评估模型的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition",
        "summary": "License plate recognition in open environments is widely applicable across\nvarious domains; however, the diversity of license plate types and imaging\nconditions presents significant challenges. To address the limitations\nencountered by CNN and CRNN-based approaches in license plate recognition, this\npaper proposes a unified solution that integrates a lightweight visual encoder\nwith a text decoder, within a pre-training framework tailored for single and\ndouble-line Chinese license plates. To mitigate the scarcity of double-line\nlicense plate datasets, we constructed a single/double-line license plate\ndataset by synthesizing images, applying texture mapping onto real scenes, and\nblending them with authentic license plate images. Furthermore, to enhance the\nsystem's recognition accuracy, we introduce a perspective correction network\n(PTN) that employs license plate corner coordinate regression as an implicit\nvariable, supervised by license plate view classification information. This\nnetwork offers improved stability, interpretability, and low annotation costs.\nThe proposed algorithm achieves an average recognition accuracy of 99.34% on\nthe corrected CCPD test set under coarse localization disturbance. When\nevaluated under fine localization disturbance, the accuracy further improves to\n99.58%. On the double-line license plate test set, it achieves an average\nrecognition accuracy of 98.70%, with processing speeds reaching up to 167\nframes per second, indicating strong practical applicability.",
        "url": "http://arxiv.org/abs/2507.17335v1",
        "published_date": "2025-07-23T09:03:01+00:00",
        "updated_date": "2025-07-23T09:03:01+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Guangzhu Xu",
            "Zhi Ke",
            "Pengcheng Zuo",
            "Bangjun Lei"
        ],
        "tldr": "This paper proposes TransLPRNet, a lightweight vision-language network for recognizing single/dual-line Chinese license plates, incorporating a perspective correction network and achieving high accuracy and speed on various datasets.",
        "tldr_zh": "本文提出TransLPRNet，一个轻量级的视觉-语言网络，用于识别单/双行中文车牌，结合了透视校正网络，并在各种数据集上实现了高精度和速度。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]