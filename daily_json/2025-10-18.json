[
    {
        "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
        "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3\nseries that advances the next frontier of native image generation. BLIP3o-NEXT\nunifies text-to-image generation and image editing within a single\narchitecture, demonstrating strong image generation and image editing\ncapabilities. In developing the state-of-the-art native image generation model,\nwe identify four key insights: (1) Most architectural choices yield comparable\nperformance; an architecture can be deemed effective provided it scales\nefficiently and supports fast inference; (2) The successful application of\nreinforcement learning can further push the frontier of native image\ngeneration; (3) Image editing still remains a challenging task, yet instruction\nfollowing and the consistency between generated and reference images can be\nsignificantly enhanced through post-training and data engine; (4) Data quality\nand scale continue to be decisive factors that determine the upper bound of\nmodel performance. Building upon these insights, BLIP3o-NEXT leverages an\nAutoregressive + Diffusion architecture in which an autoregressive model first\ngenerates discrete image tokens conditioned on multimodal inputs, whose hidden\nstates are then used as conditioning signals for a diffusion model to generate\nhigh-fidelity images. This architecture integrates the reasoning strength and\ninstruction following of autoregressive models with the fine-detail rendering\nability of diffusion models, achieving a new level of coherence and realism.\nExtensive evaluations of various text-to-image and image-editing benchmarks\nshow that BLIP3o-NEXT achieves superior performance over existing models.",
        "url": "http://arxiv.org/abs/2510.15857v1",
        "published_date": "2025-10-17T17:50:58+00:00",
        "updated_date": "2025-10-17T17:50:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiuhai Chen",
            "Le Xue",
            "Zhiyang Xu",
            "Xichen Pan",
            "Shusheng Yang",
            "Can Qin",
            "An Yan",
            "Honglu Zhou",
            "Zeyuan Chen",
            "Lifu Huang",
            "Tianyi Zhou",
            "Junnan Li",
            "Silvio Savarese",
            "Caiming Xiong",
            "Ran Xu"
        ],
        "tldr": "BLIP3o-NEXT is a new open-source foundation model that unifies text-to-image generation and image editing, achieving state-of-the-art performance through an autoregressive + diffusion architecture and careful consideration of data quality and scaling.",
        "tldr_zh": "BLIP3o-NEXT是一个新的开源基础模型，统一了文本到图像的生成和图像编辑，通过自回归+扩散架构以及对数据质量和规模的仔细考虑，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Deep generative priors for 3D brain analysis",
        "summary": "Diffusion models have recently emerged as powerful generative models in\nmedical imaging. However, it remains a major challenge to combine these\ndata-driven models with domain knowledge to guide brain imaging problems. In\nneuroimaging, Bayesian inverse problems have long provided a successful\nframework for inference tasks, where incorporating domain knowledge of the\nimaging process enables robust performance without requiring extensive training\ndata. However, the anatomical modeling component of these approaches typically\nrelies on classical mathematical priors that often fail to capture the complex\nstructure of brain anatomy. In this work, we present the first general-purpose\napplication of diffusion models as priors for solving a wide range of medical\nimaging inverse problems. Our approach leverages a score-based diffusion prior\ntrained extensively on diverse brain MRI data, paired with flexible forward\nmodels that capture common image processing tasks such as super-resolution,\nbias field correction, inpainting, and combinations thereof. We further\ndemonstrate how our framework can refine outputs from existing deep learning\nmethods to improve anatomical fidelity. Experiments on heterogeneous clinical\nand research MRI data show that our method achieves state-of-the-art\nperformance producing consistent, high-quality solutions without requiring\npaired training datasets. These results highlight the potential of diffusion\npriors as versatile tools for brain MRI analysis.",
        "url": "http://arxiv.org/abs/2510.15119v1",
        "published_date": "2025-10-16T20:20:50+00:00",
        "updated_date": "2025-10-16T20:20:50+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ana Lawry Aguila",
            "Dina Zemlyanker",
            "You Cheng",
            "Sudeshna Das",
            "Daniel C. Alexander",
            "Oula Puonti",
            "Annabel Sorby-Adams",
            "W. Taylor Kimberly",
            "Juan Eugenio Iglesias"
        ],
        "tldr": "This paper introduces a method using diffusion models as priors for solving inverse problems in brain MRI analysis, showing state-of-the-art performance in tasks like super-resolution and bias field correction without paired training data.",
        "tldr_zh": "本文提出了一种使用扩散模型作为先验知识来解决脑部MRI分析中逆问题的方法，无需配对训练数据，即可在超分辨率和偏置场校正等任务中实现最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Controlling the image generation process with parametric activation functions",
        "summary": "As image generative models continue to increase not only in their fidelity\nbut also in their ubiquity the development of tools that leverage direct\ninteraction with their internal mechanisms in an interpretable way has received\nlittle attention In this work we introduce a system that allows users to\ndevelop a better understanding of the model through interaction and\nexperimentation By giving users the ability to replace activation functions of\na generative network with parametric ones and a way to set the parameters of\nthese functions we introduce an alternative approach to control the networks\noutput We demonstrate the use of our method on StyleGAN2 and BigGAN networks\ntrained on FFHQ and ImageNet respectively.",
        "url": "http://arxiv.org/abs/2510.15778v1",
        "published_date": "2025-10-17T16:02:23+00:00",
        "updated_date": "2025-10-17T16:02:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ilia Pavlov"
        ],
        "tldr": "The paper introduces a system for controlling image generation by allowing users to replace and parameterize activation functions in generative networks like StyleGAN2 and BigGAN, offering a new approach for understanding and manipulating model outputs.",
        "tldr_zh": "该论文介绍了一个通过允许用户替换和参数化生成网络（如StyleGAN2和BigGAN）中的激活函数来控制图像生成的系统，为理解和操纵模型输出提供了一种新方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion",
        "summary": "We present QSilk, a lightweight, always-on stabilization layer for latent\ndiffusion that improves high-frequency fidelity while suppressing rare\nactivation spikes. QSilk combines (i) a per-sample micro clamp that gently\nlimits extreme values without washing out texture, and (ii) Adaptive Quantile\nClip (AQClip), which adapts the allowed value corridor per region. AQClip can\noperate in a proxy mode using local structure statistics or in an attention\nentropy guided mode (model confidence). Integrated into the CADE 2.5 rendering\npipeline, QSilk yields cleaner, sharper results at low step counts and\nultra-high resolutions with negligible overhead. It requires no training or\nfine-tuning and exposes minimal user controls. We report consistent qualitative\nimprovements across SD/SDXL backbones and show synergy with CFG/Rescale,\nenabling slightly higher guidance without artifacts.",
        "url": "http://arxiv.org/abs/2510.15761v1",
        "published_date": "2025-10-17T15:50:30+00:00",
        "updated_date": "2025-10-17T15:50:30+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "68T07, 68U10",
            "I.2.10; I.4.8; I.4.9"
        ],
        "authors": [
            "Denis Rychkovskiy"
        ],
        "tldr": "QSilk is a training-free stabilization layer for latent diffusion models that improves high-frequency detail and suppresses artifacts by using micro-clamping and adaptive quantile clipping. It shows qualitative improvements on SD/SDXL with negligible overhead.",
        "tldr_zh": "QSilk是一个用于潜在扩散模型的免训练稳定层，通过使用微钳位和自适应分位数裁剪来改善高频细节并抑制伪影。它在SD/SDXL上表现出质的改进，且开销可忽略不计。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration",
        "summary": "Current self-supervised denoising techniques achieve impressive results, yet\ntheir real-world application is frequently constrained by substantial\ncomputational and memory demands, necessitating a compromise between inference\nspeed and reconstruction quality. In this paper, we present an\nultra-lightweight model that addresses this challenge, achieving both fast\ndenoising and high quality image restoration. Built upon the Noise2Noise\ntraining framework-which removes the reliance on clean reference images or\nexplicit noise modeling-we introduce an innovative multistage denoising\npipeline named Noise2Detail (N2D). During inference, this approach disrupts the\nspatial correlations of noise patterns to produce intermediate smooth\nstructures, which are subsequently refined to recapture fine details directly\nfrom the noisy input. Extensive testing reveals that Noise2Detail surpasses\nexisting dataset-free techniques in performance, while requiring only a\nfraction of the computational resources. This combination of efficiency, low\ncomputational cost, and data-free approach make it a valuable tool for\nbiomedical imaging, overcoming the challenges of scarce clean training data-due\nto rare and complex imaging modalities-while enabling fast inference for\npractical use.",
        "url": "http://arxiv.org/abs/2510.15611v1",
        "published_date": "2025-10-17T12:59:21+00:00",
        "updated_date": "2025-10-17T12:59:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tomáš Chobola",
            "Julia A. Schnabel",
            "Tingying Peng"
        ],
        "tldr": "This paper introduces Noise2Detail (N2D), a lightweight, data-free denoising pipeline for biomedical images that achieves high-quality restoration with low computational cost, surpassing existing dataset-free techniques.",
        "tldr_zh": "本文介绍了一种名为 Noise2Detail (N2D) 的轻量级、无数据的生物医学图像去噪管道，该管道以低计算成本实现高质量的图像恢复，超越了现有的无数据集技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Diffusion Model without Variational Autoencoder",
        "summary": "Recent progress in diffusion-based visual generation has largely relied on\nlatent diffusion models with variational autoencoders (VAEs). While effective\nfor high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited\ntraining efficiency, slow inference, and poor transferability to broader vision\ntasks. These issues stem from a key limitation of VAE latent spaces: the lack\nof clear semantic separation and strong discriminative structure. Our analysis\nconfirms that these properties are crucial not only for perception and\nunderstanding tasks, but also for the stable and efficient training of latent\ndiffusion models. Motivated by this insight, we introduce SVG, a novel latent\ndiffusion model without variational autoencoders, which leverages\nself-supervised representations for visual generation. SVG constructs a feature\nspace with clear semantic discriminability by leveraging frozen DINO features,\nwhile a lightweight residual branch captures fine-grained details for\nhigh-fidelity reconstruction. Diffusion models are trained directly on this\nsemantically structured latent space to facilitate more efficient learning. As\na result, SVG enables accelerated diffusion training, supports few-step\nsampling, and improves generative quality. Experimental results further show\nthat SVG preserves the semantic and discriminative capabilities of the\nunderlying self-supervised representations, providing a principled pathway\ntoward task-general, high-quality visual representations.",
        "url": "http://arxiv.org/abs/2510.15301v1",
        "published_date": "2025-10-17T04:17:44+00:00",
        "updated_date": "2025-10-17T04:17:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Minglei Shi",
            "Haolin Wang",
            "Wenzhao Zheng",
            "Ziyang Yuan",
            "Xiaoshi Wu",
            "Xintao Wang",
            "Pengfei Wan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "tldr": "This paper introduces SVG, a latent diffusion model without VAEs that utilizes self-supervised DINO features for efficient training, few-step sampling, and improved image generation quality, addressing limitations of VAE-based latent diffusion models.",
        "tldr_zh": "本文介绍了SVG，一种不使用VAE的潜在扩散模型，它利用自监督的DINO特征来实现高效的训练、少步采样和改进的图像生成质量，从而解决了基于VAE的潜在扩散模型的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
        "summary": "Lens flare significantly degrades image quality, impacting critical computer\nvision tasks like object detection and autonomous driving. Recent Single Image\nFlare Removal (SIFR) methods perform poorly when off-frame light sources are\nincomplete or absent. We propose LightsOut, a diffusion-based outpainting\nframework tailored to enhance SIFR by reconstructing off-frame light sources.\nOur method leverages a multitask regression module and LoRA fine-tuned\ndiffusion model to ensure realistic and physically consistent outpainting\nresults. Comprehensive experiments demonstrate LightsOut consistently boosts\nthe performance of existing SIFR methods across challenging scenarios without\nadditional retraining, serving as a universally applicable plug-and-play\npreprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
        "url": "http://arxiv.org/abs/2510.15868v1",
        "published_date": "2025-10-17T17:59:50+00:00",
        "updated_date": "2025-10-17T17:59:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shr-Ruei Tsai",
            "Wei-Cheng Chang",
            "Jie-Ying Lee",
            "Chih-Hai Su",
            "Yu-Lun Liu"
        ],
        "tldr": "The paper introduces LightsOut, a diffusion-based outpainting framework that reconstructs off-frame light sources to enhance single image flare removal methods as a plug-and-play preprocessing step.",
        "tldr_zh": "该论文介绍了LightsOut，一种基于扩散模型的图像外推框架，通过重建画面外的光源来增强单张图像的镜头光晕去除方法，作为一个即插即用的预处理步骤。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]