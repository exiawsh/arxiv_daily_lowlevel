[
    {
        "title": "Region-based Cluster Discrimination for Visual Representation Learning",
        "summary": "Learning visual representations is foundational for a broad spectrum of\ndownstream tasks. Although recent vision-language contrastive models, such as\nCLIP and SigLIP, have achieved impressive zero-shot performance via large-scale\nvision-language alignment, their reliance on global representations constrains\ntheir effectiveness for dense prediction tasks, such as grounding, OCR, and\nsegmentation. To address this gap, we introduce Region-Aware Cluster\nDiscrimination (RICE), a novel method that enhances region-level visual and OCR\ncapabilities. We first construct a billion-scale candidate region dataset and\npropose a Region Transformer layer to extract rich regional semantics. We\nfurther design a unified region cluster discrimination loss that jointly\nsupports object and OCR learning within a single classification framework,\nenabling efficient and scalable distributed training on large-scale data.\nExtensive experiments show that RICE consistently outperforms previous methods\non tasks, including segmentation, dense detection, and visual perception for\nMultimodal Large Language Models (MLLMs). The pre-trained models have been\nreleased at https://github.com/deepglint/MVT.",
        "url": "http://arxiv.org/abs/2507.20025v1",
        "published_date": "2025-07-26T17:47:09+00:00",
        "updated_date": "2025-07-26T17:47:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yin Xie",
            "Kaicheng Yang",
            "Xiang An",
            "Kun Wu",
            "Yongle Zhao",
            "Weimo Deng",
            "Zimin Ran",
            "Yumeng Wang",
            "Ziyong Feng",
            "Roy Miles",
            "Ismail Elezi",
            "Jiankang Deng"
        ],
        "tldr": "The paper introduces RICE, a region-aware cluster discrimination method for learning visual representations that enhances region-level visual and OCR capabilities, improving performance on dense prediction tasks like segmentation and detection, and benefiting MLLMs.",
        "tldr_zh": "该论文介绍了一种区域感知聚类判别方法RICE，用于学习视觉表征，增强区域级别的视觉和OCR能力，提高在密集预测任务（如分割和检测）上的性能，并有益于多模态大型语言模型（MLLMs）。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text",
        "summary": "Automated data visualization plays a crucial role in simplifying data\ninterpretation, enhancing decision-making, and improving efficiency. While\nlarge language models (LLMs) have shown promise in generating visualizations\nfrom natural language, the absence of comprehensive benchmarks limits the\nrigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark\ndesigned to assess text-to-visualization models, covering 20+ chart types and\ndiverse data science queries, including trend analysis, correlation, outlier\ndetection, and predictive analytics. It comprises 1,985 samples, each with a\ndata table, natural language query, short answer, visualization code, and\nannotated charts. The queries involve complex reasoning, conversational turns,\nand dynamic data retrieval. We benchmark 11 open-source and closed-source\nmodels, revealing significant performance gaps, highlighting key challenges,\nand offering insights for future advancements. To close this gap, we propose\nthe first cross-modal actor-critic agentic framework that jointly refines the\ntextual answer and visualization code, increasing GPT-4o`s pass rate from 26%\nto 42% over the direct approach and improving chart quality. We also introduce\nan automated LLM-based evaluation framework that enables scalable assessment\nacross thousands of samples without human annotation, measuring answer\ncorrectness, code execution success, visualization readability, and chart\naccuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.",
        "url": "http://arxiv.org/abs/2507.19969v1",
        "published_date": "2025-07-26T14:59:04+00:00",
        "updated_date": "2025-07-26T14:59:04+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Mizanur Rahman",
            "Md Tahmid Rahman Laskar",
            "Shafiq Joty",
            "Enamul Hoque"
        ],
        "tldr": "The paper introduces Text2Vis, a new benchmark for evaluating text-to-visualization models, and proposes a novel actor-critic framework that improves performance and an automated LLM-based evaluation method for scalable assessment.",
        "tldr_zh": "该论文介绍了Text2Vis，一个新的用于评估文本到可视化模型的基准，并提出了一个新的actor-critic框架，提高了性能，以及一种基于LLM的自动化评估方法，用于可扩展的评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention",
        "summary": "We propose Reverse Contrast Attention (RCA), a plug-in method that enhances\nobject localization in vision-language transformers without retraining. RCA\nreweights final-layer attention by suppressing extremes and amplifying\nmid-level activations to let semantically relevant but subdued tokens guide\npredictions. We evaluate it on Open Vocabulary Referring Object Detection\n(OV-RefOD), introducing FitAP, a confidence-free average precision metric based\non IoU and box area. RCA improves FitAP in 11 out of 15 open-source VLMs, with\ngains up to $+26.6\\%$. Effectiveness aligns with attention sharpness and fusion\ntiming; while late-fusion models benefit consistently, models like\n$\\texttt{DeepSeek-VL2}$ also improve, pointing to capacity and disentanglement\nas key factors. RCA offers both interpretability and performance gains for\nmultimodal transformers. Codes and dataset are available from\nhttps://github.com/earl-juanico/rca",
        "url": "http://arxiv.org/abs/2507.19891v2",
        "published_date": "2025-07-26T09:43:09+00:00",
        "updated_date": "2025-07-30T04:47:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Drandreb Earl O. Juanico",
            "Rowel O. Atienza",
            "Jeffrey Kenneth Go"
        ],
        "tldr": "The paper introduces Reverse Contrast Attention (RCA), a plug-in method improving object localization in vision-language transformers, evaluated on Open Vocabulary Referring Object Detection with a new metric, FitAP, showing significant gains across various VLMs.",
        "tldr_zh": "本文介绍了一种名为反向对比注意力（RCA）的插件方法，该方法提高了视觉语言转换器中的目标定位能力。该方法使用新的FitAP指标在开放词汇指代目标检测上进行了评估，并在各种VLM中显示出显著的提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GNSP: Gradient Null Space Projection for Preserving Cross-Modal Alignment in VLMs Continual Learning",
        "summary": "Contrastive Language-Image Pretraining has demonstrated remarkable zero-shot\ngeneralization by aligning visual and textual modalities in a shared embedding\nspace. However, when continuously fine-tuned on diverse tasks, CLIP suffers\nfrom catastrophic forgetting and degradation of its embedding alignment,\nundermining its zero-shot capabilities. In this work, we propose Gradient Null\nSpace Projection (GNSP), an efficient continual learning method that projects\ntask-specific gradients onto the null space of previously learned knowledge.\nThis orthogonal projection mathematically prevents interference with previous\ntasks without relying on rehearsal or architectural modification. Furthermore,\nto preserve the inherent generalization property of CLIP, we introduce\nknowledge distillation and combine it with a modality alignment preservation\nloss inspired by CLIP pre-training to stabilize the structure of the multimodal\nembedding space during fine-tuning. On the MTIL benchmark consisting of 11\ntasks, our method achieved SOTA performance on both the Average and Last key\nmetrics. More importantly, experiments show that our method successfully\nmaintains the original modality gap and cross-modal retrieval performance of\nCLIP, confirming its effectiveness in maintaining a robust visual-language\nspace throughout the continual learning process.",
        "url": "http://arxiv.org/abs/2507.19839v1",
        "published_date": "2025-07-26T07:22:12+00:00",
        "updated_date": "2025-07-26T07:22:12+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Tiantian Peng",
            "Yuyang Liu",
            "Shuo Yang",
            "Qiuhe Hong",
            "YongHong Tian"
        ],
        "tldr": "This paper introduces Gradient Null Space Projection (GNSP), a continual learning method for VLMs that preserves cross-modal alignment and mitigates catastrophic forgetting during fine-tuning by projecting task-specific gradients and using knowledge distillation with a modality alignment loss.",
        "tldr_zh": "本文介绍了一种名为梯度零空间投影（GNSP）的VLM持续学习方法，该方法通过投影任务特定的梯度并使用知识蒸馏和模态对齐损失来保留跨模态对齐，并减轻微调过程中的灾难性遗忘。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Learning for Product Attributes with Compact Multimodal Models",
        "summary": "Image-based product attribute prediction in e-commerce is a crucial task with\nnumerous applications. The supervised fine-tuning of Vision Language Models\n(VLMs) faces significant scale challenges due to the cost of manual or API\nbased annotation. In this paper, we investigate label-efficient semi-supervised\nfine-tuning strategies for compact VLMs (2B-3B parameters) that leverage\nunlabeled product listings through Direct Preference Optimization (DPO).\nBeginning with a small, API-based, annotated, and labeled set, we first employ\nPEFT to train low-rank adapter modules. To update the adapter weights with\nunlabeled data, we generate multiple reasoning-and-answer chains per unlabeled\nsample and segregate these chains into preferred and dispreferred based on\nself-consistency. We then fine-tune the model with DPO loss and use the updated\nmodel for the next iteration. By using PEFT fine-tuning with DPO, our method\nachieves efficient convergence with minimal compute overhead. On a dataset\nspanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes\nonly unlabeled data, demonstrates a significant improvement over the supervised\nmodel. Moreover, experiments demonstrate that accuracy with DPO training\nimproves with more unlabeled data, indicating that a large pool of unlabeled\nsamples can be effectively leveraged to improve performance.",
        "url": "http://arxiv.org/abs/2507.19679v1",
        "published_date": "2025-07-25T21:12:11+00:00",
        "updated_date": "2025-07-25T21:12:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mandar Kulkarni"
        ],
        "tldr": "This paper presents a semi-supervised fine-tuning strategy using Direct Preference Optimization (DPO) for compact VLMs to predict product attributes from images, leveraging unlabeled data to improve performance with minimal compute overhead.",
        "tldr_zh": "本文提出了一种半监督微调策略，使用直接偏好优化（DPO）对紧凑型VLM进行微调，以预测图像中的产品属性，利用未标记数据以最小的计算开销提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Object-centric Video Question Answering with Visual Grounding and Referring",
        "summary": "Video Large Language Models (VideoLLMs) have recently demonstrated remarkable\nprogress in general video understanding. However, existing models primarily\nfocus on high-level comprehension and are limited to text-only responses,\nrestricting the flexibility for object-centric, multiround interactions. In\nthis paper, we make three contributions: (i) we address these limitations by\nintroducing a VideoLLM model, capable of performing both object referring for\ninput and grounding for output in video reasoning tasks, i.e., allowing users\nto interact with videos using both textual and visual prompts; (ii) we propose\nSTOM (Spatial-Temporal Overlay Module), a novel approach that propagates\narbitrary visual prompts input at any single timestamp to the remaining frames\nwithin a video; (iii) we present VideoInfer, a manually curated object-centric\nvideo instruction dataset featuring questionanswering pairs that require\nreasoning. We conduct comprehensive experiments on VideoInfer and other\nexisting benchmarks across video question answering and referring object\nsegmentation. The results on 12 benchmarks of 6 tasks show that our proposed\nmodel consistently outperforms baselines in both video question answering and\nsegmentation, underscoring its robustness in multimodal, object-centric video\nand image understanding. Project page:\nhttps://qirui-chen.github.io/RGA3-release/.",
        "url": "http://arxiv.org/abs/2507.19599v1",
        "published_date": "2025-07-25T18:11:23+00:00",
        "updated_date": "2025-07-25T18:11:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haochen Wang",
            "Qirui Chen",
            "Cilin Yan",
            "Jiayin Cai",
            "Xiaolong Jiang",
            "Yao Hu",
            "Weidi Xie",
            "Stratis Gavves"
        ],
        "tldr": "This paper introduces a VideoLLM capable of object referring and grounding, along with a novel spatial-temporal overlay module (STOM) and a new object-centric video instruction dataset (VideoInfer), demonstrating improved performance on video question answering and segmentation tasks.",
        "tldr_zh": "该论文介绍了一个具备对象指代和定位能力的视频大语言模型（VideoLLM），以及一个新的时空覆盖模块（STOM）和一个新的以对象为中心的视频指令数据集（VideoInfer），并在视频问答和分割任务中表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Predicting Brain Responses To Natural Movies With Multimodal LLMs",
        "summary": "We present MedARC's team solution to the Algonauts 2025 challenge. Our\npipeline leveraged rich multimodal representations from various\nstate-of-the-art pretrained models across video (V-JEPA2), speech (Whisper),\ntext (Llama 3.2), vision-text (InternVL3), and vision-text-audio\n(Qwen2.5-Omni). These features extracted from the models were linearly\nprojected to a latent space, temporally aligned to the fMRI time series, and\nfinally mapped to cortical parcels through a lightweight encoder comprising a\nshared group head plus subject-specific residual heads. We trained hundreds of\nmodel variants across hyperparameter settings, validated them on held-out\nmovies and assembled ensembles targeted to each parcel in each subject. Our\nfinal submission achieved a mean Pearson's correlation of 0.2085 on the test\nsplit of withheld out-of-distribution movies, placing our team in fourth place\nfor the competition. We further discuss a last-minute optimization that would\nhave raised us to second place. Our results highlight how combining features\nfrom models trained in different modalities, using a simple architecture\nconsisting of shared-subject and single-subject components, and conducting\ncomprehensive model selection and ensembling improves generalization of\nencoding models to novel movie stimuli. All code is available on GitHub.",
        "url": "http://arxiv.org/abs/2507.19956v1",
        "published_date": "2025-07-26T13:57:08+00:00",
        "updated_date": "2025-07-26T13:57:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "q-bio.NC"
        ],
        "authors": [
            "Cesar Kadir Torrico Villanueva",
            "Jiaxin Cindy Tu",
            "Mihir Tripathy",
            "Connor Lane",
            "Rishab Iyer",
            "Paul S. Scotti"
        ],
        "tldr": "This paper presents a multimodal LLM-based pipeline for predicting brain responses to natural movies, achieving fourth place in the Algonauts 2025 challenge by ensembling models trained on video, speech, text, and audio features and achieving a 0.2085 mean Pearson's correlation.",
        "tldr_zh": "本文提出了一种基于多模态LLM的流程，用于预测大脑对自然电影的反应。通过集成在视频、语音、文本和音频特征上训练的模型，并在Algonauts 2025挑战赛中获得第四名，平均皮尔逊相关系数为0.2085。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AF-CLIP: Zero-Shot Anomaly Detection via Anomaly-Focused CLIP Adaptation",
        "summary": "Visual anomaly detection has been widely used in industrial inspection and\nmedical diagnosis. Existing methods typically demand substantial training\nsamples, limiting their utility in zero-/few-shot scenarios. While recent\nefforts have leveraged CLIP's zero-shot recognition capability for this task,\nthey often ignore optimizing visual features to focus on local anomalies,\nreducing their efficacy. In this work, we propose AF-CLIP (Anomaly-Focused\nCLIP) by dramatically enhancing its visual representations to focus on local\ndefects. Our approach introduces a lightweight adapter that emphasizes\nanomaly-relevant patterns in visual features, simultaneously optimizing both\nclass-level features for image classification and patch-level features for\nprecise localization. To capture anomalies of different sizes and improve\ndetection accuracy, prior to the adapter, we develop a multi-scale spatial\naggregation mechanism to effectively consolidate neighborhood context.\nComplementing these visual enhancements, we design learnable textual prompts\nthat generically characterize normal and abnormal states. After optimization on\nauxiliary datasets using a composite objective function, AF-CLIP demonstrates\nstrong zero-shot detection capability. Our method is also extended to few-shot\nscenarios by extra memory banks. Experimental results across diverse industrial\nand medical datasets demonstrate the effectiveness and generalization of our\nproposed method. Code is available at https://github.com/Faustinaqq/AF-CLIP.",
        "url": "http://arxiv.org/abs/2507.19949v1",
        "published_date": "2025-07-26T13:34:38+00:00",
        "updated_date": "2025-07-26T13:34:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingqing Fang",
            "Wenxi Lv",
            "Qinliang Su"
        ],
        "tldr": "AF-CLIP enhances CLIP for zero-shot anomaly detection by focusing on local defects through a lightweight adapter, multi-scale spatial aggregation, and learnable textual prompts, achieving strong performance on industrial and medical datasets.",
        "tldr_zh": "AF-CLIP通过轻量级适配器、多尺度空间聚合和可学习的文本提示增强了CLIP，专注于局部缺陷，从而实现零样本异常检测，并在工业和医疗数据集中表现出色。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving",
        "summary": "Federated domain generalization has shown promising progress in image\nclassification by enabling collaborative training across multiple clients\nwithout sharing raw data. However, its potential in the semantic segmentation\nof autonomous driving remains underexplored. In this paper, we propose FedS2R,\nthe first one-shot federated domain generalization framework for\nsynthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises\ntwo components: an inconsistency-driven data augmentation strategy that\ngenerates images for unstable classes, and a multi-client knowledge\ndistillation scheme with feature fusion that distills a global model from\nmultiple client models. Experiments on five real-world datasets, Cityscapes,\nBDD100K, Mapillary, IDD, and ACDC, show that the global model significantly\noutperforms individual client models and is only 2 mIoU points behind the model\ntrained with simultaneous access to all client data. These results demonstrate\nthe effectiveness of FedS2R in synthetic-to-real semantic segmentation for\nautonomous driving under federated learning",
        "url": "http://arxiv.org/abs/2507.19881v1",
        "published_date": "2025-07-26T09:24:00+00:00",
        "updated_date": "2025-07-26T09:24:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tao Lian",
            "Jose L. Gómez",
            "Antonio M. López"
        ],
        "tldr": "The paper introduces FedS2R, a one-shot federated domain generalization framework for synthetic-to-real semantic segmentation in autonomous driving, using inconsistency-driven data augmentation and multi-client knowledge distillation.",
        "tldr_zh": "该论文介绍了FedS2R，一种用于自动驾驶中合成到真实语义分割的单次联邦域泛化框架，采用不一致性驱动的数据增强和多客户端知识蒸馏。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ATCTrack: Aligning Target-Context Cues with Dynamic Target States for Robust Vision-Language Tracking",
        "summary": "Vision-language tracking aims to locate the target object in the video\nsequence using a template patch and a language description provided in the\ninitial frame. To achieve robust tracking, especially in complex long-term\nscenarios that reflect real-world conditions as recently highlighted by MGIT,\nit is essential not only to characterize the target features but also to\nutilize the context features related to the target. However, the visual and\ntextual target-context cues derived from the initial prompts generally align\nonly with the initial target state. Due to their dynamic nature, target states\nare constantly changing, particularly in complex long-term sequences. It is\nintractable for these cues to continuously guide Vision-Language Trackers\n(VLTs). Furthermore, for the text prompts with diverse expressions, our\nexperiments reveal that existing VLTs struggle to discern which words pertain\nto the target or the context, complicating the utilization of textual cues. In\nthis work, we present a novel tracker named ATCTrack, which can obtain\nmultimodal cues Aligned with the dynamic target states through comprehensive\nTarget-Context feature modeling, thereby achieving robust tracking.\nSpecifically, (1) for the visual modality, we propose an effective temporal\nvisual target-context modeling approach that provides the tracker with timely\nvisual cues. (2) For the textual modality, we achieve precise target words\nidentification solely based on textual content, and design an innovative\ncontext words calibration method to adaptively utilize auxiliary context words.\n(3) We conduct extensive experiments on mainstream benchmarks and ATCTrack\nachieves a new SOTA performance. The code and models will be released at:\nhttps://github.com/XiaokunFeng/ATCTrack.",
        "url": "http://arxiv.org/abs/2507.19875v1",
        "published_date": "2025-07-26T09:05:12+00:00",
        "updated_date": "2025-07-26T09:05:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "X. Feng",
            "S. Hu",
            "X. Li",
            "D. Zhang",
            "M. Wu",
            "J. Zhang",
            "X. Chen",
            "K. Huang"
        ],
        "tldr": "The paper introduces ATCTrack, a novel vision-language tracker that aligns target-context cues with dynamic target states for robust tracking by incorporating temporal visual context modeling and precise target/context word identification in the textual modality. It achieves state-of-the-art performance on mainstream benchmarks.",
        "tldr_zh": "该论文介绍了ATCTrack，一种新型的视觉-语言跟踪器，通过结合时间视觉上下文建模和文本模态中精确的目标/上下文词识别，将目标-上下文线索与动态目标状态对齐，以实现鲁棒的跟踪。它在主流基准测试中实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration",
        "summary": "Open-world object detection (OWOD) extends traditional object detection to\nidentifying both known and unknown object, necessitating continuous model\nadaptation as new annotations emerge. Current approaches face significant\nlimitations: 1) data-hungry training due to reliance on a large number of\ncrowdsourced annotations, 2) susceptibility to \"partial feature overfitting,\"\nand 3) limited flexibility due to required model architecture modifications. To\ntackle these issues, we present OW-CLIP, a visual analytics system that\nprovides curated data and enables data-efficient OWOD model incremental\ntraining. OW-CLIP implements plug-and-play multimodal prompt tuning tailored\nfor OWOD settings and introduces a novel \"Crop-Smoothing\" technique to mitigate\npartial feature overfitting. To meet the data requirements for the training\nmethodology, we propose dual-modal data refinement methods that leverage large\nlanguage models and cross-modal similarity for data generation and filtering.\nSimultaneously, we develope a visualization interface that enables users to\nexplore and deliver high-quality annotations: including class-specific visual\nfeature phrases and fine-grained differentiated images. Quantitative evaluation\ndemonstrates that OW-CLIP achieves competitive performance at 89% of\nstate-of-the-art performance while requiring only 3.8% self-generated data,\nwhile outperforming SOTA approach when trained with equivalent data volumes. A\ncase study shows the effectiveness of the developed method and the improved\nannotation quality of our visualization system.",
        "url": "http://arxiv.org/abs/2507.19870v1",
        "published_date": "2025-07-26T08:58:56+00:00",
        "updated_date": "2025-07-26T08:58:56+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Junwen Duan",
            "Wei Xue",
            "Ziyao Kang",
            "Shixia Liu",
            "Jiazhi Xia"
        ],
        "tldr": "OW-CLIP introduces a data-efficient visual supervision framework for open-world object detection, leveraging human-AI collaboration with multimodal prompt tuning and a novel Crop-Smoothing technique to achieve competitive performance with significantly less data.",
        "tldr_zh": "OW-CLIP 提出了一种数据高效的视觉监督框架，用于开放世界目标检测，该框架利用人机协作、多模态提示调优和新颖的 Crop-Smoothing 技术，以显著更少的数据实现了具有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Knowledge Regularized Negative Feature Tuning of Vision-Language Models for Out-of-Distribution Detection",
        "summary": "Out-of-distribution (OOD) detection is crucial for building reliable machine\nlearning models. Although negative prompt tuning has enhanced the OOD detection\ncapabilities of vision-language models, these tuned models often suffer from\nreduced generalization performance on unseen classes and styles. To address\nthis challenge, we propose a novel method called Knowledge Regularized Negative\nFeature Tuning (KR-NFT), which integrates an innovative adaptation architecture\ntermed Negative Feature Tuning (NFT) and a corresponding\nknowledge-regularization (KR) optimization strategy. Specifically, NFT applies\ndistribution-aware transformations to pre-trained text features, effectively\nseparating positive and negative features into distinct spaces. This separation\nmaximizes the distinction between in-distribution (ID) and OOD images.\nAdditionally, we introduce image-conditional learnable factors through a\nlightweight meta-network, enabling dynamic adaptation to individual images and\nmitigating sensitivity to class and style shifts. Compared to traditional\nnegative prompt tuning, NFT demonstrates superior efficiency and scalability.\nTo optimize this adaptation architecture, the KR optimization strategy is\ndesigned to enhance the discrimination between ID and OOD sets while mitigating\npre-trained knowledge forgetting. This enhances OOD detection performance on\ntrained ID classes while simultaneously improving OOD detection on unseen ID\ndatasets. Notably, when trained with few-shot samples from ImageNet dataset,\nKR-NFT not only improves ID classification accuracy and OOD detection but also\nsignificantly reduces the FPR95 by 5.44\\% under an unexplored generalization\nsetting with unseen ID categories. Codes can be found at\n\\href{https://github.com/ZhuWenjie98/KRNFT}.",
        "url": "http://arxiv.org/abs/2507.19847v2",
        "published_date": "2025-07-26T07:44:04+00:00",
        "updated_date": "2025-07-29T14:15:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjie Zhu",
            "Yabin Zhang",
            "Xin Jin",
            "Wenjun Zeng",
            "Lei Zhang"
        ],
        "tldr": "The paper introduces Knowledge Regularized Negative Feature Tuning (KR-NFT) to improve out-of-distribution detection in vision-language models by separating positive and negative features and mitigating knowledge forgetting, resulting in improved generalization performance.",
        "tldr_zh": "该论文介绍了知识正则化负特征调整 (KR-NFT)，通过分离正负特征并减轻知识遗忘，从而提高视觉语言模型中的分布外检测能力，从而提高泛化性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Taking Language Embedded 3D Gaussian Splatting into the Wild",
        "summary": "Recent advances in leveraging large-scale Internet photo collections for 3D\nreconstruction have enabled immersive virtual exploration of landmarks and\nhistoric sites worldwide. However, little attention has been given to the\nimmersive understanding of architectural styles and structural knowledge, which\nremains largely confined to browsing static text-image pairs. Therefore, can we\ndraw inspiration from 3D in-the-wild reconstruction techniques and use\nunconstrained photo collections to create an immersive approach for\nunderstanding the 3D structure of architectural components? To this end, we\nextend language embedded 3D Gaussian splatting (3DGS) and propose a novel\nframework for open-vocabulary scene understanding from unconstrained photo\ncollections. Specifically, we first render multiple appearance images from the\nsame viewpoint as the unconstrained image with the reconstructed radiance\nfield, then extract multi-appearance CLIP features and two types of language\nfeature uncertainty maps-transient and appearance uncertainty-derived from the\nmulti-appearance features to guide the subsequent optimization process. Next,\nwe propose a transient uncertainty-aware autoencoder, a multi-appearance\nlanguage field 3DGS representation, and a post-ensemble strategy to effectively\ncompress, learn, and fuse language features from multiple appearances. Finally,\nto quantitatively evaluate our method, we introduce PT-OVS, a new benchmark\ndataset for assessing open-vocabulary segmentation performance on unconstrained\nphoto collections. Experimental results show that our method outperforms\nexisting methods, delivering accurate open-vocabulary segmentation and enabling\napplications such as interactive roaming with open-vocabulary queries,\narchitectural style pattern recognition, and 3D scene editing.",
        "url": "http://arxiv.org/abs/2507.19830v1",
        "published_date": "2025-07-26T07:00:32+00:00",
        "updated_date": "2025-07-26T07:00:32+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Yuze Wang",
            "Yue Qi"
        ],
        "tldr": "This paper extends language-embedded 3D Gaussian Splatting for open-vocabulary scene understanding from unconstrained photo collections, introducing techniques like transient uncertainty-aware autoencoders and a new benchmark dataset (PT-OVS). It enables applications like interactive roaming and 3D scene editing.",
        "tldr_zh": "本文扩展了语言嵌入的3D高斯溅射技术，用于从无约束照片集中实现开放词汇场景理解，引入了诸如瞬态不确定性感知自动编码器和新的基准数据集（PT-OVS）等技术。它能够实现诸如交互式漫游和3D场景编辑之类的应用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SeeDiff: Off-the-Shelf Seeded Mask Generation from Diffusion Models",
        "summary": "Entrusted with the goal of pixel-level object classification, the semantic\nsegmentation networks entail the laborious preparation of pixel-level\nannotation masks. To obtain pixel-level annotation masks for a given class\nwithout human efforts, recent few works have proposed to generate pairs of\nimages and annotation masks by employing image and text relationships modeled\nby text-to-image generative models, especially Stable Diffusion. However, these\nworks do not fully exploit the capability of text-guided Diffusion models and\nthus require a pre-trained segmentation network, careful text prompt tuning, or\nthe training of a segmentation network to generate final annotation masks. In\nthis work, we take a closer look at attention mechanisms of Stable Diffusion,\nfrom which we draw connections with classical seeded segmentation approaches.\nIn particular, we show that cross-attention alone provides very coarse object\nlocalization, which however can provide initial seeds. Then, akin to region\nexpansion in seeded segmentation, we utilize the\nsemantic-correspondence-modeling capability of self-attention to iteratively\nspread the attention to the whole class from the seeds using multi-scale\nself-attention maps. We also observe that a simple-text-guided synthetic image\noften has a uniform background, which is easier to find correspondences,\ncompared to complex-structured objects. Thus, we further refine a mask using a\nmore accurate background mask. Our proposed method, dubbed SeeDiff, generates\nhigh-quality masks off-the-shelf from Stable Diffusion, without additional\ntraining procedure, prompt tuning, or a pre-trained segmentation network.",
        "url": "http://arxiv.org/abs/2507.19808v1",
        "published_date": "2025-07-26T05:44:00+00:00",
        "updated_date": "2025-07-26T05:44:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Joon Hyun Park",
            "Kumju Jo",
            "Sungyong Baik"
        ],
        "tldr": "SeeDiff leverages Stable Diffusion's attention mechanisms to generate high-quality segmentation masks without additional training, prompt engineering, or pre-trained networks, using a seeded segmentation approach.",
        "tldr_zh": "SeeDiff利用Stable Diffusion的注意力机制，通过一种种子分割方法，无需额外训练、提示工程或预训练网络即可生成高质量的分割掩码。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks",
        "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development.",
        "url": "http://arxiv.org/abs/2507.19634v1",
        "published_date": "2025-07-25T19:00:51+00:00",
        "updated_date": "2025-07-25T19:00:51+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Sara Papi",
            "Maike Züfle",
            "Marco Gaido",
            "Beatrice Savoldi",
            "Danni Liu",
            "Ioannis Douros",
            "Luisa Bentivogli",
            "Jan Niehues"
        ],
        "tldr": "The paper introduces MCIF, a new multilingual, multimodal benchmark based on scientific talks, designed for evaluating instruction-following capabilities of MLLMs across languages and modalities in both short and long contexts.",
        "tldr_zh": "本文介绍了MCIF，这是一个新的多语言、多模态基准，基于科学讲座构建，旨在评估MLLM在短文本和长文本环境中，跨语言和模态的指令跟随能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]