[
    {
        "title": "SCALAR: Scale-wise Controllable Visual Autoregressive Learning",
        "summary": "Controllable image synthesis, which enables fine-grained control over\ngenerated outputs, has emerged as a key focus in visual generative modeling.\nHowever, controllable generation remains challenging for Visual Autoregressive\n(VAR) models due to their hierarchical, next-scale prediction style. Existing\nVAR-based methods often suffer from inefficient control encoding and disruptive\ninjection mechanisms that compromise both fidelity and efficiency. In this\nwork, we present SCALAR, a controllable generation method based on VAR,\nincorporating a novel Scale-wise Conditional Decoding mechanism. SCALAR\nleverages a pretrained image encoder to extract semantic control signal\nencodings, which are projected into scale-specific representations and injected\ninto the corresponding layers of the VAR backbone. This design provides\npersistent and structurally aligned guidance throughout the generation process.\nBuilding on SCALAR, we develop SCALAR-Uni, a unified extension that aligns\nmultiple control modalities into a shared latent space, supporting flexible\nmulti-conditional guidance in a single model. Extensive experiments show that\nSCALAR achieves superior generation quality and control precision across\nvarious tasks.",
        "url": "http://arxiv.org/abs/2507.19946v2",
        "published_date": "2025-07-26T13:23:08+00:00",
        "updated_date": "2025-07-29T03:38:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ryan Xu",
            "Dongyang Jin",
            "Yancheng Bai",
            "Rui Lan",
            "Xu Duan",
            "Lei Sun",
            "Xiangxiang Chu"
        ],
        "tldr": "The paper introduces SCALAR, a novel Visual Autoregressive (VAR) method for controllable image synthesis using scale-wise conditional decoding, offering improved generation quality and control precision, further extended to SCALAR-Uni for multi-conditional guidance.",
        "tldr_zh": "该论文介绍了SCALAR，一种新颖的视觉自回归（VAR）方法，用于可控图像合成，采用scale-wise条件解码，提高了生成质量和控制精度，并进一步扩展到SCALAR-Uni以实现多条件引导。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "All-in-One Medical Image Restoration with Latent Diffusion-Enhanced Vector-Quantized Codebook Prior",
        "summary": "All-in-one medical image restoration (MedIR) aims to address multiple MedIR\ntasks using a unified model, concurrently recovering various high-quality (HQ)\nmedical images (e.g., MRI, CT, and PET) from low-quality (LQ) counterparts.\nHowever, all-in-one MedIR presents significant challenges due to the\nheterogeneity across different tasks. Each task involves distinct degradations,\nleading to diverse information losses in LQ images. Existing methods struggle\nto handle these diverse information losses associated with different tasks. To\naddress these challenges, we propose a latent diffusion-enhanced\nvector-quantized codebook prior and develop \\textbf{DiffCode}, a novel\nframework leveraging this prior for all-in-one MedIR. Specifically, to\ncompensate for diverse information losses associated with different tasks,\nDiffCode constructs a task-adaptive codebook bank to integrate task-specific HQ\nprior features across tasks, capturing a comprehensive prior. Furthermore, to\nenhance prior retrieval from the codebook bank, DiffCode introduces a latent\ndiffusion strategy that utilizes the diffusion model's powerful mapping\ncapabilities to iteratively refine the latent feature distribution, estimating\nmore accurate HQ prior features during restoration. With the help of the\ntask-adaptive codebook bank and latent diffusion strategy, DiffCode achieves\nsuperior performance in both quantitative metrics and visual quality across\nthree MedIR tasks: MRI super-resolution, CT denoising, and PET synthesis.",
        "url": "http://arxiv.org/abs/2507.19874v1",
        "published_date": "2025-07-26T09:04:14+00:00",
        "updated_date": "2025-07-26T09:04:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haowei Chen",
            "Zhiwen Yang",
            "Haotian Hou",
            "Hui Zhang",
            "Bingzheng Wei",
            "Gang Zhou",
            "Yan Xu"
        ],
        "tldr": "This paper introduces DiffCode, a novel framework for all-in-one medical image restoration that uses a latent diffusion-enhanced vector-quantized codebook prior to handle diverse degradations across MRI super-resolution, CT denoising, and PET synthesis tasks.",
        "tldr_zh": "本文介绍了一种名为DiffCode的新型一体化医学图像恢复框架，该框架利用潜在扩散增强的向量量化码本先验来处理MRI超分辨率、CT降噪和PET合成任务中不同的降级。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoFRR: Mixture of Diffusion Models for Face Retouching Restoration",
        "summary": "The widespread use of face retouching on social media platforms raises\nconcerns about the authenticity of face images. While existing methods focus on\ndetecting face retouching, how to accurately recover the original faces from\nthe retouched ones has yet to be answered. This paper introduces Face\nRetouching Restoration (FRR), a novel computer vision task aimed at restoring\noriginal faces from their retouched counterparts. FRR differs from traditional\nimage restoration tasks by addressing the complex retouching operations with\nvarious types and degrees, which focuses more on the restoration of the\nlow-frequency information of the faces. To tackle this challenge, we propose\nMoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert\nisolation strategy, the MoFRR uses sparse activation of specialized experts\nhandling distinct retouching types and the engagement of a shared expert\ndealing with universal retouching traces. Each specialized expert follows a\ndual-branch structure with a DDIM-based low-frequency branch guided by an\nIterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based\nHigh-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a\nnewly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the\neffectiveness of MoFRR for FRR.",
        "url": "http://arxiv.org/abs/2507.19770v1",
        "published_date": "2025-07-26T03:45:53+00:00",
        "updated_date": "2025-07-26T03:45:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxin Liu",
            "Qichao Ying",
            "Zhenxing Qian",
            "Sheng Li",
            "Runqi Zhang",
            "Jian Liu",
            "Xinpeng Zhang"
        ],
        "tldr": "The paper introduces a new task, Face Retouching Restoration (FRR), and proposes a mixture of diffusion models (MoFRR) to restore original faces from retouched images, using specialized experts for different retouching types and a shared expert for universal traces.",
        "tldr_zh": "该论文介绍了一个新的任务，即面部修饰恢复（FRR），并提出了一种混合扩散模型（MoFRR）来从修饰图像中恢复原始面部，该模型使用专门的专家处理不同的修饰类型，并使用共享的专家处理通用痕迹。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SkinDualGen: Prompt-Driven Diffusion for Simultaneous Image-Mask Generation in Skin Lesions",
        "summary": "Medical image analysis plays a pivotal role in the early diagnosis of\ndiseases such as skin lesions. However, the scarcity of data and the class\nimbalance significantly hinder the performance of deep learning models. We\npropose a novel method that leverages the pretrained Stable Diffusion-2.0 model\nto generate high-quality synthetic skin lesion images and corresponding\nsegmentation masks. This approach augments training datasets for classification\nand segmentation tasks. We adapt Stable Diffusion-2.0 through domain-specific\nLow-Rank Adaptation (LoRA) fine-tuning and joint optimization of\nmulti-objective loss functions, enabling the model to simultaneously generate\nclinically relevant images and segmentation masks conditioned on textual\ndescriptions in a single step. Experimental results show that the generated\nimages, validated by FID scores, closely resemble real images in quality. A\nhybrid dataset combining real and synthetic data markedly enhances the\nperformance of classification and segmentation models, achieving substantial\nimprovements in accuracy and F1-score of 8% to 15%, with additional positive\ngains in other key metrics such as the Dice coefficient and IoU. Our approach\noffers a scalable solution to address the challenges of medical imaging data,\ncontributing to improved accuracy and reliability in diagnosing rare diseases.",
        "url": "http://arxiv.org/abs/2507.19970v1",
        "published_date": "2025-07-26T15:00:37+00:00",
        "updated_date": "2025-07-26T15:00:37+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhaobin Xu"
        ],
        "tldr": "The paper introduces SkinDualGen, a prompt-driven diffusion model for generating synthetic skin lesion images and segmentation masks to augment training data, improving classification and segmentation performance.",
        "tldr_zh": "该论文介绍了 SkinDualGen，一种提示驱动的扩散模型，用于生成合成皮肤病变图像和分割掩码，以扩充训练数据，从而提高分类和分割性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "JDATT: A Joint Distillation Framework for Atmospheric Turbulence Mitigation and Target Detection",
        "summary": "Atmospheric turbulence (AT) introduces severe degradations, such as rippling,\nblur, and intensity fluctuations, that hinder both image quality and downstream\nvision tasks like target detection. While recent deep learning-based approaches\nhave advanced AT mitigation using transformer and Mamba architectures, their\nhigh complexity and computational cost make them unsuitable for real-time\napplications, especially in resource-constrained settings such as remote\nsurveillance. Moreover, the common practice of separating turbulence mitigation\nand object detection leads to inefficiencies and suboptimal performance. To\naddress these challenges, we propose JDATT, a Joint Distillation framework for\nAtmospheric Turbulence mitigation and Target detection. JDATT integrates\nstate-of-the-art AT mitigation and detection modules and introduces a unified\nknowledge distillation strategy that compresses both components while\nminimizing performance loss. We employ a hybrid distillation scheme:\nfeature-level distillation via Channel-Wise Distillation (CWD) and Masked\nGenerative Distillation (MGD), and output-level distillation via\nKullback-Leibler divergence. Experiments on synthetic and real-world turbulence\ndatasets demonstrate that JDATT achieves superior visual restoration and\ndetection accuracy while significantly reducing model size and inference time,\nmaking it well-suited for real-time deployment.",
        "url": "http://arxiv.org/abs/2507.19780v1",
        "published_date": "2025-07-26T04:06:48+00:00",
        "updated_date": "2025-07-26T04:06:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiming Liu",
            "Paul Hill",
            "Nantheera Anantrasirichai"
        ],
        "tldr": "The paper introduces JDATT, a joint knowledge distillation framework for atmospheric turbulence mitigation and target detection, achieving better performance with reduced model size and inference time.",
        "tldr_zh": "该论文介绍了JDATT，一个用于大气湍流缓解和目标检测的联合知识蒸馏框架，它在减小模型大小和推理时间的同时实现了更好的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]