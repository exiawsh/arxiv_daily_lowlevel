[
    {
        "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts",
        "summary": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
        "url": "http://arxiv.org/abs/2507.20939v1",
        "published_date": "2025-07-28T15:52:36+00:00",
        "updated_date": "2025-07-28T15:52:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuying Ge",
            "Yixiao Ge",
            "Chen Li",
            "Teng Wang",
            "Junfu Pu",
            "Yizhuo Li",
            "Lu Qiu",
            "Jin Ma",
            "Lisheng Duan",
            "Xinyu Zuo",
            "Jinwen Luo",
            "Weibo Gu",
            "Zexuan Li",
            "Xiaojing Zhang",
            "Yangyu Tao",
            "Han Hu",
            "Di Wang",
            "Ying Shan"
        ],
        "tldr": "The paper introduces ARC-Hunyuan-Video-7B, a multimodal model for comprehensive understanding of real-world short videos, trained with a comprehensive pipeline and demonstrating strong performance on a newly introduced benchmark, with real-world deployment showing user engagement improvements.",
        "tldr_zh": "该论文介绍了ARC-Hunyuan-Video-7B，一个用于全面理解真实世界短视频的多模态模型，通过全面的流程进行训练，并在新引入的基准测试中表现出色，实际部署表明用户参与度有所提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset",
        "summary": "Recent advancements in large multimodal models like GPT-4o have set a new\nstandard for high-fidelity, instruction-guided image editing. However, the\nproprietary nature of these models and their training data creates a\nsignificant barrier for open-source research. To bridge this gap, we introduce\nGPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus\ncontaining more than 1.5 million high-quality triplets (instruction, source\nimage, edited image). We systematically construct this dataset by leveraging\nthe versatile capabilities of GPT-4o to unify and refine three popular\nimage-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our\nmethodology involves 1) regenerating output images to enhance visual quality\nand instruction alignment, and 2) selectively rewriting prompts to improve\nsemantic clarity. To validate the efficacy of our dataset, we fine-tune\nadvanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are\nexciting, e.g., the fine-tuned FluxKontext achieves highly competitive\nperformance across a comprehensive suite of benchmarks, including 7.24 on\nGEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger\ninstruction following and higher perceptual quality while maintaining identity.\nThese scores markedly exceed all previously published open-source methods and\nsubstantially narrow the gap to leading proprietary models. We hope the full\nrelease of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in\ninstruction-guided image editing.",
        "url": "http://arxiv.org/abs/2507.21033v1",
        "published_date": "2025-07-28T17:54:04+00:00",
        "updated_date": "2025-07-28T17:54:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhan Wang",
            "Siwei Yang",
            "Bingchen Zhao",
            "Letian Zhang",
            "Qing Liu",
            "Yuyin Zhou",
            "Cihang Xie"
        ],
        "tldr": "The paper introduces GPT-IMAGE-EDIT-1.5M, a large-scale, publicly available image editing dataset generated using GPT-4o, and demonstrates its effectiveness by fine-tuning open-source models that achieve state-of-the-art performance compared to other open-source models.",
        "tldr_zh": "该论文介绍了GPT-IMAGE-EDIT-1.5M，一个大规模、公开可用的图像编辑数据集，该数据集使用GPT-4o生成。论文通过对开源模型进行微调，证明了该数据集的有效性，并表明微调后的模型相对于其他开源模型取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM",
        "summary": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality.",
        "url": "http://arxiv.org/abs/2507.20994v1",
        "published_date": "2025-07-28T16:59:53+00:00",
        "updated_date": "2025-07-28T16:59:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shen Li",
            "Liuyi Yao",
            "Wujia Niu",
            "Lan Zhang",
            "Yaliang Li"
        ],
        "tldr": "This paper introduces \"security tensors\" as a method to extend text-based safety mechanisms in LLMs to the visual modality in LVLMs, enhancing their ability to reject harmful visual inputs without sacrificing benign task performance.",
        "tldr_zh": "本文介绍了\"安全张量\"，作为一种将LLM中基于文本的安全机制扩展到LVLM中的视觉模态的方法，增强了它们拒绝有害视觉输入的能力，同时不牺牲良性任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with Attention-Guided Refinement",
        "summary": "Img2LaTeX is a practically significant task that involves converting\nmathematical expressions or tabular data from images into LaTeX code. In recent\nyears, vision-language models (VLMs) have demonstrated strong performance\nacross a variety of visual understanding tasks, owing to their generalization\ncapabilities. While some studies have explored the use of VLMs for the\nImg2LaTeX task, their performance often falls short of expectations.\nEmpirically, VLMs sometimes struggle with fine-grained visual elements, leading\nto inaccurate LaTeX predictions. To address this challenge, we propose\n$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\nAttention-Guided Refinement, a framework that effectively integrates attention\nlocalization and iterative refinement within a visual reasoning framework,\nenabling VLMs to perform self-correction and progressively improve prediction\nquality. For effective evaluation, we introduce a new dataset,\nImg2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging\nexamples designed to rigorously evaluate the capabilities of VLMs within this\ntask domain. Extensive experimental results demonstrate that: (1) $A^2R^2$\nsignificantly improves model performance across six evaluation metrics spanning\nboth textual and visual levels, consistently outperforming other baseline\nmethods; (2) Increasing the number of inference rounds yields notable\nperformance gains, underscoring the potential of $A^2R^2$ in test-time scaling\nscenarios; (3) Ablation studies and human evaluations validate the practical\neffectiveness of our approach, as well as the strong synergy among its core\ncomponents during inference.",
        "url": "http://arxiv.org/abs/2507.20890v1",
        "published_date": "2025-07-28T14:41:57+00:00",
        "updated_date": "2025-07-28T14:41:57+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Zhecheng Li",
            "Guoxian Song",
            "Yiwei Wang",
            "Zhen Xiong",
            "Junsong Yuan",
            "Yujun Cai"
        ],
        "tldr": "The paper introduces $A^2R^2$, a framework for improving Img2LaTeX conversion by integrating attention localization and iterative refinement within a VLM, along with a new challenging dataset, Img2LaTex-Hard-1K, showing significant performance improvements over baselines.",
        "tldr_zh": "该论文介绍了$A^2R^2$框架，通过在VLM中集成注意力定位和迭代细化来改进Img2LaTeX转换，并提出了一个新的具有挑战性的数据集Img2LaTex-Hard-1K，结果表明其性能比基线方法有显著提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception",
        "summary": "Vision-Language Models (VLMs) are advancing autonomous driving, yet their\npotential is constrained by myopic decision-making and passive perception,\nlimiting reliability in complex environments. We introduce DriveAgent-R1 to\ntackle these challenges in long-horizon, high-level behavioral decision-making.\nDriveAgent-R1 features two core innovations: a Hybrid-Thinking framework that\nadaptively switches between efficient text-based and in-depth tool-based\nreasoning, and an Active Perception mechanism with a vision toolkit to\nproactively resolve uncertainties, thereby balancing decision-making efficiency\nand reliability. The agent is trained using a novel, three-stage progressive\nreinforcement learning strategy designed to master these hybrid capabilities.\nExtensive experiments demonstrate that DriveAgent-R1 achieves state-of-the-art\nperformance, outperforming even leading proprietary large multimodal models,\nsuch as Claude Sonnet 4. Ablation studies validate our approach and confirm\nthat the agent's decisions are robustly grounded in actively perceived visual\nevidence, paving a path toward safer and more intelligent autonomous systems.",
        "url": "http://arxiv.org/abs/2507.20879v1",
        "published_date": "2025-07-28T14:33:15+00:00",
        "updated_date": "2025-07-28T14:33:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weicheng Zheng",
            "Xiaofei Mao",
            "Nanfei Ye",
            "Pengxiang Li",
            "Kun Zhan",
            "Xianpeng Lang",
            "Hang Zhao"
        ],
        "tldr": "DriveAgent-R1 improves autonomous driving by using a hybrid reasoning approach (text & tools) and active perception with a vision toolkit, achieving state-of-the-art performance compared to other VLMs.",
        "tldr_zh": "DriveAgent-R1通过混合推理方法（文本和工具）和主动感知（使用视觉工具包）改进了自动驾驶，其性能优于其他视觉语言模型，达到了最先进的水平。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models",
        "summary": "Vision encoders serve as the cornerstone of multimodal understanding.\nSingle-encoder architectures like CLIP exhibit inherent constraints in\ngeneralizing across diverse multimodal tasks, while recent multi-encoder fusion\nmethods introduce prohibitive computational overhead to achieve superior\nperformance using complementary visual representations from multiple vision\nencoders. To address this, we propose a progressive pruning framework, namely\nMulti-Encoder collaboraTivE tOken pRuning (METEOR), that eliminates redundant\nvisual tokens across the encoding, fusion, and decoding stages for\nmulti-encoder MLLMs. For multi-vision encoding, we discard redundant tokens\nwithin each encoder via a rank guided collaborative token assignment strategy.\nSubsequently, for multi-vision fusion, we combine the visual features from\ndifferent encoders while reducing cross-encoder redundancy with cooperative\npruning. Finally, we propose an adaptive token pruning method in the LLM\ndecoding stage to further discard irrelevant tokens based on the text prompts\nwith dynamically adjusting pruning ratios for specific task demands. To our\nbest knowledge, this is the first successful attempt that achieves an efficient\nmulti-encoder based vision language model with multi-stage pruning strategies.\nExtensive experiments on 11 benchmarks demonstrate the effectiveness of our\nproposed approach. Compared with EAGLE, a typical multi-encoder MLLMs, METEOR\nreduces 76% visual tokens with only 0.3% performance drop in average. The code\nis available at https://github.com/YuchenLiu98/METEOR.",
        "url": "http://arxiv.org/abs/2507.20842v1",
        "published_date": "2025-07-28T13:50:53+00:00",
        "updated_date": "2025-07-28T13:50:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuchen Liu",
            "Yaoming Wang",
            "Bowen Shi",
            "Xiaopeng Zhang",
            "Wenrui Dai",
            "Chenglin Li",
            "Hongkai Xiong",
            "Qi Tian"
        ],
        "tldr": "The paper proposes METEOR, a multi-stage token pruning framework for efficient multi-encoder vision language models, achieving significant token reduction with minimal performance drop.",
        "tldr_zh": "该论文提出了METEOR，一个用于高效多编码器视觉语言模型的多阶段令牌剪枝框架，在性能损失最小的情况下实现了显著的令牌缩减。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning",
        "summary": "Remote sensing (RS) images from multiple modalities and platforms exhibit\ndiverse details due to differences in sensor characteristics and imaging\nperspectives. Existing vision-language research in RS largely relies on\nrelatively homogeneous data sources. Moreover, they still remain limited to\nconventional visual perception tasks such as classification or captioning. As a\nresult, these methods fail to serve as a unified and standalone framework\ncapable of effectively handling RS imagery from diverse sources in real-world\napplications. To address these issues, we propose RingMo-Agent, a model\ndesigned to handle multi-modal and multi-platform data that performs perception\nand reasoning tasks based on user textual instructions. Compared with existing\nmodels, RingMo-Agent 1) is supported by a large-scale vision-language dataset\nnamed RS-VL3M, comprising over 3 million image-text pairs, spanning optical,\nSAR, and infrared (IR) modalities collected from both satellite and UAV\nplatforms, covering perception and challenging reasoning tasks; 2) learns\nmodality adaptive representations by incorporating separated embedding layers\nto construct isolated features for heterogeneous modalities and reduce\ncross-modal interference; 3) unifies task modeling by introducing task-specific\ntokens and employing a token-based high-dimensional hidden state decoding\nmechanism designed for long-horizon spatial tasks. Extensive experiments on\nvarious RS vision-language tasks demonstrate that RingMo-Agent not only proves\neffective in both visual understanding and sophisticated analytical tasks, but\nalso exhibits strong generalizability across different platforms and sensing\nmodalities.",
        "url": "http://arxiv.org/abs/2507.20776v1",
        "published_date": "2025-07-28T12:39:33+00:00",
        "updated_date": "2025-07-28T12:39:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huiyang Hu",
            "Peijin Wang",
            "Yingchao Feng",
            "Kaiwen Wei",
            "Wenxin Yin",
            "Wenhui Diao",
            "Mengyu Wang",
            "Hanbo Bi",
            "Kaiyue Kang",
            "Tong Ling",
            "Kun Fu",
            "Xian Sun"
        ],
        "tldr": "The paper introduces RingMo-Agent, a unified remote sensing vision-language model designed to handle multi-modal, multi-platform data for perception and reasoning tasks, supported by a large-scale RS-VL3M dataset.",
        "tldr_zh": "该论文介绍了RingMo-Agent，一个统一的遥感视觉语言模型，旨在处理多模态、多平台数据，用于感知和推理任务，并由大规模RS-VL3M数据集支持。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Only with Images: Visual Reinforcement Learning with Reasoning, Rendering, and Visual Feedback",
        "summary": "Multimodal Large Language Models (MLLMs) exhibit impressive performance\nacross various visual tasks. Subsequent investigations into enhancing their\nvisual reasoning abilities have significantly expanded their performance\nenvelope. However, a critical bottleneck in the advancement of MLLMs toward\ndeep visual reasoning is their heavy reliance on curated image-text\nsupervision. To solve this problem, we introduce a novel framework termed\n``Reasoning-Rendering-Visual-Feedback'' (RRVF), which enables MLLMs to learn\ncomplex visual reasoning from only raw images. This framework builds on the\n``Asymmetry of Verification'' principle to train MLLMs, i.e., verifying the\nrendered output against a source image is easier than generating it. We\ndemonstrate that this relative ease provides an ideal reward signal for\noptimization via Reinforcement Learning (RL) training, reducing reliance on the\nimage-text supervision. Guided by the above principle, RRVF implements a\nclosed-loop iterative process encompassing reasoning, rendering, and visual\nfeedback components, enabling the model to perform self-correction through\nmulti-turn interactions, while this pipeline can be optimized end-to-end by the\nGRPO algorithm. Extensive evaluations are conducted on image-to-code generation\nacross two diverse domains: data charts and web interfaces. The RRVF-trained\nmodel not only outperforms existing open-source MLLMs and supervised\nfine-tuning baselines but also exhibits superior generalization to unseen\ndatasets. Critically, the model's performance surpasses that of the more\nadvanced MLLM used to provide the feedback signal during training. This work\nestablishes a self-improvement paradigm that offers a viable path to robust,\ngeneralizable models without reliance on explicit supervision. Code will be\navailable at https://github.com/L-O-I/RRVF.",
        "url": "http://arxiv.org/abs/2507.20766v2",
        "published_date": "2025-07-28T12:21:19+00:00",
        "updated_date": "2025-07-30T11:18:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Chen",
            "Yufan Shen",
            "Wenxuan Huang",
            "Sheng Zhou",
            "Qunshu Lin",
            "Xinyu Cai",
            "Zhi Yu",
            "Jiajun Bu",
            "Botian Shi",
            "Yu Qiao"
        ],
        "tldr": "The paper introduces RRVF, a novel framework for training MLLMs to learn visual reasoning from only raw images using reinforcement learning and a closed-loop process of reasoning, rendering, and visual feedback, demonstrating strong performance on image-to-code generation tasks.",
        "tldr_zh": "该论文介绍了一种名为RRVF的新框架，该框架使用强化学习以及推理、渲染和视觉反馈的闭环过程，训练 MLLM 仅从原始图像中学习视觉推理，并在图像到代码生成任务上表现出强大的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model",
        "summary": "Large Vision-Language Models (LVLMs) have advanced multimodal learning but\nface high computational costs due to the large number of visual tokens,\nmotivating token pruning to improve inference efficiency. The key challenge\nlies in identifying which tokens are truly important. Most existing approaches\nrely on attention-based criteria to estimate token importance. However, they\ninherently suffer from certain limitations, such as positional bias. In this\nwork, we explore a new perspective on token importance based on token\ntransitions in LVLMs. We observe that the transition of token representations\nprovides a meaningful signal of semantic information. Based on this insight, we\npropose TransPrune, a training-free and efficient token pruning method.\nSpecifically, TransPrune progressively prunes tokens by assessing their\nimportance through a combination of Token Transition Variation (TTV)-which\nmeasures changes in both the magnitude and direction of token\nrepresentations-and Instruction-Guided Attention (IGA), which measures how\nstrongly the instruction attends to image tokens via attention. Extensive\nexperiments demonstrate that TransPrune achieves comparable multimodal\nperformance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight\nbenchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV\nalone can serve as an effective criterion without relying on attention,\nachieving performance comparable to attention-based methods. The code will be\nmade publicly available upon acceptance of the paper at\nhttps://github.com/liaolea/TransPrune.",
        "url": "http://arxiv.org/abs/2507.20630v1",
        "published_date": "2025-07-28T08:44:58+00:00",
        "updated_date": "2025-07-28T08:44:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ao Li",
            "Yuxiang Duan",
            "Jinghui Zhang",
            "Congbo Ma",
            "Yutong Xie",
            "Gustavo Carneiro",
            "Mohammad Yaqub",
            "Hu Wang"
        ],
        "tldr": "The paper introduces TransPrune, a training-free token pruning method for LVLMs based on token transition variation and instruction-guided attention, achieving significant TFLOPs reduction with comparable performance.",
        "tldr_zh": "该论文介绍了 TransPrune，一种基于 token 转换变化和指令引导注意力的 LVLM 免训练 token 剪枝方法，在性能相当的情况下显著降低了 TFLOPs。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation",
        "summary": "Text-to-Image (T2I) generative models have revolutionized content creation\nbut remain highly sensitive to prompt phrasing, often requiring users to\nrepeatedly refine prompts multiple times without clear feedback. While\ntechniques such as automatic prompt engineering, controlled text embeddings,\ndenoising, and multi-turn generation mitigate these issues, they offer limited\ncontrollability, or often necessitate additional training, restricting the\ngeneralization abilities. Thus, we introduce T2I-Copilot, a training-free\nmulti-agent system that leverages collaboration between (Multimodal) Large\nLanguage Models to automate prompt phrasing, model selection, and iterative\nrefinement. This approach significantly simplifies prompt engineering while\nenhancing generation quality and text-image alignment compared to direct\ngeneration. Specifically, T2I-Copilot consists of three agents: (1) Input\nInterpreter, which parses the input prompt, resolves ambiguities, and generates\na standardized report; (2) Generation Engine, which selects the appropriate\nmodel from different types of T2I models and organizes visual and textual\nprompts to initiate generation; and (3) Quality Evaluator, which assesses\naesthetic quality and text-image alignment, providing scores and feedback for\npotential regeneration. T2I-Copilot can operate fully autonomously while also\nsupporting human-in-the-loop intervention for fine-grained control. On\nGenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA\nscore comparable to commercial models RecraftV3 and Imagen 3, surpasses\nFLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and\nSD 3.5 Large by 9.11% and 6.36%. Code will be released at:\nhttps://github.com/SHI-Labs/T2I-Copilot.",
        "url": "http://arxiv.org/abs/2507.20536v2",
        "published_date": "2025-07-28T05:41:22+00:00",
        "updated_date": "2025-07-29T06:16:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC"
        ],
        "authors": [
            "Chieh-Yun Chen",
            "Min Shi",
            "Gong Zhang",
            "Humphrey Shi"
        ],
        "tldr": "The paper introduces T2I-Copilot, a training-free multi-agent system using LLMs to improve text-to-image generation through automated prompt engineering, model selection, and iterative refinement, achieving competitive results on GenAI-Bench.",
        "tldr_zh": "该论文介绍了T2I-Copilot，一个无需训练的多智能体系统，利用大型语言模型通过自动提示工程、模型选择和迭代改进来提升文本到图像的生成效果，并在GenAI-Bench上取得了有竞争力的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Spatial Reasoning through Visual and Textual Thinking",
        "summary": "The spatial reasoning task aims to reason about the spatial relationships in\n2D and 3D space, which is a fundamental capability for Visual Question\nAnswering (VQA) and robotics. Although vision language models (VLMs) have\ndeveloped rapidly in recent years, they are still struggling with the spatial\nreasoning task. In this paper, we introduce a method that can enhance Spatial\nreasoning through Visual and Textual thinking Simultaneously (SpatialVTS). In\nthe spatial visual thinking phase, our model is trained to generate\nlocation-related specific tokens of essential targets automatically. Not only\nare the objects mentioned in the problem addressed, but also the potential\nobjects related to the reasoning are considered. During the spatial textual\nthinking phase, Our model conducts long-term thinking based on visual cues and\ndialogues, gradually inferring the answers to spatial reasoning problems. To\neffectively support the model's training, we perform manual corrections to the\nexisting spatial reasoning dataset, eliminating numerous incorrect labels\nresulting from automatic annotation, restructuring the data input format to\nenhance generalization ability, and developing thinking processes with logical\nreasoning details. Without introducing additional information (such as masks or\ndepth), our model's overall average level in several spatial understanding\ntasks has significantly improved compared with other models.",
        "url": "http://arxiv.org/abs/2507.20529v1",
        "published_date": "2025-07-28T05:24:54+00:00",
        "updated_date": "2025-07-28T05:24:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xun Liang",
            "Xin Guo",
            "Zhongming Jin",
            "Weihang Pan",
            "Penghui Shang",
            "Deng Cai",
            "Binbin Lin",
            "Jieping Ye"
        ],
        "tldr": "The paper introduces SpatialVTS, a method enhancing spatial reasoning in VLMs through visual and textual thinking, involving location-specific token generation and long-term inferential dialogue, and improving existing spatial reasoning datasets.",
        "tldr_zh": "该论文介绍了一种名为SpatialVTS的方法，通过视觉和文本思考来增强视觉语言模型中的空间推理能力，包括生成特定于位置的token和进行长期推理对话，并改进了现有的空间推理数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AgroBench: Vision-Language Model Benchmark in Agriculture",
        "summary": "Precise automated understanding of agricultural tasks such as disease\nidentification is essential for sustainable crop production. Recent advances in\nvision-language models (VLMs) are expected to further expand the range of\nagricultural tasks by facilitating human-model interaction through easy,\ntext-based communication. Here, we introduce AgroBench (Agronomist AI\nBenchmark), a benchmark for evaluating VLM models across seven agricultural\ntopics, covering key areas in agricultural engineering and relevant to\nreal-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is\nannotated by expert agronomists. Our AgroBench covers a state-of-the-art range\nof categories, including 203 crop categories and 682 disease categories, to\nthoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal\nthat VLMs have room for improvement in fine-grained identification tasks.\nNotably, in weed identification, most open-source VLMs perform close to random.\nWith our wide range of topics and expert-annotated categories, we analyze the\ntypes of errors made by VLMs and suggest potential pathways for future VLM\ndevelopment. Our dataset and code are available at\nhttps://dahlian00.github.io/AgroBenchPage/ .",
        "url": "http://arxiv.org/abs/2507.20519v1",
        "published_date": "2025-07-28T04:58:29+00:00",
        "updated_date": "2025-07-28T04:58:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Risa Shinoda",
            "Nakamasa Inoue",
            "Hirokatsu Kataoka",
            "Masaki Onishi",
            "Yoshitaka Ushiku"
        ],
        "tldr": "The paper introduces AgroBench, a new expert-annotated VLM benchmark for agriculture, revealing the limitations of current VLMs in fine-grained identification tasks like weed identification.",
        "tldr_zh": "该论文介绍了AgroBench，这是一个新的由专家注释的农业VLM基准，揭示了当前VLM在细粒度识别任务（如杂草识别）中的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VESPA: Towards un(Human)supervised Open-World Pointcloud Labeling for Autonomous Driving",
        "summary": "Data collection for autonomous driving is rapidly accelerating, but manual\nannotation, especially for 3D labels, remains a major bottleneck due to its\nhigh cost and labor intensity. Autolabeling has emerged as a scalable\nalternative, allowing the generation of labels for point clouds with minimal\nhuman intervention. While LiDAR-based autolabeling methods leverage geometric\ninformation, they struggle with inherent limitations of lidar data, such as\nsparsity, occlusions, and incomplete object observations. Furthermore, these\nmethods typically operate in a class-agnostic manner, offering limited semantic\ngranularity. To address these challenges, we introduce VESPA, a multimodal\nautolabeling pipeline that fuses the geometric precision of LiDAR with the\nsemantic richness of camera images. Our approach leverages vision-language\nmodels (VLMs) to enable open-vocabulary object labeling and to refine detection\nquality directly in the point cloud domain. VESPA supports the discovery of\nnovel categories and produces high-quality 3D pseudolabels without requiring\nground-truth annotations or HD maps. On Nuscenes dataset, VESPA achieves an AP\nof 52.95% for object discovery and up to 46.54% for multiclass object\ndetection, demonstrating strong performance in scalable 3D scene understanding.\nCode will be available upon acceptance.",
        "url": "http://arxiv.org/abs/2507.20397v1",
        "published_date": "2025-07-27T19:39:29+00:00",
        "updated_date": "2025-07-27T19:39:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Levente Tempfli",
            "Esteban Rivera",
            "Markus Lienkamp"
        ],
        "tldr": "The paper introduces VESPA, a multimodal autolabeling pipeline that combines LiDAR and camera data with vision-language models for open-vocabulary object labeling in autonomous driving, achieving strong performance on the NuScenes dataset without ground-truth annotations or HD maps.",
        "tldr_zh": "该论文介绍了VESPA，一个多模态自动标注流程，它结合了激光雷达和相机数据以及视觉语言模型，用于自动驾驶中的开放词汇对象标注，并在NuScenes数据集上取得了优异的性能，无需地面真值标注或高清地图。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image Segmentation",
        "summary": "Referring Image Segmentation (RIS), which aims to segment specific objects\nbased on natural language descriptions, plays an essential role in\nvision-language understanding. Despite its progress in remote sensing\napplications, RIS in Low-Altitude Drone (LAD) scenarios remains underexplored.\nExisting datasets and methods are typically designed for high-altitude and\nstatic-view imagery. They struggle to handle the unique characteristics of LAD\nviews, such as diverse viewpoints and high object density. To fill this gap, we\npresent RIS-LAD, the first fine-grained RIS benchmark tailored for LAD\nscenarios. This dataset comprises 13,871 carefully annotated image-text-mask\ntriplets collected from realistic drone footage, with a focus on small,\ncluttered, and multi-viewpoint scenes. It highlights new challenges absent in\nprevious benchmarks, such as category drift caused by tiny objects and object\ndrift under crowded same-class objects. To tackle these issues, we propose the\nSemantic-Aware Adaptive Reasoning Network (SAARN). Rather than uniformly\ninjecting all linguistic features, SAARN decomposes and routes semantic\ninformation to different stages of the network. Specifically, the\nCategory-Dominated Linguistic Enhancement (CDLE) aligns visual features with\nobject categories during early encoding, while the Adaptive Reasoning Fusion\nModule (ARFM) dynamically selects semantic cues across scales to improve\nreasoning in complex scenes. The experimental evaluation reveals that RIS-LAD\npresents substantial challenges to state-of-the-art RIS algorithms, and also\ndemonstrates the effectiveness of our proposed model in addressing these\nchallenges. The dataset and code will be publicly released soon at:\nhttps://github.com/AHideoKuzeA/RIS-LAD/.",
        "url": "http://arxiv.org/abs/2507.20920v1",
        "published_date": "2025-07-28T15:21:03+00:00",
        "updated_date": "2025-07-28T15:21:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kai Ye",
            "YingShi Luan",
            "Zhudi Chen",
            "Guangyue Meng",
            "Pingyang Dai",
            "Liujuan Cao"
        ],
        "tldr": "The paper introduces RIS-LAD, a new benchmark dataset for Referring Image Segmentation in Low-Altitude Drone imagery, and proposes a Semantic-Aware Adaptive Reasoning Network (SAARN) to address the challenges of this domain.",
        "tldr_zh": "该论文介绍了RIS-LAD，一个新的低空无人机图像分割基准数据集，并提出了一种语义感知自适应推理网络（SAARN）来解决该领域的挑战。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection",
        "summary": "The rapid evolution of face manipulation techniques poses a critical\nchallenge for face forgery detection: cross-domain generalization. Conventional\nmethods, which rely on simple classification objectives, often fail to learn\ndomain-invariant representations. We propose HAMLET-FFD, a cognitively inspired\nHierarchical Adaptive Multi-modal Learning framework that tackles this\nchallenge via bidirectional cross-modal reasoning. Building on contrastive\nvision-language models such as CLIP, HAMLET-FFD introduces a knowledge\nrefinement loop that iteratively assesses authenticity by integrating visual\nevidence with conceptual cues, emulating expert forensic analysis. A key\ninnovation is a bidirectional fusion mechanism in which textual authenticity\nembeddings guide the aggregation of hierarchical visual features, while\nmodulated visual features refine text embeddings to generate image-adaptive\nprompts. This closed-loop process progressively aligns visual observations with\nsemantic priors to enhance authenticity assessment. By design, HAMLET-FFD\nfreezes all pretrained parameters, serving as an external plugin that preserves\nCLIP's original capabilities. Extensive experiments demonstrate its superior\ngeneralization to unseen manipulations across multiple benchmarks, and visual\nanalyses reveal a division of labor among embeddings, with distinct\nrepresentations specializing in fine-grained artifact recognition.",
        "url": "http://arxiv.org/abs/2507.20913v1",
        "published_date": "2025-07-28T15:09:52+00:00",
        "updated_date": "2025-07-28T15:09:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jialei Cui",
            "Jianwei Du",
            "Yanzhe Li",
            "Lei Gao",
            "Hui Jiang",
            "Chenfu Bao"
        ],
        "tldr": "The paper introduces HAMLET-FFD, a novel framework for face forgery detection that uses bidirectional cross-modal reasoning between visual and textual embeddings to improve generalization across different manipulation techniques.",
        "tldr_zh": "该论文介绍了 HAMLET-FFD，一种用于人脸伪造检测的新框架，它使用视觉和文本嵌入之间的双向跨模态推理来提高在不同操作技术中的泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the Inductive Setting",
        "summary": "CLIP is a foundational model with transferable classification performance in\nthe few-shot setting. Several methods have shown improved performance of CLIP\nusing few-shot examples. However, so far, all these techniques have been\nbenchmarked using standard few-shot datasets. We argue that this mode of\nevaluation does not provide a true indication of the inductive generalization\nability using few-shot examples. As most datasets have been seen by the CLIP\nmodel, the resultant setting can be termed as partially transductive. To solve\nthis, we propose a pipeline that uses an unlearning technique to obtain true\ninductive baselines. In this new inductive setting, the methods show a\nsignificant drop in performance (-55% on average among 13 baselines with\nmultiple datasets). We validate the unlearning technique using oracle\nbaselines. An improved few-shot classification technique is proposed that\nconsistently obtains state-of-the-art performance over 13 other recent baseline\nmethods on a comprehensive analysis with 5880 experiments - varying the\ndatasets, differing number of few-shot examples, unlearning setting, and with\ndifferent seeds. Thus, we identify the issue with the evaluation of CLIP-based\nfew-shot classification, provide a solution using unlearning, propose new\nbenchmarks, and provide an improved method.",
        "url": "http://arxiv.org/abs/2507.20834v1",
        "published_date": "2025-07-28T13:41:24+00:00",
        "updated_date": "2025-07-28T13:41:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexey Kravets",
            "Da Chen",
            "Vinay P. Namboodiri"
        ],
        "tldr": "The paper identifies a flaw in how few-shot CLIP models are evaluated (partially transductive setting), proposes an unlearning-based solution for inductive evaluation, and introduces a new few-shot classification method achieving state-of-the-art performance on new benchmarks.",
        "tldr_zh": "该论文指出了一种评估小样本 CLIP 模型的方法存在缺陷（部分转导设置），提出了一种基于非学习的归纳评估解决方案，并引入了一种新的小样本分类方法，在新基准测试中实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study",
        "summary": "While Multimodal Large Language Models (MLLMs) demonstrate impressive\ncapabilities, their substantial computational and memory requirements pose\nsignificant barriers to practical deployment. Current parameter reduction\ntechniques primarily involve training MLLMs from Small Language Models (SLMs),\nbut these methods offer limited flexibility and remain computationally\nintensive. To address this gap, we propose to directly compress existing MLLMs\nthrough structural pruning combined with efficient recovery training.\nSpecifically, we investigate two structural pruning paradigms--layerwise and\nwidthwise pruning--applied to the language model backbone of MLLMs, alongside\nsupervised finetuning and knowledge distillation. Additionally, we assess the\nfeasibility of conducting recovery training with only a small fraction of the\navailable data. Our results show that widthwise pruning generally maintains\nbetter performance in low-resource scenarios with limited computational\nresources or insufficient finetuning data. As for the recovery training,\nfinetuning only the multimodal projector is sufficient at small compression\nlevels (< 20%). Furthermore, a combination of supervised finetuning and\nhidden-state distillation yields optimal recovery across various pruning\nlevels. Notably, effective recovery can be achieved with as little as 5% of the\noriginal training data, while retaining over 95% of the original performance.\nThrough empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and\nBunny-v1.0-3B, this study offers actionable insights for practitioners aiming\nto compress MLLMs effectively without extensive computation resources or\nsufficient data.",
        "url": "http://arxiv.org/abs/2507.20749v1",
        "published_date": "2025-07-28T11:57:52+00:00",
        "updated_date": "2025-07-28T11:57:52+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Yiran Huang",
            "Lukas Thede",
            "Massimiliano Mancini",
            "Wenjia Xu",
            "Zeynep Akata"
        ],
        "tldr": "This paper explores structural pruning and recovery techniques for compressing MLLMs, demonstrating effective compression with minimal performance loss and reduced data requirements, offering practical guidance for resource-constrained scenarios.",
        "tldr_zh": "本文研究了压缩多模态大型语言模型的结构化剪枝和恢复技术，证明了在性能损失最小和数据需求减少的情况下实现有效压缩，为资源受限的场景提供了实用指导。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond Class Tokens: LLM-guided Dominant Property Mining for Few-shot Classification",
        "summary": "Few-shot Learning (FSL), which endeavors to develop the generalization\nability for recognizing novel classes using only a few images, faces\nsignificant challenges due to data scarcity. Recent CLIP-like methods based on\ncontrastive language-image pertaining mitigate the issue by leveraging textual\nrepresentation of the class name for unseen image discovery. Despite the\nachieved success, simply aligning visual representations to class name\nembeddings would compromise the visual diversity for novel class\ndiscrimination. To this end, we proposed a novel Few-Shot Learning (FSL) method\n(BCT-CLIP) that explores \\textbf{dominating properties} via contrastive\nlearning beyond simply using class tokens. Through leveraging LLM-based prior\nknowledge, our method pushes forward FSL with comprehensive structural image\nrepresentations, including both global category representation and the\npatch-aware property embeddings. In particular, we presented a novel\nmulti-property generator (MPG) with patch-aware cross-attentions to generate\nmultiple visual property tokens, a Large-Language Model (LLM)-assistant\nretrieval procedure with clustering-based pruning to obtain dominating property\ndescriptions, and a new contrastive learning strategy for property-token\nlearning. The superior performances on the 11 widely used datasets demonstrate\nthat our investigation of dominating properties advances discriminative\nclass-specific representation learning and few-shot classification.",
        "url": "http://arxiv.org/abs/2507.20511v2",
        "published_date": "2025-07-28T04:23:06+00:00",
        "updated_date": "2025-07-29T07:25:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Zhuo",
            "Runjie Luo",
            "Wufeng Xue",
            "Linlin Shen"
        ],
        "tldr": "This paper introduces BCT-CLIP, a novel few-shot learning method that leverages LLMs to mine dominating visual properties beyond class tokens for improved image classification. It achieves state-of-the-art performance on multiple datasets by generating patch-aware property embeddings and using a new contrastive learning strategy.",
        "tldr_zh": "本文提出了一种名为BCT-CLIP的新型少样本学习方法，该方法利用大型语言模型挖掘超越类别标记的显性视觉属性，从而改进图像分类。通过生成感知patch的属性嵌入并使用一种新的对比学习策略，该方法在多个数据集上实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder",
        "summary": "Text-to-image (T2I) diffusion models often exhibit gender bias, particularly\nby generating stereotypical associations between professions and gendered\nsubjects. This paper presents SAE Debias, a lightweight and model-agnostic\nframework for mitigating such bias in T2I generation. Unlike prior approaches\nthat rely on CLIP-based filtering or prompt engineering, which often require\nmodel-specific adjustments and offer limited control, SAE Debias operates\ndirectly within the feature space without retraining or architectural\nmodifications. By leveraging a k-sparse autoencoder pre-trained on a gender\nbias dataset, the method identifies gender-relevant directions within the\nsparse latent space, capturing professional stereotypes. Specifically, a biased\ndirection per profession is constructed from sparse latents and suppressed\nduring inference to steer generations toward more gender-balanced outputs.\nTrained only once, the sparse autoencoder provides a reusable debiasing\ndirection, offering effective control and interpretable insight into biased\nsubspaces. Extensive evaluations across multiple T2I models, including Stable\nDiffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially\nreduces gender bias while preserving generation quality. To the best of our\nknowledge, this is the first work to apply sparse autoencoders for identifying\nand intervening in gender bias within T2I models. These findings contribute\ntoward building socially responsible generative AI, providing an interpretable\nand model-agnostic tool to support fairness in text-to-image generation.",
        "url": "http://arxiv.org/abs/2507.20973v1",
        "published_date": "2025-07-28T16:36:13+00:00",
        "updated_date": "2025-07-28T16:36:13+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Chao Wu",
            "Zhenyi Wang",
            "Kangxian Xie",
            "Naresh Kumar Devulapally",
            "Vishnu Suresh Lokhande",
            "Mingchen Gao"
        ],
        "tldr": "The paper introduces SAE Debias, a model-agnostic framework using sparse autoencoders to mitigate gender bias in text-to-image diffusion models by identifying and suppressing gender-relevant directions in the latent space, showing substantial bias reduction across various models without retraining.",
        "tldr_zh": "该论文介绍了一种名为SAE Debias的与模型无关的框架，该框架使用稀疏自编码器来减轻文本到图像扩散模型中的性别偏见，通过识别和抑制潜在空间中与性别相关的方向，在各种模型中显著减少偏见，且无需重新训练。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]