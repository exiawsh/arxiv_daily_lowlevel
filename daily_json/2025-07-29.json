[
    {
        "title": "Onboard Hyperspectral Super-Resolution with Deep Pushbroom Neural Network",
        "summary": "Hyperspectral imagers on satellites obtain the fine spectral signatures\nessential for distinguishing one material from another at the expense of\nlimited spatial resolution. Enhancing the latter is thus a desirable\npreprocessing step in order to further improve the detection capabilities\noffered by hyperspectral images on downstream tasks. At the same time, there is\na growing interest towards deploying inference methods directly onboard of\nsatellites, which calls for lightweight image super-resolution methods that can\nbe run on the payload in real time. In this paper, we present a novel neural\nnetwork design, called Deep Pushbroom Super-Resolution (DPSR) that matches the\npushbroom acquisition of hyperspectral sensors by processing an image line by\nline in the along-track direction with a causal memory mechanism to exploit\npreviously acquired lines. This design greatly limits memory requirements and\ncomputational complexity, achieving onboard real-time performance, i.e., the\nability to super-resolve a line in the time it takes to acquire the next one,\non low-power hardware. Experiments show that the quality of the super-resolved\nimages is competitive or even outperforms state-of-the-art methods that are\nsignificantly more complex.",
        "url": "http://arxiv.org/abs/2507.20765v1",
        "published_date": "2025-07-28T12:18:52+00:00",
        "updated_date": "2025-07-28T12:18:52+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Davide Piccinini",
            "Diego Valsesia",
            "Enrico Magli"
        ],
        "tldr": "This paper introduces Deep Pushbroom Super-Resolution (DPSR), a lightweight neural network designed for onboard hyperspectral image super-resolution, optimized for real-time performance on low-power hardware.",
        "tldr_zh": "本文介绍了一种名为深度推扫超分辨率（DPSR）的轻量级神经网络，专为星载高光谱图像超分辨率设计，并针对低功耗硬件上的实时性能进行了优化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Harnessing Diffusion-Yielded Score Priors for Image Restoration",
        "summary": "Deep image restoration models aim to learn a mapping from degraded image\nspace to natural image space. However, they face several critical challenges:\nremoving degradation, generating realistic details, and ensuring pixel-level\nconsistency. Over time, three major classes of methods have emerged, including\nMSE-based, GAN-based, and diffusion-based methods. However, they fail to\nachieve a good balance between restoration quality, fidelity, and speed. We\npropose a novel method, HYPIR, to address these challenges. Our solution\npipeline is straightforward: it involves initializing the image restoration\nmodel with a pre-trained diffusion model and then fine-tuning it with\nadversarial training. This approach does not rely on diffusion loss, iterative\nsampling, or additional adapters. We theoretically demonstrate that\ninitializing adversarial training from a pre-trained diffusion model positions\nthe initial restoration model very close to the natural image distribution.\nConsequently, this initialization improves numerical stability, avoids mode\ncollapse, and substantially accelerates the convergence of adversarial\ntraining. Moreover, HYPIR inherits the capabilities of diffusion models with\nrich user control, enabling text-guided restoration and adjustable texture\nrichness. Requiring only a single forward pass, it achieves faster convergence\nand inference speed than diffusion-based methods. Extensive experiments show\nthat HYPIR outperforms previous state-of-the-art methods, achieving efficient\nand high-quality image restoration.",
        "url": "http://arxiv.org/abs/2507.20590v2",
        "published_date": "2025-07-28T07:55:34+00:00",
        "updated_date": "2025-07-29T23:59:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinqi Lin",
            "Fanghua Yu",
            "Jinfan Hu",
            "Zhiyuan You",
            "Wu Shi",
            "Jimmy S. Ren",
            "Jinjin Gu",
            "Chao Dong"
        ],
        "tldr": "The paper introduces HYPIR, a novel image restoration method that leverages a pre-trained diffusion model for initialization and fine-tunes it with adversarial training, achieving faster convergence, better restoration quality, and user control capabilities.",
        "tldr_zh": "该论文介绍了一种新的图像修复方法 HYPIR，它利用预训练的扩散模型进行初始化，并通过对抗训练进行微调，从而实现更快的收敛速度、更好的修复质量和用户控制能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation",
        "summary": "Text-to-Image (T2I) generative models have revolutionized content creation\nbut remain highly sensitive to prompt phrasing, often requiring users to\nrepeatedly refine prompts multiple times without clear feedback. While\ntechniques such as automatic prompt engineering, controlled text embeddings,\ndenoising, and multi-turn generation mitigate these issues, they offer limited\ncontrollability, or often necessitate additional training, restricting the\ngeneralization abilities. Thus, we introduce T2I-Copilot, a training-free\nmulti-agent system that leverages collaboration between (Multimodal) Large\nLanguage Models to automate prompt phrasing, model selection, and iterative\nrefinement. This approach significantly simplifies prompt engineering while\nenhancing generation quality and text-image alignment compared to direct\ngeneration. Specifically, T2I-Copilot consists of three agents: (1) Input\nInterpreter, which parses the input prompt, resolves ambiguities, and generates\na standardized report; (2) Generation Engine, which selects the appropriate\nmodel from different types of T2I models and organizes visual and textual\nprompts to initiate generation; and (3) Quality Evaluator, which assesses\naesthetic quality and text-image alignment, providing scores and feedback for\npotential regeneration. T2I-Copilot can operate fully autonomously while also\nsupporting human-in-the-loop intervention for fine-grained control. On\nGenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA\nscore comparable to commercial models RecraftV3 and Imagen 3, surpasses\nFLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and\nSD 3.5 Large by 9.11% and 6.36%. Code will be released at:\nhttps://github.com/SHI-Labs/T2I-Copilot.",
        "url": "http://arxiv.org/abs/2507.20536v2",
        "published_date": "2025-07-28T05:41:22+00:00",
        "updated_date": "2025-07-29T06:16:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC"
        ],
        "authors": [
            "Chieh-Yun Chen",
            "Min Shi",
            "Gong Zhang",
            "Humphrey Shi"
        ],
        "tldr": "The paper introduces T2I-Copilot, a training-free multi-agent system that improves text-to-image generation by automating prompt engineering, model selection, and iterative refinement, achieving competitive results with commercial models.",
        "tldr_zh": "该论文介绍了T2I-Copilot，一个无需训练的多智能体系统，通过自动化提示工程、模型选择和迭代改进来提升文本到图像的生成效果，并取得了与商业模型相媲美的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis",
        "summary": "Visual autoregressive modeling, based on the next-scale prediction paradigm,\nexhibits notable advantages in image quality and model scalability over\ntraditional autoregressive and diffusion models. It generates images by\nprogressively refining resolution across multiple stages. However, the\ncomputational overhead in high-resolution stages remains a critical challenge\ndue to the substantial number of tokens involved. In this paper, we introduce\nSparseVAR, a plug-and-play acceleration framework for next-scale prediction\nthat dynamically excludes low-frequency tokens during inference without\nrequiring additional training. Our approach is motivated by the observation\nthat tokens in low-frequency regions have a negligible impact on image quality\nin high-resolution stages and exhibit strong similarity with neighboring\ntokens. Additionally, we observe that different blocks in the next-scale\nprediction model focus on distinct regions, with some concentrating on\nhigh-frequency areas. SparseVAR leverages these insights by employing\nlightweight MSE-based metrics to identify low-frequency tokens while preserving\nthe fidelity of excluded regions through a small set of uniformly sampled\nanchor tokens. By significantly reducing the computational cost while\nmaintaining high image generation quality, SparseVAR achieves notable\nacceleration in both HART and Infinity. Specifically, SparseVAR achieves up to\na 2 times speedup with minimal quality degradation in Infinity-2B.",
        "url": "http://arxiv.org/abs/2507.20454v1",
        "published_date": "2025-07-28T01:13:24+00:00",
        "updated_date": "2025-07-28T01:13:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhuokun Chen",
            "Jugang Fan",
            "Zhuowei Yu",
            "Bohan Zhuang",
            "Mingkui Tan"
        ],
        "tldr": "The paper introduces SparseVAR, a plug-and-play acceleration framework for high-resolution image generation that dynamically excludes low-frequency tokens during inference, achieving significant speedups with minimal quality degradation.",
        "tldr_zh": "该论文介绍了SparseVAR，一个即插即用的高分辨率图像生成加速框架，它在推理过程中动态排除低频tokens，从而在最小化质量损失的同时实现显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex Envelope",
        "summary": "Sparse regularization is fundamental in signal processing for efficient\nsignal recovery and feature extraction. However, it faces a fundamental\ndilemma: the most powerful sparsity-inducing penalties are often\nnon-differentiable, conflicting with gradient-based optimizers that dominate\nthe field. We introduce WEEP (Weakly-convex Envelope of Piecewise Penalty), a\nnovel, fully differentiable sparse regularizer derived from the weakly-convex\nenvelope framework. WEEP provides strong, unbiased sparsity while maintaining\nfull differentiability and L-smoothness, making it natively compatible with any\ngradient-based optimizer. This resolves the conflict between statistical\nperformance and computational tractability. We demonstrate superior performance\ncompared to the L1-norm and other established non-convex sparse regularizers on\nchallenging signal and image denoising tasks.",
        "url": "http://arxiv.org/abs/2507.20447v1",
        "published_date": "2025-07-28T00:40:48+00:00",
        "updated_date": "2025-07-28T00:40:48+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Takanobu Furuhashi",
            "Hidekata Hontani",
            "Tatsuya Yokota"
        ],
        "tldr": "The paper introduces WEEP, a novel, fully differentiable sparse regularizer for signal processing that balances sparsity and compatibility with gradient-based optimization, demonstrating superior performance in denoising tasks.",
        "tldr_zh": "该论文介绍了一种名为WEEP的新型全可微稀疏正则化方法，用于信号处理。该方法平衡了稀疏性和与基于梯度的优化的兼容性，并在去噪任务中表现出卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PanoGAN A Deep Generative Model for Panoramic Dental Radiographs",
        "summary": "This paper presents the development of a generative adversarial network (GAN)\nfor synthesizing dental panoramic radiographs. Although exploratory in nature,\nthe study aims to address the scarcity of data in dental research and\neducation. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss\nwith gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying\nquality. The focus was on the dentoalveolar regions, other anatomical\nstructures were cropped out. Extensive preprocessing and data cleaning were\nperformed to standardize the inputs while preserving anatomical variability. We\nexplored four candidate models by varying critic iterations, feature depth, and\nthe use of denoising prior to training. A clinical expert evaluated the\ngenerated radiographs based on anatomical visibility and realism, using a\n5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical\ndepiction, although some were degraded by artifacts. A trade-off was observed\nthe model trained on non-denoised data yielded finer details especially in\nstructures like the mandibular canal and trabecular bone, while a model trained\non denoised data offered superior overall image clarity and sharpness. These\nfindings provide a foundation for future work on GAN-based methods in dental\nimaging.",
        "url": "http://arxiv.org/abs/2507.21200v1",
        "published_date": "2025-07-28T10:55:44+00:00",
        "updated_date": "2025-07-28T10:55:44+00:00",
        "categories": [
            "cs.CV",
            "cs.ET",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Soren Pedersen",
            "Sanyam Jain",
            "Mikkel Chavez",
            "Viktor Ladehoff",
            "Bruna Neves de Freitas",
            "Ruben Pauwels"
        ],
        "tldr": "This paper explores using DCGANs to generate panoramic dental radiographs, aiming to address data scarcity in dental research and education. The generated images show moderate anatomical depiction, with trade-offs between detail and clarity.",
        "tldr_zh": "本文探讨了使用DCGAN生成全景牙科X光片，旨在解决牙科研究和教育中的数据稀缺问题。生成的图像显示出适度的解剖结构，但在细节和清晰度之间存在权衡。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]