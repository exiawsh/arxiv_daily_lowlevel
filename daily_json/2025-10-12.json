[
    {
        "title": "Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?",
        "summary": "Recent advances in large generative models have shown that simple\nautoregressive formulations, when scaled appropriately, can exhibit strong\nzero-shot generalization across domains. Motivated by this trend, we\ninvestigate whether autoregressive video modeling principles can be directly\napplied to medical imaging tasks, despite the model never being trained on\nmedical data. Specifically, we evaluate a large vision model (LVM) in a\nzero-shot setting across four representative tasks: organ segmentation,\ndenoising, super-resolution, and motion prediction. Remarkably, even without\ndomain-specific fine-tuning, the LVM can delineate anatomical structures in CT\nscans and achieve competitive performance on segmentation, denoising, and\nsuper-resolution. Most notably, in radiotherapy motion prediction, the model\nforecasts future 3D CT phases directly from prior phases of a 4D CT scan,\nproducing anatomically consistent predictions that capture patient-specific\nrespiratory dynamics with realistic temporal coherence. We evaluate the LVM on\n4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite no\nprior exposure to medical data, the model achieves strong performance across\nall tasks and surpasses specialized DVF-based and generative baselines in\nmotion prediction, achieving state-of-the-art spatial accuracy. These findings\nreveal the emergence of zero-shot capabilities in medical video modeling and\nhighlight the potential of general-purpose video models to serve as unified\nlearners and reasoners laying the groundwork for future medical foundation\nmodels built on video models.",
        "url": "http://arxiv.org/abs/2510.10254v1",
        "published_date": "2025-10-11T15:19:03+00:00",
        "updated_date": "2025-10-11T15:19:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxiang Lai",
            "Jike Zhong",
            "Ming Li",
            "Yuheng Li",
            "Xiaofeng Yang"
        ],
        "tldr": "This paper explores the zero-shot capabilities of a large vision model (LVM) on medical imaging tasks, demonstrating surprisingly good performance in organ segmentation, denoising, super-resolution, and especially motion prediction without any medical data training.",
        "tldr_zh": "本文探索了大型视觉模型（LVM）在医学影像任务中的零样本能力，展示了在器官分割、去噪、超分辨率，尤其是运动预测方面出色的性能，而无需任何医学数据训练。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeepFusionNet: Autoencoder-Based Low-Light Image Enhancement and Super-Resolution",
        "summary": "Computer vision and image processing applications suffer from dark and\nlow-light images, particularly during real-time image transmission. Currently,\nlow light and dark images are converted to bright and colored forms using\nautoencoders; however, these methods often achieve low SSIM and PSNR scores and\nrequire high computational power due to their large number of parameters. To\naddress these challenges, the DeepFusionNet architecture has been developed.\nAccording to the results obtained with the LOL-v1 dataset, DeepFusionNet\nachieved an SSIM of 92.8% and a PSNR score of 26.30, while containing only\napproximately 2.5 million parameters. On the other hand, conversion of blurry\nand low-resolution images into high-resolution and blur-free images has gained\nimportance in image processing applications. Unlike GAN-based super-resolution\nmethods, an autoencoder-based super resolution model has been developed that\ncontains approximately 100 thousand parameters and uses the DeepFusionNet\narchitecture. According to the results of the tests, the DeepFusionNet based\nsuper-resolution method achieved a PSNR of 25.30 and a SSIM score of 80.7\npercent according to the validation set.",
        "url": "http://arxiv.org/abs/2510.10122v1",
        "published_date": "2025-10-11T09:04:22+00:00",
        "updated_date": "2025-10-11T09:04:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T45, 68T10",
            "I.2.10; I.4.9"
        ],
        "authors": [
            "Halil Hüseyin Çalışkan",
            "Talha Koruk"
        ],
        "tldr": "The paper introduces DeepFusionNet, an autoencoder-based architecture for low-light image enhancement and super-resolution, claiming competitive performance with fewer parameters than existing methods.",
        "tldr_zh": "该论文介绍了DeepFusionNet，一种基于自动编码器的低光图像增强和超分辨率架构，声称与现有方法相比，性能具有竞争力且参数更少。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enabling High-Quality In-the-Wild Imaging from Severely Aberrated Metalens Bursts",
        "summary": "We tackle the challenge of robust, in-the-wild imaging using ultra-thin\nnanophotonic metalens cameras. Meta-lenses, composed of planar arrays of\nnanoscale scatterers, promise dramatic reductions in size and weight compared\nto conventional refractive optics. However, severe chromatic aberration,\npronounced light scattering, narrow spectral bandwidth, and low light\nefficiency continue to limit their practical adoption. In this work, we present\nan end-to-end solution for in-the-wild imaging that pairs a metalens several\ntimes thinner than conventional optics with a bespoke multi-image restoration\nframework optimized for practical metalens cameras. Our method centers on a\nlightweight convolutional network paired with a memory-efficient burst fusion\nalgorithm that adaptively corrects noise, saturation clipping, and lens-induced\ndistortions across rapid sequences of extremely degraded metalens captures.\nExtensive experiments on diverse, real-world handheld captures demonstrate that\nour approach consistently outperforms existing burst-mode and single-image\nrestoration techniques.These results point toward a practical route for\ndeploying metalens-based cameras in everyday imaging applications.",
        "url": "http://arxiv.org/abs/2510.10083v1",
        "published_date": "2025-10-11T07:41:04+00:00",
        "updated_date": "2025-10-11T07:41:04+00:00",
        "categories": [
            "physics.optics",
            "cs.CV"
        ],
        "authors": [
            "Debabrata Mandal",
            "Zhihan Peng",
            "Yujie Wang",
            "Praneeth Chakravarthula"
        ],
        "tldr": "This paper presents an end-to-end solution for in-the-wild imaging using metalenses, employing a lightweight convolutional network and burst fusion algorithm to overcome challenges like chromatic aberration and low light efficiency. It shows improved performance over existing methods.",
        "tldr_zh": "本文提出了一种用于金属透镜的端到端解决方案，用于现实场景的成像，该方案采用轻量级卷积网络和突发融合算法，克服了色差和低光效等挑战。实验表明，该方法优于现有的图像修复方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HeadsUp! High-Fidelity Portrait Image Super-Resolution",
        "summary": "Portrait pictures, which typically feature both human subjects and natural\nbackgrounds, are one of the most prevalent forms of photography on social\nmedia. Existing image super-resolution (ISR) techniques generally focus either\non generic real-world images or strictly aligned facial images (i.e., face\nsuper-resolution). In practice, separate models are blended to handle portrait\nphotos: the face specialist model handles the face region, and the general\nmodel processes the rest. However, these blending approaches inevitably\nintroduce blending or boundary artifacts around the facial regions due to\ndifferent model training recipes, while human perception is particularly\nsensitive to facial fidelity. To overcome these limitations, we study the\nportrait image supersolution (PortraitISR) problem, and propose HeadsUp, a\nsingle-step diffusion model that is capable of seamlessly restoring and\nupscaling portrait images in an end-to-end manner. Specifically, we build our\nmodel on top of a single-step diffusion model and develop a face supervision\nmechanism to guide the model in focusing on the facial region. We then\nintegrate a reference-based mechanism to help with identity restoration,\nreducing face ambiguity in low-quality face restoration. Additionally, we have\nbuilt a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to\nsupport model training and benchmarking for portrait images. Extensive\nexperiments show that HeadsUp achieves state-of-the-art performance on the\nPortraitISR task while maintaining comparable or higher performance on both\ngeneral image and aligned face datasets.",
        "url": "http://arxiv.org/abs/2510.09924v1",
        "published_date": "2025-10-10T23:48:50+00:00",
        "updated_date": "2025-10-10T23:48:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Renjie Li",
            "Zihao Zhu",
            "Xiaoyu Wang",
            "Zhengzhong Tu"
        ],
        "tldr": "The paper introduces HeadsUp, a single-step diffusion model for portrait image super-resolution, addressing blending artifacts common in existing methods by using face supervision and reference-based mechanisms, and it also includes a new 4K portrait image dataset.",
        "tldr_zh": "该论文介绍了一种名为HeadsUp的单步扩散模型，用于人像图像超分辨率，通过使用面部监督和基于参考的机制解决了现有方法中常见的混合伪影，并包含一个新的4K人像图像数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Denoising Diffusion as a New Framework for Underwater Images",
        "summary": "Underwater images play a crucial role in ocean research and marine\nenvironmental monitoring since they provide quality information about the\necosystem. However, the complex and remote nature of the environment results in\npoor image quality with issues such as low visibility, blurry textures, color\ndistortion, and noise. In recent years, research in image enhancement has\nproven to be effective but also presents its own limitations, like poor\ngeneralization and heavy reliance on clean datasets. One of the challenges\nherein is the lack of diversity and the low quality of images included in these\ndatasets. Also, most existing datasets consist only of monocular images, a fact\nthat limits the representation of different lighting conditions and angles. In\nthis paper, we propose a new plan of action to overcome these limitations. On\none hand, we call for expanding the datasets using a denoising diffusion model\nto include a variety of image types such as stereo, wide-angled, macro, and\nclose-up images. On the other hand, we recommend enhancing the images using\nControlnet to evaluate and increase the quality of the corresponding datasets,\nand hence improve the study of the marine ecosystem.\n  Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet",
        "url": "http://arxiv.org/abs/2510.09934v1",
        "published_date": "2025-10-11T00:22:32+00:00",
        "updated_date": "2025-10-11T00:22:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Nilesh Jain",
            "Elie Alhajjar"
        ],
        "tldr": "This paper proposes using denoising diffusion models and ControlNet to expand and enhance underwater image datasets, aiming to improve marine ecosystem studies.",
        "tldr_zh": "本文提出使用去噪扩散模型和ControlNet来扩展和增强水下图像数据集，旨在改善海洋生态系统的研究。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Cross-Sensor Touch Generation",
        "summary": "Today's visuo-tactile sensors come in many shapes and sizes, making it\nchallenging to develop general-purpose tactile representations. This is because\nmost models are tied to a specific sensor design. To address this challenge, we\npropose two approaches to cross-sensor image generation. The first is an\nend-to-end method that leverages paired data (Touch2Touch). The second method\nbuilds an intermediate depth representation and does not require paired data\n(T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific\nmodels across multiple sensors via the cross-sensor touch generation process.\nTogether, these models offer flexible solutions for sensor translation,\ndepending on data availability and application needs. We demonstrate their\neffectiveness on downstream tasks such as in-hand pose estimation and behavior\ncloning, successfully transferring models trained on one sensor to another.\nProject page: https://samantabelen.github.io/cross_sensor_touch_generation.",
        "url": "http://arxiv.org/abs/2510.09817v1",
        "published_date": "2025-10-10T19:32:15+00:00",
        "updated_date": "2025-10-10T19:32:15+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Samanta Rodriguez",
            "Yiming Dou",
            "Miquel Oller",
            "Andrew Owens",
            "Nima Fazeli"
        ],
        "tldr": "The paper introduces two cross-sensor touch image generation methods (Touch2Touch and T2D2) to enable the transfer of sensor-specific models across different tactile sensors, demonstrated on in-hand pose estimation and behavior cloning.",
        "tldr_zh": "本文介绍了两种跨传感器触觉图像生成方法 (Touch2Touch 和 T2D2)，旨在实现传感器特定模型在不同触觉传感器之间的迁移，并在手部姿势估计和行为克隆方面进行了演示。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]