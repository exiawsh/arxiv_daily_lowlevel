[
    {
        "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
        "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.",
        "url": "http://arxiv.org/abs/2508.13154v1",
        "published_date": "2025-08-18T17:59:55+00:00",
        "updated_date": "2025-08-18T17:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaoxi Chen",
            "Tianqi Liu",
            "Long Zhuo",
            "Jiawei Ren",
            "Zeng Tao",
            "He Zhu",
            "Fangzhou Hong",
            "Liang Pan",
            "Ziwei Liu"
        ],
        "tldr": "4DNeX introduces a feed-forward framework for generating dynamic 3D scene representations from a single image by fine-tuning a pretrained video diffusion model, using a newly constructed large-scale 4D dataset and a unified 6D video representation.",
        "tldr_zh": "4DNeX 提出了一种前馈框架，通过微调预训练的视频扩散模型，从单张图像生成动态 3D 场景表示。该框架利用了一个新建的大规模 4D 数据集和一个统一的 6D 视频表示。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models",
        "summary": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion\nmodels for enhancing sample quality and prompt adherence. However, through an\nempirical analysis on Gaussian mixture modeling with a closed-form solution, we\nobserve a discrepancy between the suboptimal results produced by CFG and the\nground truth. The model's excessive reliance on these suboptimal predictions\noften leads to semantic incoherence and low-quality outputs. To address this\nissue, we first empirically demonstrate that the model's suboptimal predictions\ncan be effectively refined using sub-networks of the model itself. Building on\nthis insight, we propose S^2-Guidance, a novel method that leverages stochastic\nblock-dropping during the forward process to construct stochastic sub-networks,\neffectively guiding the model away from potential low-quality predictions and\ntoward high-quality outputs. Extensive qualitative and quantitative experiments\non text-to-image and text-to-video generation tasks demonstrate that\nS^2-Guidance delivers superior performance, consistently surpassing CFG and\nother advanced guidance strategies. Our code will be released.",
        "url": "http://arxiv.org/abs/2508.12880v1",
        "published_date": "2025-08-18T12:31:20+00:00",
        "updated_date": "2025-08-18T12:31:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chubin Chen",
            "Jiashu Zhu",
            "Xiaokun Feng",
            "Nisha Huang",
            "Meiqi Wu",
            "Fangyuan Mao",
            "Jiahong Wu",
            "Xiangxiang Chu",
            "Xiu Li"
        ],
        "tldr": "The paper introduces S^2-Guidance, a novel training-free method for enhancing diffusion models by using stochastic sub-networks to guide the model away from suboptimal predictions, leading to improved image and video generation.",
        "tldr_zh": "该论文介绍了S^2-Guidance，一种新的免训练方法，通过使用随机子网络引导模型远离次优预测来增强扩散模型，从而改进图像和视频生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics",
        "summary": "Continuous and reliable underwater monitoring is essential for assessing\nmarine biodiversity, detecting ecological changes and supporting autonomous\nexploration in aquatic environments. Underwater monitoring platforms rely on\nmainly visual data for marine biodiversity analysis, ecological assessment and\nautonomous exploration. However, underwater environments present significant\nchallenges due to light scattering, absorption and turbidity, which degrade\nimage clarity and distort colour information, which makes accurate observation\ndifficult. To address these challenges, we propose DEEP-SEA, a novel deep\nlearning-based underwater image restoration model to enhance both low- and\nhigh-frequency information while preserving spatial structures. The proposed\nDual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to\nadaptively refine feature representations in frequency domains and\nsimultaneously spatial information for better structural preservation. Our\ncomprehensive experiments on EUVP and LSUI datasets demonstrate the superiority\nover the state of the art in restoring fine-grained image detail and structural\nconsistency. By effectively mitigating underwater visual degradation, DEEP-SEA\nhas the potential to improve the reliability of underwater monitoring platforms\nfor more accurate ecological observation, species identification and autonomous\nnavigation.",
        "url": "http://arxiv.org/abs/2508.12824v1",
        "published_date": "2025-08-18T11:07:26+00:00",
        "updated_date": "2025-08-18T11:07:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuang Chen",
            "Ronald Thenius",
            "Farshad Arvin",
            "Amir Atapour-Abarghouei"
        ],
        "tldr": "The paper introduces DEEP-SEA, a deep learning-based underwater image restoration model that enhances both low- and high-frequency information while preserving spatial structures, demonstrating superior performance on EUVP and LSUI datasets.",
        "tldr_zh": "该论文介绍了DEEP-SEA，一种基于深度学习的水下图像恢复模型，它增强了低频和高频信息，同时保留了空间结构，并在EUVP和LSUI数据集上表现出卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring",
        "summary": "Single image defocus deblurring aims to recover an all-in-focus image from a\ndefocus counterpart, where accurately modeling spatially varying blur kernels\nremains a key challenge. Most existing methods rely on spatial features for\nkernel estimation, but their performance degrades in severely blurry regions\nwhere local high-frequency details are missing. To address this, we propose a\nFrequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates\nfrequency-domain representations to enhance structural identifiability in\nkernel modeling. Given the superior discriminative capability of the frequency\ndomain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction\n(DIKP) strategy that improves the accuracy of kernel estimation while\nmaintaining stability. Moreover, considering the limited number of predicted\ninverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance\nthe adaptability of the deconvolution process. Finally, we propose a\nDual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and\nprogressively improve deblurring quality from coarse to fine. Extensive\nexperiments demonstrate that our method outperforms existing approaches. Code\nwill be made publicly available.",
        "url": "http://arxiv.org/abs/2508.12736v1",
        "published_date": "2025-08-18T09:01:13+00:00",
        "updated_date": "2025-08-18T09:01:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying Zhang",
            "Xiongxin Tang",
            "Chongyi Li",
            "Qiao Chen",
            "Yuquan Wu"
        ],
        "tldr": "This paper presents a Frequency-Driven Inverse Kernel Prediction network (FDIKP) for single image defocus deblurring, leveraging frequency-domain representations and position adaptive convolution to improve kernel estimation and deblurring quality.",
        "tldr_zh": "本文提出了一种基于频率驱动的逆核预测网络（FDIKP），用于单图像散焦去模糊。该方法利用频域表示和位置自适应卷积来提高核估计和去模糊质量。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow",
        "summary": "Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic\ndiagnosis but requires gadolinium-based agents, which add cost and scan time,\nraise environmental concerns, and may pose risks to patients. In this work, we\npropose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for\nsynthesizing volumetric CE brain MRI from non-contrast inputs. First, a\npatch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).\nThen, this initial estimate is refined by a time-conditioned 3D rectified flow\nto incorporate realistic textures without compromising structural fidelity. We\ntrain this model on a multi-institutional collection of paired pre- and\npost-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360\ndiverse volumes, our best refined outputs achieve an axial FID of $12.46$ and\nKID of $0.007$ ($\\sim 68.7\\%$ lower FID than the posterior mean) while\nmaintaining low volumetric MSE of $0.057$ ($\\sim 27\\%$ higher than the\nposterior mean). Qualitative comparisons confirm that our method restores\nlesion margins and vascular details realistically, effectively navigating the\nperception-distortion trade-off for clinical deployment.",
        "url": "http://arxiv.org/abs/2508.12640v1",
        "published_date": "2025-08-18T05:55:57+00:00",
        "updated_date": "2025-08-18T05:55:57+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Bastian Brandstötter",
            "Erich Kobler"
        ],
        "tldr": "This paper introduces a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline to synthesize contrast-enhanced brain MRI from non-contrast inputs, achieving realistic textures and preserving structural fidelity, thus potentially reducing the need for gadolinium-based contrast agents.",
        "tldr_zh": "本文提出了一种两阶段后验均值校正流 (PMRF) 流程，用于从非对比输入合成对比增强的脑部 MRI，实现逼真的纹理并保留结构保真度，从而可能减少对钆基对比剂的需求。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset",
        "summary": "Nowadays, the development of a Presentation Attack Detection (PAD) system for\nID cards presents a challenge due to the lack of images available to train a\nrobust PAD system and the increase in diversity of possible attack instrument\nspecies. Today, most algorithms focus on generating attack samples and do not\ntake into account the limited number of bona fide images. This work is one of\nthe first to propose a method for mimicking bona fide images by generating\nsynthetic versions of them using Stable Diffusion, which may help improve the\ngeneralisation capabilities of the detector. Furthermore, the new images\ngenerated are evaluated in a system trained from scratch and in a commercial\nsolution. The PAD system yields an interesting result, as it identifies our\nimages as bona fide, which has a positive impact on detection performance and\ndata restrictions.",
        "url": "http://arxiv.org/abs/2508.13078v1",
        "published_date": "2025-08-18T16:48:57+00:00",
        "updated_date": "2025-08-18T16:48:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingwen Zeng",
            "Juan E. Tapia",
            "Izan Garcia",
            "Juan M. Espin",
            "Christoph Busch"
        ],
        "tldr": "This paper proposes using Stable Diffusion to generate synthetic ID card images to address the lack of bona fide training data for Presentation Attack Detection (PAD) systems, showing promising results in fooling both a custom-trained system and a commercial solution.",
        "tldr_zh": "该论文提出使用Stable Diffusion生成合成身份证图像，以解决表示攻击检测 (PAD) 系统缺乏真实训练数据的问题，并在欺骗定制训练系统和商业解决方案方面显示出有希望的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model",
        "summary": "Recent advances in interactive video generations have demonstrated diffusion\nmodel's potential as world models by capturing complex physical dynamics and\ninteractive behaviors. However, existing interactive world models depend on\nbidirectional attention and lengthy inference steps, severely limiting\nreal-time performance. Consequently, they are hard to simulate real-world\ndynamics, where outcomes must update instantaneously based on historical\ncontext and current actions. To address this, we present Matrix-Game 2.0, an\ninteractive world model generates long videos on-the-fly via few-step\nauto-regressive diffusion. Our framework consists of three key components: (1)\nA scalable data production pipeline for Unreal Engine and GTA5 environments to\neffectively produce massive amounts (about 1200 hours) of video data with\ndiverse interaction annotations; (2) An action injection module that enables\nframe-level mouse and keyboard inputs as interactive conditions; (3) A few-step\ndistillation based on the casual architecture for real-time and streaming video\ngeneration. Matrix Game 2.0 can generate high-quality minute-level videos\nacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source our\nmodel weights and codebase to advance research in interactive world modeling.",
        "url": "http://arxiv.org/abs/2508.13009v1",
        "published_date": "2025-08-18T15:28:53+00:00",
        "updated_date": "2025-08-18T15:28:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xianglong He",
            "Chunli Peng",
            "Zexiang Liu",
            "Boyang Wang",
            "Yifan Zhang",
            "Qi Cui",
            "Fei Kang",
            "Biao Jiang",
            "Mengyin An",
            "Yangyang Ren",
            "Baixin Xu",
            "Hao-Xiang Guo",
            "Kaixiong Gong",
            "Cyrus Wu",
            "Wei Li",
            "Xuchen Song",
            "Yang Liu",
            "Eric Li",
            "Yahui Zhou"
        ],
        "tldr": "Matrix-Game 2.0 introduces a real-time, streaming interactive world model using few-step auto-regressive diffusion, enabling high-quality video generation at 25 FPS with open-sourced code and weights.",
        "tldr_zh": "Matrix-Game 2.0 提出了一个实时、流式的交互式世界模型，该模型使用少步自回归扩散，能够以 25 FPS 的速度生成高质量视频，并开源了代码和权重。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Next Visual Granularity Generation",
        "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.",
        "url": "http://arxiv.org/abs/2508.12811v1",
        "published_date": "2025-08-18T10:47:37+00:00",
        "updated_date": "2025-08-18T10:47:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yikai Wang",
            "Zhouxia Wang",
            "Zhonghua Wu",
            "Qingyi Tao",
            "Kang Liao",
            "Chen Change Loy"
        ],
        "tldr": "The paper introduces a Next Visual Granularity (NVG) generation framework for image generation that iteratively refines images from global layout to fine details by generating a sequence of visual granularities. The NVG approach achieves better FID scores than VAR series models on ImageNet.",
        "tldr_zh": "该论文提出了一种名为“下一视觉粒度”（NVG）的图像生成框架，通过生成一系列视觉粒度，从全局布局到精细细节迭代地改进图像。在ImageNet上，NVG方法实现了比VAR系列模型更好的FID分数。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Stable Diffusion-Based Approach for Human De-Occlusion",
        "summary": "Humans can infer the missing parts of an occluded object by leveraging prior\nknowledge and visible cues. However, enabling deep learning models to\naccurately predict such occluded regions remains a challenging task.\nDe-occlusion addresses this problem by reconstructing both the mask and RGB\nappearance. In this work, we focus on human de-occlusion, specifically\ntargeting the recovery of occluded body structures and appearances. Our\napproach decomposes the task into two stages: mask completion and RGB\ncompletion. The first stage leverages a diffusion-based human body prior to\nprovide a comprehensive representation of body structure, combined with\noccluded joint heatmaps that offer explicit spatial cues about missing regions.\nThe reconstructed amodal mask then serves as a conditioning input for the\nsecond stage, guiding the model on which areas require RGB reconstruction. To\nfurther enhance RGB generation, we incorporate human-specific textual features\nderived using a visual question answering (VQA) model and encoded via a CLIP\nencoder. RGB completion is performed using Stable Diffusion, with decoder\nfine-tuning applied to mitigate pixel-level degradation in visible regions -- a\nknown limitation of prior diffusion-based de-occlusion methods caused by latent\nspace transformations. Our method effectively reconstructs human appearances\neven under severe occlusions and consistently outperforms existing methods in\nboth mask and RGB completion. Moreover, the de-occluded images generated by our\napproach can improve the performance of downstream human-centric tasks, such as\n2D pose estimation and 3D human reconstruction. The code will be made publicly\navailable.",
        "url": "http://arxiv.org/abs/2508.12663v1",
        "published_date": "2025-08-18T06:53:29+00:00",
        "updated_date": "2025-08-18T06:53:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seung Young Noh",
            "Ju Yong Chang"
        ],
        "tldr": "This paper presents a Stable Diffusion-based approach for human de-occlusion, using a two-stage process involving mask and RGB completion, outperforming existing methods and improving downstream tasks.",
        "tldr_zh": "本文提出了一种基于Stable Diffusion的人体去遮挡方法，该方法采用两阶段流程，包括掩码和RGB补全，优于现有方法并改善了下游任务。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation",
        "summary": "While supervised stereo matching and monocular depth estimation have advanced\nsignificantly with learning-based algorithms, self-supervised methods using\nstereo images as supervision signals have received relatively less focus and\nrequire further investigation. A primary challenge arises from ambiguity\nintroduced during photometric reconstruction, particularly due to missing\ncorresponding pixels in ill-posed regions of the target view, such as\nocclusions and out-of-frame areas. To address this and establish explicit\nphotometric correspondences, we propose DMS, a model-agnostic approach that\nutilizes geometric priors from diffusion models to synthesize novel views along\nthe epipolar direction, guided by directional prompts. Specifically, we\nfinetune a Stable Diffusion model to simulate perspectives at key positions:\nleft-left view shifted from the left camera, right-right view shifted from the\nright camera, along with an additional novel view between the left and right\ncameras. These synthesized views supplement occluded pixels, enabling explicit\nphotometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''\nmethod that seamlessly enhances self-supervised stereo matching and monocular\ndepth estimation, and relies solely on unlabeled stereo image pairs for both\ntraining and synthesizing. Extensive experiments demonstrate the effectiveness\nof our approach, with up to 35% outlier reduction and state-of-the-art\nperformance across multiple benchmark datasets.",
        "url": "http://arxiv.org/abs/2508.13091v1",
        "published_date": "2025-08-18T17:05:15+00:00",
        "updated_date": "2025-08-18T17:05:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihua Liu",
            "Yizhou Li",
            "Songyan Zhang",
            "Masatoshi Okutomi"
        ],
        "tldr": "The paper proposes a diffusion-based method (DMS) to generate novel views from stereo images to improve self-supervised depth estimation by addressing occlusion ambiguities. It leverages Stable Diffusion for view synthesis and achieves state-of-the-art performance.",
        "tldr_zh": "该论文提出了一种基于扩散的方法（DMS），通过生成来自立体图像的新视角来改善自监督深度估计，从而解决遮挡歧义。它利用 Stable Diffusion 进行视图合成，并实现了最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]