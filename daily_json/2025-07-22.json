[
    {
        "title": "Diffusion Beats Autoregressive in Data-Constrained Settings",
        "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.",
        "url": "http://arxiv.org/abs/2507.15857v4",
        "published_date": "2025-07-21T17:59:57+00:00",
        "updated_date": "2025-07-31T13:10:29+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Mihir Prabhudesai",
            "Mengning Wu",
            "Amir Zadeh",
            "Katerina Fragkiadaki",
            "Deepak Pathak"
        ]
    },
    {
        "title": "Latent Denoising Makes Good Visual Tokenizers",
        "summary": "Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.",
        "url": "http://arxiv.org/abs/2507.15856v1",
        "published_date": "2025-07-21T17:59:56+00:00",
        "updated_date": "2025-07-21T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiawei Yang",
            "Tianhong Li",
            "Lijie Fan",
            "Yonglong Tian",
            "Yue Wang"
        ]
    },
    {
        "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction",
        "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.",
        "url": "http://arxiv.org/abs/2507.15852v2",
        "published_date": "2025-07-21T17:59:02+00:00",
        "updated_date": "2025-07-22T10:51:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhixiong Zhang",
            "Shuangrui Ding",
            "Xiaoyi Dong",
            "Songxin He",
            "Jianfan Lin",
            "Junsong Tang",
            "Yuhang Zang",
            "Yuhang Cao",
            "Dahua Lin",
            "Jiaqi Wang"
        ]
    },
    {
        "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding",
        "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.",
        "url": "http://arxiv.org/abs/2507.15846v3",
        "published_date": "2025-07-21T17:53:42+00:00",
        "updated_date": "2025-07-28T16:54:13+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Fei Tang",
            "Zhangxuan Gu",
            "Zhengxi Lu",
            "Xuyang Liu",
            "Shuheng Shen",
            "Changhua Meng",
            "Wen Wang",
            "Wenqi Zhang",
            "Yongliang Shen",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ]
    },
    {
        "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers",
        "summary": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/",
        "url": "http://arxiv.org/abs/2507.15833v1",
        "published_date": "2025-07-21T17:44:10+00:00",
        "updated_date": "2025-07-21T17:44:10+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ian Chuang",
            "Andrew Lee",
            "Dechen Gao",
            "Jinyu Zou",
            "Iman Soltani"
        ]
    },
    {
        "title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models",
        "summary": "Recent progress in text-to-video (T2V) generation has enabled the synthesis\nof visually compelling and temporally coherent videos from natural language.\nHowever, these models often fall short in basic physical commonsense, producing\noutputs that violate intuitive expectations around causality, object behavior,\nand tool use. Addressing this gap, we present PhysVidBench, a benchmark\ndesigned to evaluate the physical reasoning capabilities of T2V systems. The\nbenchmark includes 383 carefully curated prompts, emphasizing tool use,\nmaterial properties, and procedural interactions, and domains where physical\nplausibility is crucial. For each prompt, we generate videos using diverse\nstate-of-the-art models and adopt a three-stage evaluation pipeline: (1)\nformulate grounded physics questions from the prompt, (2) caption the generated\nvideo with a vision-language model, and (3) task a language model to answer\nseveral physics-involved questions using only the caption. This indirect\nstrategy circumvents common hallucination issues in direct video-based\nevaluation. By highlighting affordances and tool-mediated actions, areas\noverlooked in current T2V evaluations, PhysVidBench provides a structured,\ninterpretable framework for assessing physical commonsense in generative video\nmodels.",
        "url": "http://arxiv.org/abs/2507.15824v1",
        "published_date": "2025-07-21T17:30:46+00:00",
        "updated_date": "2025-07-21T17:30:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Enes Sanli",
            "Baris Sarper Tezcan",
            "Aykut Erdem",
            "Erkut Erdem"
        ]
    },
    {
        "title": "An empirical study for the early detection of Mpox from skin lesion images using pretrained CNN models leveraging XAI technique",
        "summary": "Context: Mpox is a zoonotic disease caused by the Mpox virus, which shares\nsimilarities with other skin conditions, making accurate early diagnosis\nchallenging. Artificial intelligence (AI), especially Deep Learning (DL), has a\nstrong tool for medical image analysis; however, pre-trained models like CNNs\nand XAI techniques for mpox detection is underexplored. Objective: This study\naims to evaluate the effectiveness of pre-trained CNN models (VGG16, VGG19,\nInceptionV3, MobileNetV2) for the early detection of monkeypox using binary and\nmulti-class datasets. It also seeks to enhance model interpretability using\nGrad-CAM an XAI technique. Method: Two datasets, MSLD and MSLD v2.0, were used\nfor training and validation. Transfer learning techniques were applied to\nfine-tune pre-trained CNN models by freezing initial layers and adding custom\nlayers for adapting the final features for mpox detection task and avoid\noverfitting. Models performance were evaluated using metrics such as accuracy,\nprecision, recall, F1-score and ROC. Grad-CAM was utilized for visualizing\ncritical features. Results: InceptionV3 demonstrated the best performance on\nthe binary dataset with an accuracy of 95%, while MobileNetV2 outperformed on\nthe multi-class dataset with an accuracy of 93%. Grad-CAM successfully\nhighlighted key image regions. Despite high accuracy, some models showed\noverfitting tendencies, as videnced by discrepancies between training and\nvalidation losses. Conclusion: This study underscores the potential of\npre-trained CNN models in monkeypox detection and the value of XAI techniques.\nFuture work should address dataset limitations, incorporate multimodal data,\nand explore additional interpretability techniques to improve diagnostic\nreliability and model transparency",
        "url": "http://arxiv.org/abs/2507.15915v1",
        "published_date": "2025-07-21T17:30:08+00:00",
        "updated_date": "2025-07-21T17:30:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohammad Asifur Rahim",
            "Muhammad Nazmul Arefin",
            "Md. Mizanur Rahman",
            "Md Ali Hossain",
            "Ahmed Moustafa"
        ]
    },
    {
        "title": "Weak Links in LinkedIn: Enhancing Fake Profile Detection in the Age of LLMs",
        "summary": "Large Language Models (LLMs) have made it easier to create realistic fake\nprofiles on platforms like LinkedIn. This poses a significant risk for\ntext-based fake profile detectors. In this study, we evaluate the robustness of\nexisting detectors against LLM-generated profiles. While highly effective in\ndetecting manually created fake profiles (False Accept Rate: 6-7%), the\nexisting detectors fail to identify GPT-generated profiles (False Accept Rate:\n42-52%). We propose GPT-assisted adversarial training as a countermeasure,\nrestoring the False Accept Rate to between 1-7% without impacting the False\nReject Rates (0.5-2%). Ablation studies revealed that detectors trained on\ncombined numerical and textual embeddings exhibit the highest robustness,\nfollowed by those using numerical-only embeddings, and lastly those using\ntextual-only embeddings. Complementary analysis on the ability of prompt-based\nGPT-4Turbo and human evaluators affirms the need for robust automated detectors\nsuch as the one proposed in this study.",
        "url": "http://arxiv.org/abs/2507.16860v1",
        "published_date": "2025-07-21T17:23:52+00:00",
        "updated_date": "2025-07-21T17:23:52+00:00",
        "categories": [
            "cs.SI",
            "cs.CV",
            "cs.CY"
        ],
        "authors": [
            "Apoorva Gulati",
            "Rajesh Kumar",
            "Vinti Agarwal",
            "Aditya Sharma"
        ]
    },
    {
        "title": "Diffusion models for multivariate subsurface generation and efficient probabilistic inversion",
        "summary": "Diffusion models offer stable training and state-of-the-art performance for\ndeep generative modeling tasks. Here, we consider their use in the context of\nmultivariate subsurface modeling and probabilistic inversion. We first\ndemonstrate that diffusion models enhance multivariate modeling capabilities\ncompared to variational autoencoders and generative adversarial networks. In\ndiffusion modeling, the generative process involves a comparatively large\nnumber of time steps with update rules that can be modified to account for\nconditioning data. We propose different corrections to the popular Diffusion\nPosterior Sampling approach by Chung et al. (2023). In particular, we introduce\na likelihood approximation accounting for the noise-contamination that is\ninherent in diffusion modeling. We assess performance in a multivariate\ngeological scenario involving facies and correlated acoustic impedance.\nConditional modeling is demonstrated using both local hard data (well logs) and\nnonlinear geophysics (fullstack seismic data). Our tests show significantly\nimproved statistical robustness, enhanced sampling of the posterior probability\ndensity function and reduced computational costs, compared to the original\napproach. The method can be used with both hard and indirect conditioning data,\nindividually or simultaneously. As the inversion is included within the\ndiffusion process, it is faster than other methods requiring an outer-loop\naround the generative model, such as Markov chain Monte Carlo.",
        "url": "http://arxiv.org/abs/2507.15809v1",
        "published_date": "2025-07-21T17:10:16+00:00",
        "updated_date": "2025-07-21T17:10:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "physics.geo-ph",
            "stat.AP"
        ],
        "authors": [
            "Roberto Miele",
            "Niklas Linde"
        ]
    },
    {
        "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context",
        "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .",
        "url": "http://arxiv.org/abs/2507.15807v1",
        "published_date": "2025-07-21T17:08:18+00:00",
        "updated_date": "2025-07-21T17:08:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shuo Chen",
            "Jianzhe Liu",
            "Zhen Han",
            "Yan Xia",
            "Daniel Cremers",
            "Philip Torr",
            "Volker Tresp",
            "Jindong Gu"
        ]
    },
    {
        "title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction",
        "summary": "Pixel-level vision tasks, such as semantic segmentation, require extensive\nand high-quality annotated data, which is costly to obtain. Semi-supervised\nsemantic segmentation (SSSS) has emerged as a solution to alleviate the\nlabeling burden by leveraging both labeled and unlabeled data through\nself-training techniques. Meanwhile, the advent of foundational segmentation\nmodels pre-trained on massive data, has shown the potential to generalize\nacross domains effectively. This work explores whether a foundational\nsegmentation model can address label scarcity in the pixel-level vision task as\nan annotator for unlabeled images. Specifically, we investigate the efficacy of\nusing SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual\ninput, to generate predictive masks for unlabeled data. To address the\nshortcomings of using SEEM-generated masks as supervision, we propose\nConformalSAM, a novel SSSS framework which first calibrates the foundation\nmodel using the target domain's labeled data and then filters out unreliable\npixel labels of unlabeled data so that only high-confidence labels are used as\nsupervision. By leveraging conformal prediction (CP) to adapt foundation models\nto target data through uncertainty calibration, ConformalSAM exploits the\nstrong capability of the foundational segmentation model reliably which\nbenefits the early-stage learning, while a subsequent self-reliance training\nstrategy mitigates overfitting to SEEM-generated masks in the later training\nstage. Our experiment demonstrates that, on three standard benchmarks of SSSS,\nConformalSAM achieves superior performance compared to recent SSSS methods and\nhelps boost the performance of those methods as a plug-in.",
        "url": "http://arxiv.org/abs/2507.15803v1",
        "published_date": "2025-07-21T17:02:57+00:00",
        "updated_date": "2025-07-21T17:02:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Danhui Chen",
            "Ziquan Liu",
            "Chuxi Yang",
            "Dan Wang",
            "Yan Yan",
            "Yi Xu",
            "Xiangyang Ji"
        ]
    },
    {
        "title": "Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models",
        "summary": "The paper investigates the performance of state-of-the-art low-parameter deep\nneural networks for computer vision, focusing on bottleneck architectures and\ntheir behavior using superlinear activation functions. We address interference\nin feature maps, a phenomenon associated with superposition, where neurons\nsimultaneously encode multiple characteristics. Our research suggests that\nlimiting interference can enhance scaling and accuracy in very low-scaled\nnetworks (under 1.5M parameters). We identify key design elements that reduce\ninterference by examining various bottleneck architectures, leading to a more\nefficient neural network. Consequently, we propose a proof-of-concept\narchitecture named NoDepth Bottleneck built on mechanistic insights from our\nexperiments, demonstrating robust scaling accuracy on the ImageNet dataset.\nThese findings contribute to more efficient and scalable neural networks for\nthe low-parameter range and advance the understanding of bottlenecks in\ncomputer vision. https://caiac.pubpub.org/pub/3dh6rsel",
        "url": "http://arxiv.org/abs/2507.15798v1",
        "published_date": "2025-07-21T16:57:25+00:00",
        "updated_date": "2025-07-21T16:57:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lilian Hollard",
            "Lucas Mohimont",
            "Nathalie Gaveau",
            "Luiz-Angelo Steffenel"
        ]
    },
    {
        "title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation",
        "summary": "Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is\nincreasingly attracting interest in medical imaging due to its effectiveness\nand computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)\nis a notable approach based on the assumption that the adaptation inherently\noccurs in a low-dimensional subspace. While it has shown good performance, its\nimplementation requires a fixed and unalterable rank, which might be\nchallenging to select given the unique complexities and requirements of each\nmedical imaging downstream task. Inspired by advancements in natural image\nprocessing, we introduce a novel approach for medical image segmentation that\ndynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank\nrepresentation of the trainable weight matrices as a singular value\ndecomposition, we introduce an l_1 sparsity regularizer to the loss function,\nand tackle it with a proximal optimizer. The regularizer could be viewed as a\npenalty on the decomposition rank. Hence, its minimization enables to find\ntask-adapted ranks automatically. Our method is evaluated in a realistic\nfew-shot fine-tuning setting, where we compare it first to the standard LoRA\nand then to several other PEFT methods across two distinguishable tasks: base\norgans and novel organs. Our extensive experiments demonstrate the significant\nperformance improvements driven by our method, highlighting its efficiency and\nrobustness against suboptimal rank initialization. Our code is publicly\navailable: https://github.com/ghassenbaklouti/ARENA",
        "url": "http://arxiv.org/abs/2507.15793v1",
        "published_date": "2025-07-21T16:51:53+00:00",
        "updated_date": "2025-07-21T16:51:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ghassen Baklouti",
            "Julio Silva-Rodríguez",
            "Jose Dolz",
            "Houda Bahig",
            "Ismail Ben Ayed"
        ]
    },
    {
        "title": "Label tree semantic losses for rich multi-class medical image segmentation",
        "summary": "Rich and accurate medical image segmentation is poised to underpin the next\ngeneration of AI-defined clinical practice by delineating critical anatomy for\npre-operative planning, guiding real-time intra-operative navigation, and\nsupporting precise post-operative assessment. However, commonly used learning\nmethods for medical and surgical imaging segmentation tasks penalise all errors\nequivalently and thus fail to exploit any inter-class semantics in the labels\nspace. This becomes particularly problematic as the cardinality and richness of\nlabels increases to include subtly different classes. In this work, we propose\ntwo tree-based semantic loss functions which take advantage of a hierarchical\norganisation of the labels. We further incorporate our losses in a recently\nproposed approach for training with sparse, background-free annotations to\nextend the applicability of our proposed losses. Extensive experiments are\nreported on two medical and surgical image segmentation tasks, namely head MRI\nfor whole brain parcellation (WBP) with full supervision and neurosurgical\nhyperspectral imaging (HSI) for scene understanding with sparse annotations.\nResults demonstrate that our proposed method reaches state-of-the-art\nperformance in both cases.",
        "url": "http://arxiv.org/abs/2507.15777v1",
        "published_date": "2025-07-21T16:32:48+00:00",
        "updated_date": "2025-07-21T16:32:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junwen Wang",
            "Oscar MacCormac",
            "William Rochford",
            "Aaron Kujawa",
            "Jonathan Shapey",
            "Tom Vercauteren"
        ]
    },
    {
        "title": "Local Dense Logit Relations for Enhanced Knowledge Distillation",
        "summary": "State-of-the-art logit distillation methods exhibit versatility, simplicity,\nand efficiency. Despite the advances, existing studies have yet to delve\nthoroughly into fine-grained relationships within logit knowledge. In this\npaper, we propose Local Dense Relational Logit Distillation (LDRLD), a novel\nmethod that captures inter-class relationships through recursively decoupling\nand recombining logit information, thereby providing more detailed and clearer\ninsights for student learning. To further optimize the performance, we\nintroduce an Adaptive Decay Weight (ADW) strategy, which can dynamically adjust\nthe weights for critical category pairs using Inverse Rank Weighting (IRW) and\nExponential Rank Decay (ERD). Specifically, IRW assigns weights inversely\nproportional to the rank differences between pairs, while ERD adaptively\ncontrols weight decay based on total ranking scores of category pairs.\nFurthermore, after the recursive decoupling, we distill the remaining\nnon-target knowledge to ensure knowledge completeness and enhance performance.\nUltimately, our method improves the student's performance by transferring\nfine-grained knowledge and emphasizing the most critical relationships.\nExtensive experiments on datasets such as CIFAR-100, ImageNet-1K, and\nTiny-ImageNet demonstrate that our method compares favorably with\nstate-of-the-art logit-based distillation approaches. The code will be made\npublicly available.",
        "url": "http://arxiv.org/abs/2507.15911v1",
        "published_date": "2025-07-21T16:25:38+00:00",
        "updated_date": "2025-07-21T16:25:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liuchi Xu",
            "Kang Liu",
            "Jinshuai Liu",
            "Lu Wang",
            "Lisheng Xu",
            "Jun Cheng"
        ]
    },
    {
        "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization",
        "summary": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER.",
        "url": "http://arxiv.org/abs/2507.15765v2",
        "published_date": "2025-07-21T16:21:47+00:00",
        "updated_date": "2025-07-26T14:16:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Feng-Qi Cui",
            "Anyang Tong",
            "Jinyang Huang",
            "Jie Zhang",
            "Dan Guo",
            "Zhi Liu",
            "Meng Wang"
        ]
    },
    {
        "title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS",
        "summary": "Modern camera pipelines apply extensive on-device processing, such as\nexposure adjustment, white balance, and color correction, which, while\nbeneficial individually, often introduce photometric inconsistencies across\nviews. These appearance variations violate multi-view consistency and degrade\nthe quality of novel view synthesis. Joint optimization of scene\nrepresentations and per-image appearance embeddings has been proposed to\naddress this issue, but at the cost of increased computational complexity and\nslower training. In this work, we propose a transformer-based method that\npredicts spatially adaptive bilateral grids to correct photometric variations\nin a multi-view consistent manner, enabling robust cross-scene generalization\nwithout the need for scene-specific retraining. By incorporating the learned\ngrids into the 3D Gaussian Splatting pipeline, we improve reconstruction\nquality while maintaining high training efficiency. Extensive experiments show\nthat our approach outperforms or matches existing scene-specific optimization\nmethods in reconstruction fidelity and convergence speed.",
        "url": "http://arxiv.org/abs/2507.15748v1",
        "published_date": "2025-07-21T16:03:58+00:00",
        "updated_date": "2025-07-21T16:03:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jisu Shin",
            "Richard Shaw",
            "Seunghyun Shin",
            "Anton Pelykh",
            "Zhensong Zhang",
            "Hae-Gon Jeon",
            "Eduardo Perez-Pellitero"
        ]
    },
    {
        "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation",
        "summary": "Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ .",
        "url": "http://arxiv.org/abs/2507.15728v1",
        "published_date": "2025-07-21T15:37:33+00:00",
        "updated_date": "2025-07-21T15:37:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenqi Ouyang",
            "Zeqi Xiao",
            "Danni Yang",
            "Yifan Zhou",
            "Shuai Yang",
            "Lei Yang",
            "Jianlou Si",
            "Xingang Pan"
        ]
    },
    {
        "title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers",
        "summary": "Enabling image generation models to be spatially controlled is an important\narea of research, empowering users to better generate images according to their\nown fine-grained specifications via e.g. edge maps, poses. Although this task\nhas seen impressive improvements in recent times, a focus on rapidly producing\nstronger models has come at the cost of detailed and fair scientific\ncomparison. Differing training data, model architectures and generation\nparadigms make it difficult to disentangle the factors contributing to\nperformance. Meanwhile, the motivations and nuances of certain approaches\nbecome lost in the literature. In this work, we aim to provide clear takeaways\nacross generation paradigms for practitioners wishing to develop\ntransformer-based systems for spatially-controlled generation, clarifying the\nliterature and addressing knowledge gaps. We perform controlled experiments on\nImageNet across diffusion-based/flow-based and autoregressive (AR) models.\nFirst, we establish control token prefilling as a simple, general and\nperformant baseline approach for transformers. We then investigate previously\nunderexplored sampling time enhancements, showing that extending\nclassifier-free guidance to control, as well as softmax truncation, have a\nstrong impact on control-generation consistency. Finally, we re-clarify the\nmotivation of adapter-based approaches, demonstrating that they mitigate\n\"forgetting\" and maintain generation quality when trained on limited downstream\ndata, but underperform full training in terms of generation-control\nconsistency. Code will be released upon publication.",
        "url": "http://arxiv.org/abs/2507.15724v1",
        "published_date": "2025-07-21T15:33:49+00:00",
        "updated_date": "2025-07-21T15:33:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guoxuan Xia",
            "Harleen Hanspal",
            "Petru-Daniel Tudosiu",
            "Shifeng Zhang",
            "Sarah Parisot"
        ]
    },
    {
        "title": "Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation",
        "summary": "Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git.",
        "url": "http://arxiv.org/abs/2507.15709v1",
        "published_date": "2025-07-21T15:17:01+00:00",
        "updated_date": "2025-07-21T15:17:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Sun",
            "Weixia Zhang",
            "Linhan Cao",
            "Jun Jia",
            "Xiangyang Zhu",
            "Dandan Zhu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ]
    },
    {
        "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting",
        "summary": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in\nreconstructing high-quality novel views, as it often overfits to the\nwidely-varying high-frequency (HF) details of the sparse training views. While\nfrequency regularization can be a promising approach, its typical reliance on\nFourier transforms causes difficult parameter tuning and biases towards\ndetrimental HF learning. We propose DWTGS, a framework that rethinks frequency\nregularization by leveraging wavelet-space losses that provide additional\nspatial supervision. Specifically, we supervise only the low-frequency (LF) LL\nsubbands at multiple DWT levels, while enforcing sparsity on the HF HH subband\nin a self-supervised manner. Experiments across benchmarks show that DWTGS\nconsistently outperforms Fourier-based counterparts, as this LF-centric\nstrategy improves generalization and reduces HF hallucinations.",
        "url": "http://arxiv.org/abs/2507.15690v1",
        "published_date": "2025-07-21T14:56:46+00:00",
        "updated_date": "2025-07-21T14:56:46+00:00",
        "categories": [
            "cs.CV",
            "eess.IV",
            "eess.SP"
        ],
        "authors": [
            "Hung Nguyen",
            "Runfa Li",
            "An Le",
            "Truong Nguyen"
        ]
    },
    {
        "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression",
        "summary": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.",
        "url": "http://arxiv.org/abs/2507.15686v1",
        "published_date": "2025-07-21T14:48:54+00:00",
        "updated_date": "2025-07-21T14:48:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wenjie Huang",
            "Qi Yang",
            "Shuting Xia",
            "He Huang",
            "Zhu Li",
            "Yiling Xu"
        ]
    },
    {
        "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing",
        "summary": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera\npose from query images, is fundamental to remote sensing and UAV applications.\nExisting methods face inherent trade-offs: image-based retrieval and pose\nregression approaches lack precision, while structure-based methods that\nregister queries to Structure-from-Motion (SfM) models suffer from\ncomputational complexity and limited scalability. These challenges are\nparticularly pronounced in remote sensing scenarios due to large-scale scenes,\nhigh altitude variations, and domain gaps of existing visual priors. To\novercome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel\nscene representation that compactly encodes both 3D geometry and appearance. We\nintroduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework\nthat follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting\nthe rich semantic information and geometric constraints inherent in Gaussian\nprimitives. To handle large-scale remote sensing scenarios, we incorporate\npartitioned Gaussian training, GPU-accelerated parallel matching, and dynamic\nmemory management strategies. Our approach consists of two stages: (1) a sparse\nstage featuring a Gaussian-specific consistent render-aware sampling strategy\nand landmark-guided detector for robust and accurate initial pose estimation,\nand (2) a dense stage that iteratively refines poses through coarse-to-fine\ndense rasterization matching while incorporating reliability verification.\nThrough comprehensive evaluation on simulation data, public datasets, and real\nflight experiments, we demonstrate that our method delivers competitive\nlocalization accuracy, recall rate, and computational efficiency while\neffectively filtering unreliable pose estimates. The results confirm the\neffectiveness of our approach for practical remote sensing applications.",
        "url": "http://arxiv.org/abs/2507.15683v1",
        "published_date": "2025-07-21T14:47:56+00:00",
        "updated_date": "2025-07-21T14:47:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boni Hu",
            "Zhenyu Xia",
            "Lin Chen",
            "Pengcheng Han",
            "Shuhui Bu"
        ]
    },
    {
        "title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment",
        "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.",
        "url": "http://arxiv.org/abs/2507.15680v3",
        "published_date": "2025-07-21T14:44:46+00:00",
        "updated_date": "2025-07-23T08:20:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongkang Hou",
            "Jiarun Song"
        ]
    },
    {
        "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark",
        "summary": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.",
        "url": "http://arxiv.org/abs/2507.15655v1",
        "published_date": "2025-07-21T14:16:44+00:00",
        "updated_date": "2025-07-21T14:16:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aniket Pal",
            "Ajoy Mondal",
            "Minesh Mathew",
            "C. V. Jawahar"
        ]
    },
    {
        "title": "Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have made significant strides by\ncombining visual recognition and language understanding to generate content\nthat is both coherent and contextually accurate. However, MLLMs continue to\nstruggle with object hallucinations, where models produce seemingly plausible\nbut factually incorrect outputs, including objects that do not exist in the\nimage. Recent work has revealed that the prior knowledge in MLLMs significantly\nsuppresses visual information in deep layers, causing hallucinatory outputs.\nHowever, how these priors suppress visual information at the intermediate layer\nstage in MLLMs remains unclear. We observe that visual factual knowledge and\nthe differences between intermediate-layer prior/original probability\ndistributions show similar evolutionary trends in intermediate layers.\nMotivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a\nsimple, training-free method that dynamically selects intermediate layers with\nthe most significant visual factual information. By contrasting the output\ndistributions of the selected layer derived from the original input and\npure-text input, EVA extracts visual factual knowledge and proportionally\nincorporates it into the final layer to correct the output logits. Importantly,\nEVA is model-agnostic, seamlessly integrates with various classic decoding\nstrategies, and is applicable across different MLLMs. We validate EVA on\nwidely-used benchmarks, and the results show that it significantly reduces\nhallucination rates compared to baseline methods, underscoring its\neffectiveness in mitigating hallucinations.",
        "url": "http://arxiv.org/abs/2507.15652v1",
        "published_date": "2025-07-21T14:15:34+00:00",
        "updated_date": "2025-07-21T14:15:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Zhou",
            "Zihan Zhang",
            "Hao Chen"
        ]
    },
    {
        "title": "SIA: Enhancing Safety via Intent Awareness for Vision-Language Models",
        "summary": "As vision-language models (VLMs) are increasingly deployed in real-world\napplications, new safety risks arise from the subtle interplay between images\nand text. In particular, seemingly innocuous inputs can combine to reveal\nharmful intent, leading to unsafe model responses. Despite increasing attention\nto multimodal safety, previous approaches based on post hoc filtering or static\nrefusal prompts struggle to detect such latent risks, especially when\nharmfulness emerges only from the combination of inputs. We propose SIA (Safety\nvia Intent Awareness), a training-free prompt engineering framework that\nproactively detects and mitigates harmful intent in multimodal inputs. SIA\nemploys a three-stage reasoning process: (1) visual abstraction via captioning,\n(2) intent inference through few-shot chain-of-thought prompting, and (3)\nintent-conditioned response refinement. Rather than relying on predefined rules\nor classifiers, SIA dynamically adapts to the implicit intent inferred from the\nimage-text pair. Through extensive experiments on safety-critical benchmarks\nincluding SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves\nsubstantial safety improvements, outperforming prior methods. Although SIA\nshows a minor reduction in general reasoning accuracy on MMStar, the\ncorresponding safety gains highlight the value of intent-aware reasoning in\naligning VLMs with human-centric values.",
        "url": "http://arxiv.org/abs/2507.16856v1",
        "published_date": "2025-07-21T13:59:50+00:00",
        "updated_date": "2025-07-21T13:59:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Youngjin Na",
            "Sangheon Jeong",
            "Youngwan Lee"
        ]
    },
    {
        "title": "Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis",
        "summary": "Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems.",
        "url": "http://arxiv.org/abs/2507.15636v1",
        "published_date": "2025-07-21T13:58:24+00:00",
        "updated_date": "2025-07-21T13:58:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lisan Al Amin",
            "Md. Ismail Hossain",
            "Thanh Thi Nguyen",
            "Tasnim Jahan",
            "Mahbubul Islam",
            "Faisal Quader"
        ]
    },
    {
        "title": "Experimenting active and sequential learning in a medieval music manuscript",
        "summary": "Optical Music Recognition (OMR) is a cornerstone of music digitization\ninitiatives in cultural heritage, yet it remains limited by the scarcity of\nannotated data and the complexity of historical manuscripts. In this paper, we\npresent a preliminary study of Active Learning (AL) and Sequential Learning\n(SL) tailored for object detection and layout recognition in an old medieval\nmusic manuscript. Leveraging YOLOv8, our system selects samples with the\nhighest uncertainty (lowest prediction confidence) for iterative labeling and\nretraining. Our approach starts with a single annotated image and successfully\nboosts performance while minimizing manual labeling. Experimental results\nindicate that comparable accuracy to fully supervised training can be achieved\nwith significantly fewer labeled examples. We test the methodology as a\npreliminary investigation on a novel dataset offered to the community by the\nAnonymous project, which studies laude, a poetical-musical genre spread across\nItaly during the 12th-16th Century. We show that in the manuscript at-hand,\nuncertainty-based AL is not effective and advocates for more usable methods in\ndata-scarcity scenarios.",
        "url": "http://arxiv.org/abs/2507.15633v1",
        "published_date": "2025-07-21T13:55:54+00:00",
        "updated_date": "2025-07-21T13:55:54+00:00",
        "categories": [
            "cs.CV",
            "I.2.10; I.4.8; H.3.3"
        ],
        "authors": [
            "Sachin Sharma",
            "Federico Simonetta",
            "Michele Flammini"
        ]
    },
    {
        "title": "Gaussian Splatting with Discretized SDF for Relightable Assets",
        "summary": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and\nhighly efficient rendering speed in the novel view synthesis (NVS) task. The\napplication to inverse rendering still faces several challenges, as the\ndiscrete nature of Gaussian primitives makes it difficult to apply geometry\nconstraints. Recent works introduce the signed distance field (SDF) as an extra\ncontinuous representation to regularize the geometry defined by Gaussian\nprimitives. It improves the decomposition quality, at the cost of increasing\nmemory usage and complicating training. Unlike these works, we introduce a\ndiscretized SDF to represent the continuous SDF in a discrete manner by\nencoding it within each Gaussian using a sampled value. This approach allows us\nto link the SDF with the Gaussian opacity through an SDF-to-opacity\ntransformation, enabling rendering the SDF via splatting and avoiding the\ncomputational cost of ray marching.The key challenge is to regularize the\ndiscrete samples to be consistent with the underlying SDF, as the discrete\nrepresentation can hardly apply the gradient-based constraints (\\eg Eikonal\nloss). For this, we project Gaussians onto the zero-level set of SDF and\nenforce alignment with the surface from splatting, namely a projection-based\nconsistency loss. Thanks to the discretized SDF, our method achieves higher\nrelighting quality, while requiring no extra memory beyond GS and avoiding\ncomplex manually designed optimization. The experiments reveal that our method\noutperforms existing Gaussian-based inverse rendering methods. Our code is\navailable at https://github.com/NK-CS-ZZL/DiscretizedSDF.",
        "url": "http://arxiv.org/abs/2507.15629v1",
        "published_date": "2025-07-21T13:52:33+00:00",
        "updated_date": "2025-07-21T13:52:33+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Zuo-Liang Zhu",
            "Jian Yang",
            "Beibei Wang"
        ]
    },
    {
        "title": "A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications",
        "summary": "The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics.",
        "url": "http://arxiv.org/abs/2507.15628v1",
        "published_date": "2025-07-21T13:52:06+00:00",
        "updated_date": "2025-07-21T13:52:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shanjiang Tang",
            "Rui Huang",
            "Hsinyu Luo",
            "Chunjiang Wang",
            "Ce Yu",
            "Yusen Li",
            "Hao Fu",
            "Chao Sun",
            "and Jian Xiao"
        ]
    },
    {
        "title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation",
        "summary": "While the proposal of the Tri-plane representation has advanced the\ndevelopment of the 3D-aware image generative models, problems rooted in its\ninherent structure, such as multi-face artifacts caused by sharing the same\nfeatures in symmetric regions, limit its ability to generate 360$^\\circ$ view\nimages. In this paper, we propose CylinderPlane, a novel implicit\nrepresentation based on Cylindrical Coordinate System, to eliminate the feature\nambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different\nfrom the inevitable feature entanglement in Cartesian coordinate-based\nTri-plane representation, the cylindrical coordinate system explicitly\nseparates features at different angles, allowing our cylindrical representation\npossible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis.\nWe further introduce the nested cylinder representation that composites\nmultiple cylinders at different scales, thereby enabling the model more\nadaptable to complex geometry and varying resolutions. The combination of\ncylinders with different resolutions can effectively capture more critical\nlocations and multi-scale features, greatly facilitates fine detail learning\nand robustness to different resolutions. Moreover, our representation is\nagnostic to implicit rendering methods and can be easily integrated into any\nneural rendering pipeline. Extensive experiments on both synthetic dataset and\nunstructured in-the-wild images demonstrate that our proposed representation\nachieves superior performance over previous methods.",
        "url": "http://arxiv.org/abs/2507.15606v1",
        "published_date": "2025-07-21T13:28:59+00:00",
        "updated_date": "2025-07-21T13:28:59+00:00",
        "categories": [
            "cs.CV",
            "68T45",
            "I.4.5"
        ],
        "authors": [
            "Ru Jia",
            "Xiaozhuang Ma",
            "Jianji Wang",
            "Nanning Zheng"
        ]
    },
    {
        "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting",
        "summary": "Surface reconstruction and novel view rendering from sparse-view images are\nchallenging. Signed Distance Function (SDF)-based methods struggle with fine\ndetails, while 3D Gaussian Splatting (3DGS)-based approaches lack global\ngeometry coherence. We propose a novel hybrid method that combines the\nstrengths of both approaches: SDF captures coarse geometry to enhance\n3DGS-based rendering, while newly rendered images from 3DGS refine the details\nof SDF for accurate surface reconstruction. As a result, our method surpasses\nstate-of-the-art approaches in surface reconstruction and novel view synthesis\non the DTU and MobileBrick datasets. Code will be released at\nhttps://github.com/aim-uofa/SurfaceSplat.",
        "url": "http://arxiv.org/abs/2507.15602v2",
        "published_date": "2025-07-21T13:25:03+00:00",
        "updated_date": "2025-07-28T06:27:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zihui Gao",
            "Jia-Wang Bian",
            "Guosheng Lin",
            "Hao Chen",
            "Chunhua Shen"
        ]
    },
    {
        "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos",
        "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.",
        "url": "http://arxiv.org/abs/2507.15597v1",
        "published_date": "2025-07-21T13:19:09+00:00",
        "updated_date": "2025-07-21T13:19:09+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Hao Luo",
            "Yicheng Feng",
            "Wanpeng Zhang",
            "Sipeng Zheng",
            "Ye Wang",
            "Haoqi Yuan",
            "Jiazheng Liu",
            "Chaoyi Xu",
            "Qin Jin",
            "Zongqing Lu"
        ]
    },
    {
        "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging",
        "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\n\\href{https://github.com/Bekhouche/SegDT}{GitHub}.",
        "url": "http://arxiv.org/abs/2507.15595v1",
        "published_date": "2025-07-21T13:18:05+00:00",
        "updated_date": "2025-07-21T13:18:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Salah Eddine Bekhouche",
            "Gaby Maroun",
            "Fadi Dornaika",
            "Abdenour Hadid"
        ]
    },
    {
        "title": "Compress-Align-Detect: onboard change detection from unregistered images",
        "summary": "Change detection from satellite images typically incurs a delay ranging from\nseveral hours up to days because of latency in downlinking the acquired images\nand generating orthorectified image products at the ground stations; this may\npreclude real- or near real-time applications. To overcome this limitation, we\npropose shifting the entire change detection workflow onboard satellites. This\nrequires to simultaneously solve challenges in data storage, image registration\nand change detection with a strict complexity constraint. In this paper, we\npresent a novel and efficient framework for onboard change detection that\naddresses the aforementioned challenges in an end-to-end fashion with a deep\nneural network composed of three interlinked submodules: (1) image compression,\ntailored to minimize onboard data storage resources; (2) lightweight\nco-registration of non-orthorectified multi-temporal image pairs; and (3) a\nnovel temporally-invariant and computationally efficient change detection\nmodel. This is the first approach in the literature combining all these tasks\nin a single end-to-end framework with the constraints dictated by onboard\nprocessing. Experimental results compare each submodule with the current\nstate-of-the-art, and evaluate the performance of the overall integrated system\nin realistic setting on low-power hardware. Compelling change detection results\nare obtained in terms of F1 score as a function of compression rate, sustaining\na throughput of 0.7 Mpixel/s on a 15W accelerator.",
        "url": "http://arxiv.org/abs/2507.15578v1",
        "published_date": "2025-07-21T12:58:32+00:00",
        "updated_date": "2025-07-21T12:58:32+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Gabriele Inzerillo",
            "Diego Valsesia",
            "Aniello Fiengo",
            "Enrico Magli"
        ]
    },
    {
        "title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation",
        "summary": "Mixup has become a popular augmentation strategy for image classification,\nyet its naive pixel-wise interpolation often produces unrealistic images that\ncan hinder learning, particularly in high-stakes medical applications. We\npropose GeMix, a two-stage framework that replaces heuristic blending with a\nlearned, label-aware interpolation powered by class-conditional GANs. First, a\nStyleGAN2-ADA generator is trained on the target dataset. During augmentation,\nwe sample two label vectors from Dirichlet priors biased toward different\nclasses and blend them via a Beta-distributed coefficient. Then, we condition\nthe generator on this soft label to synthesize visually coherent images that\nlie along a continuous class manifold. We benchmark GeMix on the large-scale\nCOVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,\nEfficientNet-B0). When combined with real data, our method increases macro-F1\nover traditional mixup for all backbones, reducing the false negative rate for\nCOVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,\ndelivering stronger regularization and greater semantic fidelity, without\ndisrupting existing training pipelines. We publicly release our code at\nhttps://github.com/hugocarlesso/GeMix to foster reproducibility and further\nresearch.",
        "url": "http://arxiv.org/abs/2507.15577v1",
        "published_date": "2025-07-21T12:58:05+00:00",
        "updated_date": "2025-07-21T12:58:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Hugo Carlesso",
            "Maria Eliza Patulea",
            "Moncef Garouani",
            "Radu Tudor Ionescu",
            "Josiane Mothe"
        ]
    },
    {
        "title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging",
        "summary": "Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}.",
        "url": "http://arxiv.org/abs/2507.15576v1",
        "published_date": "2025-07-21T12:57:49+00:00",
        "updated_date": "2025-07-21T12:57:49+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Nicolas Poggi",
            "Shashank Agnihotri",
            "Margret Keuper"
        ]
    },
    {
        "title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding",
        "summary": "In recent years, the introduction of Multi-modal Large Language Models\n(MLLMs) into video understanding tasks has become increasingly prevalent.\nHowever, how to effectively integrate temporal information remains a critical\nresearch focus. Traditional approaches treat spatial and temporal information\nseparately. Due to issues like motion blur, it is challenging to accurately\nrepresent the spatial information of rapidly moving objects. This can lead to\ntemporally important regions being underemphasized during spatial feature\nextraction, which in turn hinders accurate spatio-temporal interaction and\nvideo understanding. To address this limitation, we propose an innovative video\nrepresentation method called Dynamic-Image (DynImg). Specifically, we introduce\na set of non-key frames as temporal prompts to highlight the spatial areas\ncontaining fast-moving objects. During the process of visual feature\nextraction, these prompts guide the model to pay additional attention to the\nfine-grained spatial features corresponding to these regions. Moreover, to\nmaintain the correct sequence for DynImg, we employ a corresponding 4D video\nRotary Position Embedding. This retains both the temporal and spatial adjacency\nof DynImg, helping MLLM understand the spatio-temporal order within this\ncombined format. Experimental evaluations reveal that DynImg surpasses the\nstate-of-the-art methods by approximately 2% across multiple video\nunderstanding benchmarks, proving the effectiveness of our temporal prompts in\nenhancing video comprehension.",
        "url": "http://arxiv.org/abs/2507.15569v1",
        "published_date": "2025-07-21T12:50:49+00:00",
        "updated_date": "2025-07-21T12:50:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyi Bao",
            "Chenwei Xie",
            "Hao Tang",
            "Tingyu Weng",
            "Xiaofeng Wang",
            "Yun Zheng",
            "Xingang Wang"
        ]
    },
    {
        "title": "A tissue and cell-level annotated H&E and PD-L1 histopathology image dataset in non-small cell lung cancer",
        "summary": "The tumor immune microenvironment (TIME) in non-small cell lung cancer\n(NSCLC) histopathology contains morphological and molecular characteristics\npredictive of immunotherapy response. Computational quantification of TIME\ncharacteristics, such as cell detection and tissue segmentation, can support\nbiomarker development. However, currently available digital pathology datasets\nof NSCLC for the development of cell detection or tissue segmentation\nalgorithms are limited in scope, lack annotations of clinically prevalent\nmetastatic sites, and forgo molecular information such as PD-L1\nimmunohistochemistry (IHC). To fill this gap, we introduce the IGNITE data\ntoolkit, a multi-stain, multi-centric, and multi-scanner dataset of annotated\nNSCLC whole-slide images. We publicly release 887 fully annotated regions of\ninterest from 155 unique patients across three complementary tasks: (i)\nmulti-class semantic segmentation of tissue compartments in H&E-stained slides,\nwith 16 classes spanning primary and metastatic NSCLC, (ii) nuclei detection,\nand (iii) PD-L1 positive tumor cell detection in PD-L1 IHC slides. To the best\nof our knowledge, this is the first public NSCLC dataset with manual\nannotations of H&E in metastatic sites and PD-L1 IHC.",
        "url": "http://arxiv.org/abs/2507.16855v1",
        "published_date": "2025-07-21T12:16:22+00:00",
        "updated_date": "2025-07-21T12:16:22+00:00",
        "categories": [
            "q-bio.QM",
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Joey Spronck",
            "Leander van Eekelen",
            "Dominique van Midden",
            "Joep Bogaerts",
            "Leslie Tessier",
            "Valerie Dechering",
            "Muradije Demirel-Andishmand",
            "Gabriel Silva de Souza",
            "Roland Nemeth",
            "Enrico Munari",
            "Giuseppe Bogina",
            "Ilaria Girolami",
            "Albino Eccher",
            "Balazs Acs",
            "Ceren Boyaci",
            "Natalie Klubickova",
            "Monika Looijen-Salamon",
            "Shoko Vos",
            "Francesco Ciompi"
        ]
    },
    {
        "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation",
        "summary": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.",
        "url": "http://arxiv.org/abs/2507.15542v1",
        "published_date": "2025-07-21T12:15:27+00:00",
        "updated_date": "2025-07-21T12:15:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinqian Lei",
            "Bo Wang",
            "Robby T. Tan"
        ]
    },
    {
        "title": "Towards Holistic Surgical Scene Graph",
        "summary": "Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com",
        "url": "http://arxiv.org/abs/2507.15541v2",
        "published_date": "2025-07-21T12:10:42+00:00",
        "updated_date": "2025-07-24T00:51:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jongmin Shin",
            "Enki Cho",
            "Ka Young Kim",
            "Jung Yong Kim",
            "Seong Tae Kim",
            "Namkee Oh"
        ]
    },
    {
        "title": "Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport",
        "summary": "We study the problem of self-supervised procedure learning, which discovers\nkey steps and establishes their order from a set of unlabeled procedural\nvideos. Previous procedure learning methods typically learn frame-to-frame\ncorrespondences between videos before determining key steps and their order.\nHowever, their performance often suffers from order variations,\nbackground/redundant frames, and repeated actions. To overcome these\nchallenges, we propose a self-supervised procedure learning framework, which\nutilizes a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior for computing frame-to-frame mapping between videos. However,\noptimizing exclusively for the above temporal alignment term may lead to\ndegenerate solutions, where all frames are mapped to a small cluster in the\nembedding space and hence every video is associated with only one key step. To\naddress that limitation, we further integrate a contrastive regularization\nterm, which maps different frames to different points in the embedding space,\navoiding the collapse to trivial solutions. Finally, we conduct extensive\nexperiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,\nProceL and CrossTask) benchmarks to demonstrate superior performance by our\napproach against previous methods, including OPEL which relies on a traditional\nKantorovich optimal transport formulation with an optimality prior.",
        "url": "http://arxiv.org/abs/2507.15540v1",
        "published_date": "2025-07-21T12:09:12+00:00",
        "updated_date": "2025-07-21T12:09:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Syed Ahmed Mahmood",
            "Ali Shah Ali",
            "Umer Ahmed",
            "Fawad Javed Fateh",
            "M. Zeeshan Zia",
            "Quoc-Huy Tran"
        ]
    },
    {
        "title": "CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis",
        "summary": "Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect\nterms within paired image-text data and determine their fine grained sentiment\npolarities, representing a fundamental task for improving the effectiveness of\napplications such as product review systems and public opinion monitoring.\nExisting methods face challenges such as cross modal alignment noise and\ninsufficient consistency in fine-grained representations. While global modality\nalignment methods often overlook the connection between aspect terms and their\ncorresponding local visual regions, bridging the representation gap between\ntext and images remains a challenge. To address these limitations, this paper\nintroduces an end to end Contrastive Learning framework with Adaptive\nMulti-loss and Progressive Attention Fusion(CLAMP). The framework is composed\nof three novel modules: Progressive Attention Fusion network, Multi-task\nContrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive\nAttention Fusion network enhances fine-grained alignment between textual\nfeatures and image regions via hierarchical, multi-stage cross modal\ninteractions, effectively suppressing irrelevant visual noise. Secondly,\nmulti-task contrastive learning combines global modal contrast and local\ngranularity alignment to enhance cross modal representation consistency.\nAdaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting\nmechanism to calibrate loss contributions according to each task's uncertainty,\nthereby mitigating gradient interference. Evaluation on standard public\nbenchmarks demonstrates that CLAMP consistently outperforms the vast majority\nof existing state of the art methods.",
        "url": "http://arxiv.org/abs/2507.16854v1",
        "published_date": "2025-07-21T11:49:57+00:00",
        "updated_date": "2025-07-21T11:49:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaoqiang He"
        ]
    },
    {
        "title": "RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation",
        "summary": "Accurate segmentation is crucial for clinical applications, but existing\nmodels often assume fixed, high-resolution inputs and degrade significantly\nwhen faced with lower-resolution data in real-world scenarios. To address this\nlimitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation\narchitecture that dynamically adapts its inference path to the spatial\nresolution of the input. Central to our design are multi-scale blocks\nintegrated at multiple encoder depths, a resolution-aware routing mechanism,\nand consistency-driven training that aligns multi-resolution features with\nfull-resolution representations. We evaluate RARE-UNet on two benchmark brain\nimaging tasks for hippocampus and tumor segmentation. Compared to standard\nUNet, its multi-resolution augmented variant, and nnUNet, our model achieves\nthe highest average Dice scores of 0.84 and 0.65 across resolution, while\nmaintaining consistent performance and significantly reduced inference time at\nlower resolutions. These results highlight the effectiveness and scalability of\nour architecture in achieving resolution-robust segmentation. The codes are\navailable at: https://github.com/simonsejse/RARE-UNet.",
        "url": "http://arxiv.org/abs/2507.15524v1",
        "published_date": "2025-07-21T11:49:20+00:00",
        "updated_date": "2025-07-21T11:49:20+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Simon Winther Albertsen",
            "Hjalte Svaneborg Bjørnstrup",
            "Mostafa Mehdipour Ghazi"
        ]
    },
    {
        "title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement",
        "summary": "Recent Transformer-based low-light enhancement methods have made promising\nprogress in recovering global illumination. However, they still struggle with\nnon-uniform lighting scenarios, such as backlit and shadow, appearing as\nover-exposure or inadequate brightness restoration. To address this challenge,\nwe present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)\nframework that enables accurate illumination restoration. Specifically, we\npropose a dynamic integral image representation to model the spatially-varying\nillumination, and further construct a novel Spatially-Adaptive Integral\nIllumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an\nIllumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which\nleverages the illumination to calibrate the lightness-relevant features toward\nvisual-pleased illumination enhancement. Extensive experiments on five standard\nlow-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our\nSAIGFormer significantly outperforms state-of-the-art methods in both\nquantitative and qualitative metrics. In particular, our method achieves\nsuperior performance in non-uniform illumination enhancement while exhibiting\nstrong generalization capabilities across multiple datasets. Code is available\nat https://github.com/LHTcode/SAIGFormer.git.",
        "url": "http://arxiv.org/abs/2507.15520v1",
        "published_date": "2025-07-21T11:38:56+00:00",
        "updated_date": "2025-07-21T11:38:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanting Li",
            "Fei Zhou",
            "Xin Sun",
            "Yang Hua",
            "Jungong Han",
            "Liang-Jie Zhang"
        ]
    },
    {
        "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner",
        "summary": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).",
        "url": "http://arxiv.org/abs/2507.15509v1",
        "published_date": "2025-07-21T11:22:17+00:00",
        "updated_date": "2025-07-21T11:22:17+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Lei Chen",
            "Xuanle Zhao",
            "Zhixiong Zeng",
            "Jing Huang",
            "Yufeng Zhong",
            "Lin Ma"
        ]
    },
    {
        "title": "Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization",
        "summary": "Despite recent advances, Text-to-video retrieval (TVR) is still hindered by\nmultiple inherent uncertainties, such as ambiguous textual queries, indistinct\ntext-video mappings, and low-quality video frames. Although interactive systems\nhave emerged to address these challenges by refining user intent through\nclarifying questions, current methods typically rely on heuristic or ad-hoc\nstrategies without explicitly quantifying these uncertainties, limiting their\neffectiveness. Motivated by this gap, we propose UMIVR, an\nUncertainty-Minimizing Interactive Text-to-Video Retrieval framework that\nexplicitly quantifies three critical uncertainties-text ambiguity, mapping\nuncertainty, and frame uncertainty-via principled, training-free metrics:\nsemantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon\ndivergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based\nFrame Sampler (TQFS). By adaptively generating targeted clarifying questions\nguided by these uncertainty measures, UMIVR iteratively refines user queries,\nsignificantly reducing retrieval ambiguity. Extensive experiments on multiple\nbenchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1\n(69.2\\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby\nestablishing an uncertainty-minimizing foundation for interactive TVR.",
        "url": "http://arxiv.org/abs/2507.15504v2",
        "published_date": "2025-07-21T11:12:39+00:00",
        "updated_date": "2025-07-24T10:32:09+00:00",
        "categories": [
            "cs.CV",
            "68T45",
            "I.2.10; H.3.3"
        ],
        "authors": [
            "Bingqing Zhang",
            "Zhuo Cao",
            "Heming Du",
            "Yang Li",
            "Xue Li",
            "Jiajun Liu",
            "Sen Wang"
        ]
    },
    {
        "title": "Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images",
        "summary": "Odometry is a critical task for autonomous systems for self-localization and\nnavigation. We propose a novel LiDAR-Visual odometry framework that integrates\nLiDAR point clouds and images for accurate and robust pose estimation. Our\nmethod utilizes a dense-depth map estimated from point clouds and images\nthrough depth completion, and incorporates a multi-scale feature extraction\nnetwork with attention mechanisms, enabling adaptive depth-aware\nrepresentations. Furthermore, we leverage dense depth information to refine\nflow estimation and mitigate errors in occlusion-prone regions. Our\nhierarchical pose refinement module optimizes motion estimation progressively,\nensuring robust predictions against dynamic environments and scale ambiguities.\nComprehensive experiments on the KITTI odometry benchmark demonstrate that our\napproach achieves similar or superior accuracy and robustness compared to\nstate-of-the-art visual and LiDAR odometry methods.",
        "url": "http://arxiv.org/abs/2507.15496v1",
        "published_date": "2025-07-21T10:58:10+00:00",
        "updated_date": "2025-07-21T10:58:10+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "JunYing Huang",
            "Ao Xu",
            "DongSun Yong",
            "KeRen Li",
            "YuanFeng Wang",
            "Qi Qin"
        ]
    },
    {
        "title": "GR-3 Technical Report",
        "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.",
        "url": "http://arxiv.org/abs/2507.15493v2",
        "published_date": "2025-07-21T10:54:13+00:00",
        "updated_date": "2025-07-22T15:04:37+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chilam Cheang",
            "Sijin Chen",
            "Zhongren Cui",
            "Yingdong Hu",
            "Liqun Huang",
            "Tao Kong",
            "Hang Li",
            "Yifeng Li",
            "Yuxiao Liu",
            "Xiao Ma",
            "Hao Niu",
            "Wenxuan Ou",
            "Wanli Peng",
            "Zeyu Ren",
            "Haixin Shi",
            "Jiawen Tian",
            "Hongtao Wu",
            "Xin Xiao",
            "Yuyang Xiao",
            "Jiafeng Xu",
            "Yichu Yang"
        ]
    },
    {
        "title": "An aerial color image anomaly dataset for search missions in complex forested terrain",
        "summary": "After a family murder in rural Germany, authorities failed to locate the\nsuspect in a vast forest despite a massive search. To aid the search, a\nresearch aircraft captured high-resolution aerial imagery. Due to dense\nvegetation obscuring small clues, automated analysis was ineffective, prompting\na crowd-search initiative. This effort produced a unique dataset of labeled,\nhard-to-detect anomalies under occluded, real-world conditions. It can serve as\na benchmark for improving anomaly detection approaches in complex forest\nenvironments, supporting manhunts and rescue operations. Initial benchmark\ntests showed existing methods performed poorly, highlighting the need for\ncontext-aware approaches. The dataset is openly accessible for offline\nprocessing. An additional interactive web interface supports online viewing and\ndynamic growth by allowing users to annotate and submit new findings.",
        "url": "http://arxiv.org/abs/2507.15492v1",
        "published_date": "2025-07-21T10:52:27+00:00",
        "updated_date": "2025-07-21T10:52:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rakesh John Amala Arokia Nathan",
            "Matthias Gessner",
            "Nurullah Özkan",
            "Marius Bock",
            "Mohamed Youssef",
            "Maximilian Mews",
            "Björn Piltz",
            "Ralf Berger",
            "Oliver Bimber"
        ]
    },
    {
        "title": "Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval",
        "summary": "Enabling efficient text-video retrieval on edge-end devices is critical for\nreal-world applications. Yet, existing methods face a critical challenge in\nbalancing accuracy and computational efficiency: uniform frame sampling methods\nensure content coverage but incur prohibitive computational costs, while\nsalient-frame sampling methods reduce overhead but suffer from query-agnostic\nframe selection that biases retrieval results. To address this, we propose\nProCLIP, a user-centric framework that achieves state-of-the-art accuracy with\nsignificantly improved efficiency. We design a prompt-aware frame sampling\nstrategy that dynamically guides lightweight feature extractors using textual\nprompts to select semantically relevant frames, overcoming the limitations of\nexisting salient-frame sampling methods which rely on static, query-agnostic\nselection criteria. Moreover, we adopt a two-stage candidate pruning strategy\nthat combines rapid coarse filtering via a lightweight module with CLIP-powered\nfine-grained re-ranking, enhancing retrieval efficiency while preserving\naccuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency\nreduction versus baselines while maintaining competitive accuracy, i.e.,\nR@1=49.0 in MSR-VTT dataset. Code is available at\nhttps://github.com/tiffylong/ProCLIP.",
        "url": "http://arxiv.org/abs/2507.15491v1",
        "published_date": "2025-07-21T10:46:49+00:00",
        "updated_date": "2025-07-21T10:46:49+00:00",
        "categories": [
            "cs.MM",
            "cs.CV"
        ],
        "authors": [
            "Deyu Zhang",
            "Tingting Long",
            "Jinrui Zhang",
            "Ligeng Chen",
            "Ju Ren",
            "Yaoxue Zhang"
        ]
    },
    {
        "title": "DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification",
        "summary": "Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency\ndomain information, which is crucial for accurate lesion classification in\nmedical imaging. However, effectively integrating multi-sequence MRI data for\nrobust 3D lesion classification remains a challenge. In this paper, we propose\nDeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel\nframework designed to extract decoupled representations and adaptively fuse\nspatial and spectral features for lesion classification. DeSamba introduces a\nDecoupled Representation Learning Module (DRLM) that decouples features from\ndifferent MRI sequences through self-reconstruction and cross-reconstruction,\nand a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,\nenabling dynamic fusion of spectral and spatial information based on lesion\ncharacteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On\na six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1\naccuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external\nvalidation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On\na spondylitis dataset (n=251) involving a challenging binary classification\ntask, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal\nand external validation sets, respectively. Ablation studies demonstrate that\nboth DRLM and SAMB significantly contribute to overall performance, with over\n10% relative improvement compared to the baseline. Our results highlight the\npotential of DeSamba as a generalizable and effective solution for 3D lesion\nclassification in multi-sequence medical imaging.",
        "url": "http://arxiv.org/abs/2507.15487v2",
        "published_date": "2025-07-21T10:42:21+00:00",
        "updated_date": "2025-07-22T02:14:23+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Dezhen Wang",
            "Sheng Miao",
            "Rongxin Chai",
            "Jiufa Cui"
        ]
    },
    {
        "title": "One Last Attention for Your Vision-Language Model",
        "summary": "Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable\nzero-shot performance, yet their downstream potential hinges on effective\nfine-tuning. Most adaptation methods typically focus on refining representation\nfrom separate modalities (text or vision) but neglect the critical role of\ntheir fused representations in the decision-making process, \\emph{\\ie} rational\nmatrix that drives the final prediction. To bridge the gap, we propose a simple\nyet effective \\textbf{R}ational \\textbf{Ada}ptaion ({RAda}) to explicitly\nexploit the final fused representation during fine-tuning. RAda employs a\nlearned mask, obtained from a lightweight attention layer attached at the end\nof a VLM, to dynamically calibrate the contribution of each element in the\nrational matrix, enabling targeted adjustments to the final cross-modal\ninteractions without incurring costly modifications to intermediate features.\nExperiments in different settings (i.e., updating, or freezing pretrained\nencoders in adaptation, and test-time training that can only access the\nunlabeled test data) show that RAda serves as a versatile fine-tuning\ntechnique, improving the baseline with minimal code and performing comparably\nagainst current arts in most settings. Code is available at\n\\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.",
        "url": "http://arxiv.org/abs/2507.15480v2",
        "published_date": "2025-07-21T10:35:32+00:00",
        "updated_date": "2025-07-28T04:47:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liang Chen",
            "Ghazi Shazan Ahmad",
            "Tianjun Yao",
            "Lingqiao Liu",
            "Zhiqiang Shen"
        ]
    },
    {
        "title": "A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization",
        "summary": "Surface defect detection of steel, especially the recognition of multi-scale\ndefects, has always been a major challenge in industrial manufacturing. Steel\nsurfaces not only have defects of various sizes and shapes, which limit the\naccuracy of traditional image processing and detection methods in complex\nenvironments. However, traditional defect detection methods face issues of\ninsufficient accuracy and high miss-detection rates when dealing with small\ntarget defects. To address this issue, this study proposes a detection\nframework based on deep learning, specifically YOLOv9s, combined with the\nC3Ghost module, SCConv module, and CARAFE upsampling operator, to improve\ndetection accuracy and model performance. First, the SCConv module is used to\nreduce feature redundancy and optimize feature representation by reconstructing\nthe spatial and channel dimensions. Second, the C3Ghost module is introduced to\nenhance the model's feature extraction ability by reducing redundant\ncomputations and parameter volume, thereby improving model efficiency. Finally,\nthe CARAFE upsampling operator, which can more finely reorganize feature maps\nin a content-aware manner, optimizes the upsampling process and ensures\ndetailed restoration of high-resolution defect regions. Experimental results\ndemonstrate that the proposed model achieves higher accuracy and robustness in\nsteel surface defect detection tasks compared to other methods, effectively\naddressing defect detection problems.",
        "url": "http://arxiv.org/abs/2507.15476v1",
        "published_date": "2025-07-21T10:30:38+00:00",
        "updated_date": "2025-07-21T10:30:38+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Cong Chen",
            "Ming Chen",
            "Hoileong Lee",
            "Yan Li",
            "Jiyang Yu"
        ]
    },
    {
        "title": "ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting",
        "summary": "3D Gaussian Splatting is renowned for its high-fidelity reconstructions and\nreal-time novel view synthesis, yet its lack of semantic understanding limits\nobject-level perception. In this work, we propose ObjectGS, an object-aware\nframework that unifies 3D scene reconstruction with semantic understanding.\nInstead of treating the scene as a unified whole, ObjectGS models individual\nobjects as local anchors that generate neural Gaussians and share object IDs,\nenabling precise object-level reconstruction. During training, we dynamically\ngrow or prune these anchors and optimize their features, while a one-hot ID\nencoding with a classification loss enforces clear semantic constraints. We\nshow through extensive experiments that ObjectGS not only outperforms\nstate-of-the-art methods on open-vocabulary and panoptic segmentation tasks,\nbut also integrates seamlessly with applications like mesh extraction and scene\nediting. Project page: https://ruijiezhu94.github.io/ObjectGS_page",
        "url": "http://arxiv.org/abs/2507.15454v1",
        "published_date": "2025-07-21T10:06:23+00:00",
        "updated_date": "2025-07-21T10:06:23+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Ruijie Zhu",
            "Mulin Yu",
            "Linning Xu",
            "Lihan Jiang",
            "Yixuan Li",
            "Tianzhu Zhang",
            "Jiangmiao Pang",
            "Bo Dai"
        ]
    },
    {
        "title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe",
        "summary": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.",
        "url": "http://arxiv.org/abs/2507.15444v1",
        "published_date": "2025-07-21T09:53:42+00:00",
        "updated_date": "2025-07-21T09:53:42+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Leonard Bauersfeld",
            "Davide Scaramuzza"
        ]
    },
    {
        "title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent",
        "summary": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning.",
        "url": "http://arxiv.org/abs/2507.15428v1",
        "published_date": "2025-07-21T09:27:45+00:00",
        "updated_date": "2025-07-21T09:27:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiaao Li",
            "Kaiyuan Li",
            "Chen Gao",
            "Yong Li",
            "Xinlei Chen"
        ]
    },
    {
        "title": "SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition",
        "summary": "Surgical phase recognition plays a crucial role in surgical workflow\nanalysis, enabling various applications such as surgical monitoring, skill\nassessment, and workflow optimization. Despite significant advancements in deep\nlearning-based surgical phase recognition, these models remain inherently\nopaque, making it difficult to understand how they make decisions. This lack of\ninterpretability hinders trust and makes it challenging to debug the model. To\naddress this challenge, we propose SurgX, a novel concept-based explanation\nframework that enhances the interpretability of surgical phase recognition\nmodels by associating neurons with relevant concepts. In this paper, we\nintroduce the process of selecting representative example sequences for\nneurons, constructing a concept set tailored to the surgical video dataset,\nassociating neurons with concepts and identifying neurons crucial for\npredictions. Through extensive experiments on two surgical phase recognition\nmodels, we validate our method and analyze the explanation for prediction. This\nhighlights the potential of our method in explaining surgical phase\nrecognition. The code is available at https://github.com/ailab-kyunghee/SurgX",
        "url": "http://arxiv.org/abs/2507.15418v1",
        "published_date": "2025-07-21T09:19:34+00:00",
        "updated_date": "2025-07-21T09:19:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ka Young Kim",
            "Hyeon Bae Kim",
            "Seong Tae Kim"
        ]
    },
    {
        "title": "Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond",
        "summary": "Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.",
        "url": "http://arxiv.org/abs/2507.15401v3",
        "published_date": "2025-07-21T09:04:29+00:00",
        "updated_date": "2025-07-24T08:24:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huiyu Zhai",
            "Xingxing Yang",
            "Yalan Ye",
            "Chenyang Li",
            "Bin Fan",
            "Changze Li"
        ]
    },
    {
        "title": "Blended Point Cloud Diffusion for Localized Text-guided Shape Editing",
        "summary": "Natural language offers a highly intuitive interface for enabling localized\nfine-grained edits of 3D shapes. However, prior works face challenges in\npreserving global coherence while locally modifying the input 3D shape. In this\nwork, we introduce an inpainting-based framework for editing shapes represented\nas point clouds. Our approach leverages foundation 3D diffusion models for\nachieving localized shape edits, adding structural guidance in the form of a\npartial conditional shape, ensuring that other regions correctly preserve the\nshape's identity. Furthermore, to encourage identity preservation also within\nthe local edited region, we propose an inference-time coordinate blending\nalgorithm which balances reconstruction of the full shape with inpainting at a\nprogression of noise levels during the inference process. Our coordinate\nblending algorithm seamlessly blends the original shape with its edited\nversion, enabling a fine-grained editing of 3D shapes, all while circumventing\nthe need for computationally expensive and often inaccurate inversion.\nExtensive experiments show that our method outperforms alternative techniques\nacross a wide range of metrics that evaluate both fidelity to the original\nshape and also adherence to the textual description.",
        "url": "http://arxiv.org/abs/2507.15399v1",
        "published_date": "2025-07-21T09:00:19+00:00",
        "updated_date": "2025-07-21T09:00:19+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Etai Sella",
            "Noam Atia",
            "Ron Mokady",
            "Hadar Averbuch-Elor"
        ]
    },
    {
        "title": "To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models",
        "summary": "Active learning (AL) seeks to reduce annotation costs by selecting the most\ninformative samples for labeling, making it particularly valuable in\nresource-constrained settings. However, traditional evaluation methods, which\nfocus solely on final accuracy, fail to capture the full dynamics of the\nlearning process. To address this gap, we propose PALM (Performance Analysis of\nActive Learning Models), a unified and interpretable mathematical model that\ncharacterizes AL trajectories through four key parameters: achievable accuracy,\ncoverage efficiency, early-stage performance, and scalability. PALM provides a\npredictive description of AL behavior from partial observations, enabling the\nestimation of future performance and facilitating principled comparisons across\ndifferent strategies. We validate PALM through extensive experiments on\nCIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and\nself-supervised embeddings. Our results demonstrate that PALM generalizes\neffectively across datasets, budgets, and strategies, accurately predicting\nfull learning curves from limited labeled data. Importantly, PALM reveals\ncrucial insights into learning efficiency, data space coverage, and the\nscalability of AL methods. By enabling the selection of cost-effective\nstrategies and predicting performance under tight budget constraints, PALM lays\nthe basis for more systematic, reproducible, and data-efficient evaluation of\nAL in both research and real-world applications. The code is available at:\nhttps://github.com/juliamachnio/PALM.",
        "url": "http://arxiv.org/abs/2507.15381v1",
        "published_date": "2025-07-21T08:37:44+00:00",
        "updated_date": "2025-07-21T08:37:44+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Julia Machnio",
            "Mads Nielsen",
            "Mostafa Mehdipour Ghazi"
        ]
    },
    {
        "title": "Coarse-to-fine crack cue for robust crack detection",
        "summary": "Crack detection is an important task in computer vision. Despite impressive\nin-dataset performance, deep learning-based methods still struggle in\ngeneralizing to unseen domains. The thin structure property of cracks is\nusually overlooked by previous methods. In this work, we introduce CrackCue, a\nnovel method for robust crack detection based on coarse-to-fine crack cue\ngeneration. The core concept lies on leveraging the thin structure property to\ngenerate a robust crack cue, guiding the crack detection. Specifically, we\nfirst employ a simple max-pooling and upsampling operation on the crack image.\nThis results in a coarse crack-free background, based on which a fine\ncrack-free background can be obtained via a reconstruction network. The\ndifference between the original image and fine crack-free background provides a\nfine crack cue. This fine cue embeds robust crack prior information which is\nunaffected by complex backgrounds, shadow, and varied lighting. As a\nplug-and-play method, we incorporate the proposed CrackCue into three advanced\ncrack detection networks. Extensive experimental results demonstrate that the\nproposed CrackCue significantly improves the generalization ability and\nrobustness of the baseline methods. The source code will be publicly available.",
        "url": "http://arxiv.org/abs/2507.16851v1",
        "published_date": "2025-07-21T08:36:05+00:00",
        "updated_date": "2025-07-21T08:36:05+00:00",
        "categories": [
            "cs.CV",
            "cs.NE",
            "eess.IV"
        ],
        "authors": [
            "Zelong Liu",
            "Yuliang Gu",
            "Zhichao Sun",
            "Huachao Zhu",
            "Xin Xiao",
            "Bo Du",
            "Laurent Najman",
            "Yongchao Xu"
        ]
    },
    {
        "title": "Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors",
        "summary": "Monocular 3D human pose estimation remains a challenging and ill-posed\nproblem, particularly in real-time settings and unconstrained environments.\nWhile direct imageto-3D approaches require large annotated datasets and heavy\nmodels, 2D-to-3D lifting offers a more lightweight and flexible\nalternative-especially when enhanced with prior knowledge. In this work, we\npropose a framework that combines real-time 2D keypoint detection with\ngeometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics\nand subject-specific anatomical priors. Our approach builds on recent advances\nin self-calibration and biomechanically-constrained inverse kinematics to\ngenerate large-scale, plausible 2D-3D training pairs from MoCap and synthetic\ndatasets. We discuss how these ingredients can enable fast, personalized, and\naccurate 3D pose estimation from monocular images without requiring specialized\nhardware. This proposal aims to foster discussion on bridging data-driven\nlearning and model-based priors to improve accuracy, interpretability, and\ndeployability of 3D human motion capture on edge devices in the wild.",
        "url": "http://arxiv.org/abs/2507.16850v1",
        "published_date": "2025-07-21T08:18:23+00:00",
        "updated_date": "2025-07-21T08:18:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohamed Adjel"
        ]
    },
    {
        "title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data",
        "summary": "The state of the art in human-centric computer vision achieves high accuracy\nand robustness across a diverse range of tasks. The most effective models in\nthis domain have billions of parameters, thus requiring extremely large\ndatasets, expensive training regimes, and compute-intensive inference. In this\npaper, we demonstrate that it is possible to train models on much smaller but\nhigh-fidelity synthetic datasets, with no loss in accuracy and higher\nefficiency. Using synthetic training data provides us with excellent levels of\ndetail and perfect labels, while providing strong guarantees for data\nprovenance, usage rights, and user consent. Procedural data synthesis also\nprovides us with explicit control on data diversity, that we can use to address\nunfairness in the models we train. Extensive quantitative assessment on real\ninput images demonstrates accuracy of our models on three dense prediction\ntasks: depth estimation, surface normal estimation, and soft foreground\nsegmentation. Our models require only a fraction of the cost of training and\ninference when compared with foundational models of similar accuracy. Our\nhuman-centric synthetic dataset and trained models are available at\nhttps://aka.ms/DAViD.",
        "url": "http://arxiv.org/abs/2507.15365v1",
        "published_date": "2025-07-21T08:17:41+00:00",
        "updated_date": "2025-07-21T08:17:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fatemeh Saleh",
            "Sadegh Aliakbarian",
            "Charlie Hewitt",
            "Lohit Petikam",
            "Xiao-Xian",
            "Antonio Criminisi",
            "Thomas J. Cashman",
            "Tadas Baltrušaitis"
        ]
    },
    {
        "title": "Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation",
        "summary": "Medical image segmentation suffers from data scarcity, particularly in polyp\ndetection where annotation requires specialized expertise. We present SynDiff,\na framework combining text-guided synthetic data generation with efficient\ndiffusion-based segmentation. Our approach employs latent diffusion models to\ngenerate clinically realistic synthetic polyps through text-conditioned\ninpainting, augmenting limited training data with semantically diverse samples.\nUnlike traditional diffusion methods requiring iterative denoising, we\nintroduce direct latent estimation enabling single-step inference with T x\ncomputational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%\nIoU while maintaining real-time capability suitable for clinical deployment.\nThe framework demonstrates that controlled synthetic augmentation improves\nsegmentation robustness without distribution shift. SynDiff bridges the gap\nbetween data-hungry deep learning models and clinical constraints, offering an\nefficient solution for deployment in resourcelimited medical settings.",
        "url": "http://arxiv.org/abs/2507.15361v1",
        "published_date": "2025-07-21T08:15:17+00:00",
        "updated_date": "2025-07-21T08:15:17+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Muhammad Aqeel",
            "Maham Nazir",
            "Zanxi Ruan",
            "Francesco Setti"
        ]
    },
    {
        "title": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection",
        "summary": "Pavement defect detection faces critical challenges including limited\nannotated data, domain shift between training and deployment environments, and\nhigh variability in defect appearances across different road conditions. We\npropose RoadFusion, a framework that addresses these limitations through\nsynthetic anomaly generation with dual-path feature adaptation. A latent\ndiffusion model synthesizes diverse, realistic defects using text prompts and\nspatial masks, enabling effective training under data scarcity. Two separate\nfeature adaptors specialize representations for normal and anomalous inputs,\nimproving robustness to domain shift and defect variability. A lightweight\ndiscriminator learns to distinguish fine-grained defect patterns at the patch\nlevel. Evaluated on six benchmark datasets, RoadFusion achieves consistently\nstrong performance across both classification and localization tasks, setting\nnew state-of-the-art in multiple metrics relevant to real-world road\ninspection.",
        "url": "http://arxiv.org/abs/2507.15346v1",
        "published_date": "2025-07-21T08:01:08+00:00",
        "updated_date": "2025-07-21T08:01:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Aqeel",
            "Kidus Dagnaw Bellete",
            "Francesco Setti"
        ]
    },
    {
        "title": "MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis",
        "summary": "High-resolution volumetric computed tomography (CT) is essential for accurate\ndiagnosis and treatment planning in thoracic diseases; however, it is limited\nby radiation dose and hardware costs. We present the Transformer Volumetric\nSuper-Resolution Network (\\textbf{TVSRN-V2}), a transformer-based\nsuper-resolution (SR) framework designed for practical deployment in clinical\nlung CT analysis. Built from scalable components, including Through-Plane\nAttention Blocks (TAB) and Swin Transformer V2 -- our model effectively\nreconstructs fine anatomical details in low-dose CT volumes and integrates\nseamlessly with downstream analysis pipelines. We evaluate its effectiveness on\nthree critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis\n-- across multiple clinical cohorts. To enhance robustness across variable\nacquisition protocols, we introduce pseudo-low-resolution augmentation,\nsimulating scanner diversity without requiring private data. TVSRN-V2\ndemonstrates a significant improvement in segmentation accuracy (+4\\% Dice),\nhigher radiomic feature reproducibility, and enhanced predictive performance\n(+0.06 C-index and AUC). These results indicate that SR-driven recovery of\nstructural detail significantly enhances clinical decision support, positioning\nTVSRN-V2 as a well-engineered, clinically viable system for dose-efficient\nimaging and quantitative analysis in real-world CT workflows.",
        "url": "http://arxiv.org/abs/2507.15340v1",
        "published_date": "2025-07-21T07:53:49+00:00",
        "updated_date": "2025-07-21T07:53:49+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Marc Boubnovski Martell",
            "Kristofer Linton-Reid",
            "Mitchell Chen",
            "Sumeet Hindocha",
            "Benjamin Hunter",
            "Marco A. Calzado",
            "Richard Lee",
            "Joram M. Posma",
            "Eric O. Aboagye"
        ]
    },
    {
        "title": "ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis",
        "summary": "Industrial defect detection systems face critical limitations when confined\nto one-class anomaly detection paradigms, which assume uniform outlier\ndistributions and struggle with data scarcity in realworld manufacturing\nenvironments. We present ExDD (Explicit Dual Distribution), a novel framework\nthat transcends these limitations by explicitly modeling dual feature\ndistributions. Our approach leverages parallel memory banks that capture the\ndistinct statistical properties of both normality and anomalous patterns,\naddressing the fundamental flaw of uniform outlier assumptions. To overcome\ndata scarcity, we employ latent diffusion models with domain-specific textual\nconditioning, generating in-distribution synthetic defects that preserve\nindustrial context. Our neighborhood-aware ratio scoring mechanism elegantly\nfuses complementary distance metrics, amplifying signals in regions exhibiting\nboth deviation from normality and similarity to known defect patterns.\nExperimental validation on KSDD2 demonstrates superior performance (94.2%\nI-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.",
        "url": "http://arxiv.org/abs/2507.15335v1",
        "published_date": "2025-07-21T07:49:00+00:00",
        "updated_date": "2025-07-21T07:49:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Muhammad Aqeel",
            "Federico Leonardi",
            "Francesco Setti"
        ]
    },
    {
        "title": "Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery",
        "summary": "We propose a vision transformer (ViT)-based deep learning framework to refine\ndisaster-affected area segmentation from remote sensing imagery, aiming to\nsupport and enhance the Emergent Value Added Product (EVAP) developed by the\nTaiwan Space Agency (TASA). The process starts with a small set of manually\nannotated regions. We then apply principal component analysis (PCA)-based\nfeature space analysis and construct a confidence index (CI) to expand these\nlabels, producing a weakly supervised training set. These expanded labels are\nthen used to train ViT-based encoder-decoder models with multi-band inputs from\nSentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder\nvariants and multi-stage loss strategies to improve performance under limited\nsupervision. During the evaluation, model predictions are compared with\nhigher-resolution EVAP output to assess spatial coherence and segmentation\nconsistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes\nwildfire demonstrate that our framework improves the smoothness and reliability\nof segmentation results, offering a scalable approach for disaster mapping when\naccurate ground truth is unavailable.",
        "url": "http://arxiv.org/abs/2507.16849v1",
        "published_date": "2025-07-21T07:48:07+00:00",
        "updated_date": "2025-07-21T07:48:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yi-Shan Chu",
            "Hsuan-Cheng Wei"
        ]
    },
    {
        "title": "BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?",
        "summary": "Depth estimation is a fundamental task in computer vision with diverse\napplications. Recent advancements in deep learning have led to powerful depth\nfoundation models (DFMs), yet their evaluation remains challenging due to\ninconsistencies in existing protocols. Traditional benchmarks rely on\nalignment-based metrics that introduce biases, favor certain depth\nrepresentations, and complicate fair comparisons. In this work, we propose\nBenchDepth, a new benchmark that evaluates DFMs through five carefully selected\ndownstream proxy tasks: depth completion, stereo matching, monocular\nfeed-forward 3D scene reconstruction, SLAM, and vision-language spatial\nunderstanding. Unlike conventional evaluation protocols, our approach assesses\nDFMs based on their practical utility in real-world applications, bypassing\nproblematic alignment procedures. We benchmark eight state-of-the-art DFMs and\nprovide an in-depth analysis of key findings and observations. We hope our work\nsparks further discussion in the community on best practices for depth model\nevaluation and paves the way for future research and advancements in depth\nestimation.",
        "url": "http://arxiv.org/abs/2507.15321v1",
        "published_date": "2025-07-21T07:23:14+00:00",
        "updated_date": "2025-07-21T07:23:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenyu Li",
            "Haotong Lin",
            "Jiashi Feng",
            "Peter Wonka",
            "Bingyi Kang"
        ]
    },
    {
        "title": "Few-Shot Object Detection via Spatial-Channel State Space Model",
        "summary": "Due to the limited training samples in few-shot object detection (FSOD), we\nobserve that current methods may struggle to accurately extract effective\nfeatures from each channel. Specifically, this issue manifests in two aspects:\ni) channels with high weights may not necessarily be effective, and ii)\nchannels with low weights may still hold significant value. To handle this\nproblem, we consider utilizing the inter-channel correlation to facilitate the\nnovel model's adaptation process to novel conditions, ensuring the model can\ncorrectly highlight effective channels and rectify those incorrect ones. Since\nthe channel sequence is also 1-dimensional, its similarity with the temporal\nsequence inspires us to take Mamba for modeling the correlation in the channel\nsequence. Based on this concept, we propose a Spatial-Channel State Space\nModeling (SCSM) module for spatial-channel state modeling, which highlights the\neffective patterns and rectifies those ineffective ones in feature channels. In\nSCSM, we design the Spatial Feature Modeling (SFM) module to balance the\nlearning of spatial relationships and channel relationships, and then introduce\nthe Channel State Modeling (CSM) module based on Mamba to learn correlation in\nchannels. Extensive experiments on the VOC and COCO datasets show that the SCSM\nmodule enables the novel detector to improve the quality of focused feature\nrepresentation in channels and achieve state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2507.15308v1",
        "published_date": "2025-07-21T07:08:19+00:00",
        "updated_date": "2025-07-21T07:08:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhimeng Xin",
            "Tianxu Wu",
            "Yixiong Zou",
            "Shiming Chen",
            "Dingjie Fu",
            "Xinge You"
        ]
    },
    {
        "title": "Minutiae-Anchored Local Dense Representation for Fingerprint Matching",
        "summary": "Fingerprint matching under diverse capture conditions remains a fundamental\nchallenge in biometric recognition. To achieve robust and accurate performance\nin such scenarios, we propose DMD, a minutiae-anchored local dense\nrepresentation which captures both fine-grained ridge textures and\ndiscriminative minutiae features in a spatially structured manner.\nSpecifically, descriptors are extracted from local patches centered and\noriented on each detected minutia, forming a three-dimensional tensor, where\ntwo dimensions represent spatial locations on the fingerprint plane and the\nthird encodes semantic features. This representation explicitly captures\nabstract features of local image patches, enabling a multi-level, fine-grained\ndescription that aggregates information from multiple minutiae and their\nsurrounding ridge structures. Furthermore, thanks to its strong spatial\ncorrespondence with the patch image, DMD allows for the use of foreground\nsegmentation masks to identify valid descriptor regions. During matching,\ncomparisons are then restricted to overlapping foreground areas, improving\nefficiency and robustness. Extensive experiments on rolled, plain, parital,\ncontactless, and latent fingerprint datasets demonstrate the effectiveness and\ngeneralizability of the proposed method. It achieves state-of-the-art accuracy\nacross multiple benchmarks while maintaining high computational efficiency,\nshowing strong potential for large-scale fingerprint recognition. Corresponding\ncode is available at https://github.com/Yu-Yy/DMD.",
        "url": "http://arxiv.org/abs/2507.15297v1",
        "published_date": "2025-07-21T06:55:54+00:00",
        "updated_date": "2025-07-21T06:55:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyu Pan",
            "Xiongjun Guan",
            "Yongjie Duan",
            "Jianjiang Feng",
            "Jie Zhou"
        ]
    },
    {
        "title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control",
        "summary": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.",
        "url": "http://arxiv.org/abs/2507.15292v4",
        "published_date": "2025-07-21T06:47:44+00:00",
        "updated_date": "2025-07-24T13:26:19+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "An Wang",
            "Rulin Zhou",
            "Mengya Xu",
            "Yiru Ye",
            "Longfei Gou",
            "Yiting Chang",
            "Hao Chen",
            "Chwee Ming Lim",
            "Jiankun Wang",
            "Hongliang Ren"
        ]
    },
    {
        "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems",
        "summary": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection.",
        "url": "http://arxiv.org/abs/2507.15285v1",
        "published_date": "2025-07-21T06:35:46+00:00",
        "updated_date": "2025-07-21T06:35:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lazaro Janier Gonzalez-Soler",
            "Maciej Salwowski",
            "Christoph Busch"
        ]
    },
    {
        "title": "Conditional Video Generation for High-Efficiency Video Compression",
        "summary": "Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.",
        "url": "http://arxiv.org/abs/2507.15269v1",
        "published_date": "2025-07-21T06:16:27+00:00",
        "updated_date": "2025-07-21T06:16:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fangqiu Yi",
            "Jingyu Xu",
            "Jiawei Shao",
            "Chi Zhang",
            "Xuelong Li"
        ]
    },
    {
        "title": "MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP",
        "summary": "Image-to-point-cloud (I2P) registration is a fundamental problem in computer\nvision, focusing on establishing 2D-3D correspondences between an image and a\npoint cloud. The differential perspective-n-point (PnP) has been widely used to\nsupervise I2P registration networks by enforcing the projective constraints on\n2D-3D correspondences. However, differential PnP is highly sensitive to noise\nand outliers in the predicted correspondences. This issue hinders the\neffectiveness of correspondence learning. Inspired by the robustness of blind\nPnP against noise and outliers in correspondences, we propose an approximated\nblind PnP based correspondence learning approach. To mitigate the high\ncomputational cost of blind PnP, we simplify blind PnP to an amenable task of\nminimizing Chamfer distance between learned 2D and 3D keypoints, called\nMinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task\nlearning module, named as MinCD-Net, which can be easily integrated into the\nexisting I2P registration architectures. Extensive experiments on 7-Scenes,\nRGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net\noutperforms state-of-the-art methods and achieves a higher inlier ratio (IR)\nand registration recall (RR) in both cross-scene and cross-dataset settings.",
        "url": "http://arxiv.org/abs/2507.15257v1",
        "published_date": "2025-07-21T05:38:16+00:00",
        "updated_date": "2025-07-21T05:38:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pei An",
            "Jiaqi Yang",
            "Muyao Peng",
            "You Yang",
            "Qiong Liu",
            "Xiaolin Wu",
            "Liangliang Nan"
        ]
    },
    {
        "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers",
        "summary": "In light of recent breakthroughs in text-to-image (T2I) generation,\nparticularly with diffusion transformers (DiT), subject-driven technologies are\nincreasingly being employed for high-fidelity customized production that\npreserves subject identity from reference inputs, enabling thrilling design\nworkflows and engaging entertainment. Existing alternatives typically require\neither per-subject optimization via trainable text embeddings or training\nspecialized encoders for subject feature extraction on large-scale datasets.\nSuch dependencies on training procedures fundamentally constrain their\npractical applications. More importantly, current methodologies fail to fully\nleverage the inherent zero-shot potential of modern diffusion transformers\n(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this\ngap, we propose FreeCus, a genuinely training-free framework that activates\nDiT's capabilities through three key innovations: 1) We introduce a pivotal\nattention sharing mechanism that captures the subject's layout integrity while\npreserving crucial editing flexibility. 2) Through a straightforward analysis\nof DiT's dynamic shifting, we propose an upgraded variant that significantly\nimproves fine-grained feature extraction. 3) We further integrate advanced\nMultimodal Large Language Models (MLLMs) to enrich cross-modal semantic\nrepresentations. Extensive experiments reflect that our method successfully\nunlocks DiT's zero-shot ability for consistent subject synthesis across diverse\ncontexts, achieving state-of-the-art or comparable results compared to\napproaches that require additional training. Notably, our framework\ndemonstrates seamless compatibility with existing inpainting pipelines and\ncontrol modules, facilitating more compelling experiences. Our code is\navailable at: https://github.com/Monalissaa/FreeCus.",
        "url": "http://arxiv.org/abs/2507.15249v1",
        "published_date": "2025-07-21T05:15:45+00:00",
        "updated_date": "2025-07-21T05:15:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanbing Zhang",
            "Zhe Wang",
            "Qin Zhou",
            "Mengping Yang"
        ]
    },
    {
        "title": "Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation",
        "summary": "Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model\npre-trained with DINO combined with a prototypical classifier outperforms the\nlatest SOTA methods. A crucial limitation that needs to be overcome is that\nupdating too many parameters of the transformers leads to overfitting due to\nthe scarcity of labeled samples. To address this challenge, we propose a new\nconcept, Coalescent Projection (CP), as an effective successor to soft prompts.\nAdditionally, we propose a novel pseudo-class generation method combined with\nSelf-Supervised Transformations (SSTs) that relies solely on the base domain to\nprepare the network for encountering unseen samples from different domains. The\nproposed method exhibits its effectiveness in comprehensive experiments on the\nextreme domain shift scenario of the BSCD-FSL benchmark. Our code is published\nat https://github.com/Naeem-Paeedeh/CPLSR.",
        "url": "http://arxiv.org/abs/2507.15243v1",
        "published_date": "2025-07-21T05:01:27+00:00",
        "updated_date": "2025-07-21T05:01:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Naeem Paeedeh",
            "Mahardhika Pratama",
            "Wolfgang Mayer",
            "Jimmy Cao",
            "Ryszard Kowlczyk"
        ]
    },
    {
        "title": "Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders",
        "summary": "Interpretability is critical in high-stakes domains such as medical imaging,\nwhere understanding model decisions is essential for clinical adoption. In this\nwork, we introduce Sparse Autoencoder (SAE)-based interpretability to breast\nimaging by analyzing {Mammo-CLIP}, a vision--language foundation model\npretrained on large-scale mammogram image--report pairs. We train a patch-level\n\\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features\nassociated with clinically relevant breast concepts such as \\textit{mass} and\n\\textit{suspicious calcification}. Our findings reveal that top activated class\nlevel latent neurons in the SAE latent space often tend to align with ground\ntruth regions, and also uncover several confounding factors influencing the\nmodel's decision-making process. Additionally, we analyze which latent neurons\nthe model relies on during downstream finetuning for improving the breast\nconcept prediction. This study highlights the promise of interpretable SAE\nlatent representations in providing deeper insight into the internal workings\nof foundation models at every layer for breast imaging.",
        "url": "http://arxiv.org/abs/2507.15227v1",
        "published_date": "2025-07-21T03:59:21+00:00",
        "updated_date": "2025-07-21T03:59:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Krishna Kanth Nakka"
        ]
    },
    {
        "title": "Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel",
        "summary": "Advancements in 3D vision have increased the impact of blood vessel modeling\non medical applications. However, accurately representing the complex geometry\nand topology of blood vessels remains a challenge due to their intricate\nbranching patterns, curvatures, and irregular shapes. In this study, we propose\na hierarchical part-based frame work for 3D vessel generation that separates\nthe global binary tree-like topology from local geometric details. Our approach\nproceeds in three stages: (1) key graph generation to model the overall\nhierarchical struc ture, (2) vessel segment generation conditioned on geometric\nproperties, and (3) hierarchical vessel assembly by integrating the local\nsegments according to the global key graph. We validate our framework on real\nworld datasets, demonstrating superior performance over existing methods in\nmodeling complex vascular networks. This work marks the first successful\napplication of a part-based generative approach for 3D vessel modeling, setting\na new benchmark for vascular data generation. The code is available at:\nhttps://github.com/CybercatChen/PartVessel.git.",
        "url": "http://arxiv.org/abs/2507.15223v1",
        "published_date": "2025-07-21T03:52:25+00:00",
        "updated_date": "2025-07-21T03:52:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siqi Chen",
            "Guoqing Zhang",
            "Jiahao Lai",
            "Bingzhi Shen",
            "Sihong Zhang",
            "Caixia Dong",
            "Xuejin Chen",
            "Yang Li"
        ]
    },
    {
        "title": "Improving Joint Embedding Predictive Architecture with Diffusion Noise",
        "summary": "Self-supervised learning has become an incredibly successful method for\nfeature learning, widely applied to many downstream tasks. It has proven\nespecially effective for discriminative tasks, surpassing the trending\ngenerative models. However, generative models perform better in image\ngeneration and detail enhancement. Thus, it is natural for us to find a\nconnection between SSL and generative models to further enhance the\nrepresentation capacity of SSL. As generative models can create new samples by\napproximating the data distribution, such modeling should also lead to a\nsemantic understanding of the raw visual data, which is necessary for\nrecognition tasks. This enlightens us to combine the core principle of the\ndiffusion model: diffusion noise, with SSL to learn a competitive recognition\nmodel. Specifically, diffusion noise can be viewed as a particular state of\nmask that reveals a close relationship between masked image modeling (MIM) and\ndiffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to\nincorporate diffusion noise into MIM by the position embedding of masked\ntokens. The multi-level noise schedule is a series of feature augmentations to\nfurther enhance the robustness of our model. We perform a comprehensive study\nto confirm its effectiveness in the classification of downstream tasks. Codes\nwill be released soon in public.",
        "url": "http://arxiv.org/abs/2507.15216v1",
        "published_date": "2025-07-21T03:36:58+00:00",
        "updated_date": "2025-07-21T03:36:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuping Qiu",
            "Rui Zhu",
            "Ying-cong Chen"
        ]
    },
    {
        "title": "MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction",
        "summary": "In this paper, we introduce MeshMamba, a neural network model for learning 3D\narticulated mesh models by employing the recently proposed Mamba State Space\nModels (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large\nnumber of input tokens, enabling the generation and reconstruction of body mesh\nmodels with more than 10,000 vertices, capturing clothing and hand geometries.\nThe key to effectively learning MeshMamba is the serialization technique of\nmesh vertices into orderings that are easily processed by Mamba. This is\nachieved by sorting the vertices based on body part annotations or the 3D\nvertex locations of a template mesh, such that the ordering respects the\nstructure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,\na denoising diffusion model for generating 3D articulated meshes and 2)\nMamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape\nand pose from a single image. Experimental results showed that MambaDiff3D can\ngenerate dense 3D human meshes in clothes, with grasping hands, etc., and\noutperforms previous approaches in the 3D human shape generation task.\nAdditionally, Mamba-HMR extends the capabilities of previous non-parametric\nhuman mesh recovery approaches, which were limited to handling body-only poses\nusing around 500 vertex tokens, to the whole-body setting with face and hands,\nwhile achieving competitive performance in (near) real-time.",
        "url": "http://arxiv.org/abs/2507.15212v1",
        "published_date": "2025-07-21T03:24:30+00:00",
        "updated_date": "2025-07-21T03:24:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yusuke Yoshiyasu",
            "Leyuan Sun",
            "Ryusuke Sagawa"
        ]
    },
    {
        "title": "Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins",
        "summary": "Cardiac digital twins (CDTs) provide personalized in-silico cardiac\nrepresentations and hold great potential for precision medicine in cardiology.\nHowever, whole-heart CDT models that simulate the full organ-scale\nelectromechanics of all four heart chambers remain limited. In this work, we\npropose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh\ndirectly from multi-view 2D cardiac cine MRIs. This is achieved by learning a\nself-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the\ngeneration of personalized heart models that closely correspond to input cine\nMRIs. The resulting 4D heart meshes can facilitate the automatic extraction of\nkey cardiac variables, including ejection fraction and dynamic chamber volume\nchanges with high temporal resolution. It demonstrates the feasibility of\ninferring personalized 4D heart models from cardiac MRIs, paving the way for an\nefficient CDT platform for precision medicine. The code will be publicly\nreleased once the manuscript is accepted.",
        "url": "http://arxiv.org/abs/2507.15203v1",
        "published_date": "2025-07-21T03:01:33+00:00",
        "updated_date": "2025-07-21T03:01:33+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Xiaoyue Liu",
            "Xicheng Sheng",
            "Xiahai Zhuang",
            "Vicente Grau",
            "Mark YY Chan",
            "Ching-Hui Sia",
            "Lei Li"
        ]
    },
    {
        "title": "Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling",
        "summary": "Accurate representation of myocardial infarct geometry is crucial for\npatient-specific cardiac modeling in MI patients. While Late gadolinium\nenhancement (LGE) MRI is the clinical gold standard for infarct detection, it\nrequires contrast agents, introducing side effects and patient discomfort.\nMoreover, infarct reconstruction from LGE often relies on sparsely sampled 2D\nslices, limiting spatial resolution and accuracy. In this work, we propose a\nnovel framework for automatically reconstructing high-fidelity 3D myocardial\ninfarct geometry from 2D clinically standard cine MRI, eliminating the need for\ncontrast agents. Specifically, we first reconstruct the 4D biventricular mesh\nfrom multi-view cine MRIs via an automatic deep shape fitting model, biv-me.\nThen, we design a infarction reconstruction model, CMotion2Infarct-Net, to\nexplicitly utilize the motion patterns within this dynamic geometry to localize\ninfarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our\nmethod shows reasonable agreement with manual delineation. This study\ndemonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct\nreconstruction, paving the way for efficient digital twin of MI.",
        "url": "http://arxiv.org/abs/2507.15194v1",
        "published_date": "2025-07-21T02:43:35+00:00",
        "updated_date": "2025-07-21T02:43:35+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Yilin Lyu",
            "Fan Yang",
            "Xiaoyue Liu",
            "Zichen Jiang",
            "Joshua Dillon",
            "Debbie Zhao",
            "Martyn Nash",
            "Charlene Mauger",
            "Alistair Young",
            "Ching-Hui Sia",
            "Mark YY Chan",
            "Lei Li"
        ]
    },
    {
        "title": "A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT",
        "summary": "Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is\nessential for tumor burden estimation, prognosis, and treatment planning. It\nmay also help infer genetic clusters, reducing reliance on expensive testing.\nThis study systematically evaluates anatomical priors to identify\nconfigurations that improve deep learning-based PCC segmentation. We employed\nthe nnU-Net framework to evaluate eleven annotation strategies for accurate 3D\nsegmentation of pheochromocytoma, introducing a set of novel multi-class\nschemes based on organ-specific anatomical priors. These priors were derived\nfrom adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,\nkidney, aorta, adrenal gland, and pancreas), and were compared against a broad\nbody-region prior used in previous work. The framework was trained and tested\non 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.\nPerformance was measured using Dice Similarity Coefficient (DSC), Normalized\nSurface Distance (NSD), and instance-wise F1 score. Among all strategies, the\nTumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation\naccuracy, significantly outperforming the previously used Tumor + Body (TB)\nannotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%\nimprovement at an IoU threshold of 0.5), measured on a 70-30 train-test split.\nThe TKA model also showed superior tumor burden quantification (R^2 = 0.968)\nand strong segmentation across all genetic subtypes. In five-fold\ncross-validation, TKA consistently outperformed TB across IoU thresholds (0.1\nto 0.5), reinforcing its robustness and generalizability. These findings\nhighlight the value of incorporating relevant anatomical context into deep\nlearning models to achieve precise PCC segmentation, offering a valuable tool\nto support clinical assessment and longitudinal disease monitoring in PCC\npatients.",
        "url": "http://arxiv.org/abs/2507.15193v2",
        "published_date": "2025-07-21T02:35:29+00:00",
        "updated_date": "2025-07-24T19:33:50+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Tanjin Taher Toma",
            "Tejas Sudharshan Mathai",
            "Bikash Santra",
            "Pritam Mukherjee",
            "Jianfei Liu",
            "Wesley Jong",
            "Darwish Alabyad",
            "Vivek Batheja",
            "Abhishek Jha",
            "Mayank Patel",
            "Darko Pucar",
            "Jayadira del Rivero",
            "Karel Pacak",
            "Ronald M. Summers"
        ]
    },
    {
        "title": "Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection",
        "summary": "Anemia is a widespread global health issue, particularly among young children\nin low-resource settings. Traditional methods for anemia detection often\nrequire expensive equipment and expert knowledge, creating barriers to early\nand accurate diagnosis. To address these challenges, we explore the use of deep\nlearning models for detecting anemia through conjunctival pallor, focusing on\nthe CP-AnemiC dataset, which includes 710 images from children aged 6-59\nmonths. The dataset is annotated with hemoglobin levels, gender, age and other\ndemographic data, enabling the development of machine learning models for\naccurate anemia detection. We use the MobileNet architecture as a backbone,\nknown for its efficiency in mobile and embedded vision applications, and\nfine-tune our model end-to-end using data augmentation techniques and a\ncross-validation strategy. Our model implementation achieved an accuracy of\n0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong\nperformance on the dataset. To optimize the model for deployment on edge\ndevices, we performed post-training quantization, evaluating the impact of\ndifferent bit-widths (FP32, FP16, INT8, and INT4) on model performance.\nPreliminary results suggest that while FP16 quantization maintains high\naccuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive\nquantization (INT8 and INT4) leads to significant performance degradation.\nOverall, our study supports further exploration of quantization schemes and\nhardware optimizations to assess trade-offs between model size, inference time,\nand diagnostic accuracy in mobile healthcare applications.",
        "url": "http://arxiv.org/abs/2507.15151v1",
        "published_date": "2025-07-20T23:02:58+00:00",
        "updated_date": "2025-07-20T23:02:58+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Sebastian A. Cruz Romero",
            "Wilfredo E. Lugo Beauchamp"
        ]
    },
    {
        "title": "Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection",
        "summary": "Event-based sensors offer high temporal resolution and low latency by\ngenerating sparse, asynchronous data. However, converting this irregular data\ninto dense tensors for use in standard neural networks diminishes these\ninherent advantages, motivating research into graph representations. While such\nmethods preserve sparsity and support asynchronous inference, their performance\non downstream tasks remains limited due to suboptimal modeling of\nspatiotemporal dynamics. In this work, we propose a novel spatiotemporal\nmultigraph representation to better capture spatial structure and temporal\nchanges. Our approach constructs two decoupled graphs: a spatial graph\nleveraging B-spline basis functions to model global structure, and a temporal\ngraph utilizing motion vector-based attention for local dynamic changes. This\ndesign enables the use of efficient 2D kernels in place of computationally\nexpensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM\ndatasets for event-based object detection, achieving over a 6% improvement in\ndetection accuracy compared to previous graph-based works, with a 5x speedup,\nreduced parameter count, and no increase in computational cost. These results\nhighlight the effectiveness of structured graph modeling for asynchronous\nvision. Project page: eventbasedvision.github.io/eGSMV.",
        "url": "http://arxiv.org/abs/2507.15150v1",
        "published_date": "2025-07-20T23:02:23+00:00",
        "updated_date": "2025-07-20T23:02:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aayush Atul Verma",
            "Arpitsinh Vaghela",
            "Bharatesh Chakravarthi",
            "Kaustav Chanda",
            "Yezhou Yang"
        ]
    },
    {
        "title": "Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications",
        "summary": "The design of medical systems for remote, resource-limited environments faces\npersistent challenges due to poor interoperability, lack of offline support,\nand dependency on costly infrastructure. Many existing digital health solutions\nneglect these constraints, limiting their effectiveness for frontline health\nworkers in underserved regions. This paper presents a portable, edge-enabled\nElectronic Health Record platform optimized for offline-first operation, secure\npatient data management, and modular diagnostic integration. Running on\nsmall-form factor embedded devices, it provides AES-256 encrypted local storage\nwith optional cloud synchronization for interoperability. As a use case, we\nintegrated a non-invasive anemia screening module leveraging fingernail pallor\nanalysis. Trained on 250 patient cases (27\\% anemia prevalence) with\nKDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL\nand MAE of 1.490 g/dL. A severity-based model reached 79.2\\% sensitivity. To\noptimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,\nreducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5\nat 0.995. The system emphasizes low-cost deployment, modularity, and data\nprivacy compliance (HIPAA/GDPR), addressing critical barriers to digital health\nadoption in disconnected settings. Our work demonstrates a scalable approach to\nenhance portable health information systems and support frontline healthcare in\nunderserved regions.",
        "url": "http://arxiv.org/abs/2507.15146v1",
        "published_date": "2025-07-20T22:46:42+00:00",
        "updated_date": "2025-07-20T22:46:42+00:00",
        "categories": [
            "cs.ET",
            "cs.AI",
            "cs.CV",
            "cs.CY",
            "cs.LG",
            "cs.SE"
        ],
        "authors": [
            "Sebastian A. Cruz Romero",
            "Misael J. Mercado Hernandez",
            "Samir Y. Ali Rivera",
            "Jorge A. Santiago Fernandez",
            "Wilfredo E. Lugo Beauchamp"
        ]
    },
    {
        "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction",
        "summary": "Visual Planning for Assistance (VPA) aims to predict a sequence of user\nactions required to achieve a specified goal based on a video showing the\nuser's progress. Although recent advances in multimodal large language models\n(MLLMs) have shown promising results in video understanding, long-horizon\nvisual planning remains a challenging problem. We identify two challenges in\ntraining large MLLMs for video-based planning tasks: (1) scarcity of procedural\nannotations, limiting the model's ability to learn procedural task dynamics\neffectively, and (2) inefficiency of next-token prediction objective to\nexplicitly capture the structured action space for visual planning when\ncompared to free-form, natural language. To tackle data scarcity, we introduce\nAuxiliary Task Augmentation. We design and train our model on auxiliary tasks\nrelevant to long-horizon video-based planning (e.g., goal prediction) to\naugment the model's planning ability. To more explicitly model the structured\naction space unique to visual planning tasks, we leverage Multi-token\nPrediction, extending traditional next-token prediction by using multiple heads\nto predict multiple future tokens during training. Our approach, VideoPlan,\nachieves state-of-the-art VPA performance on the COIN and CrossTask datasets,\nsurpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3\nfuture actions. We further extend our method to the challenging Ego4D Long-term\nAction Anticipation task, and show that it is on par with the state-of-the-art\napproaches despite not using specialized egocentric features. Code will be made\navailable.",
        "url": "http://arxiv.org/abs/2507.15130v1",
        "published_date": "2025-07-20T21:39:05+00:00",
        "updated_date": "2025-07-20T21:39:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ce Zhang",
            "Yale Song",
            "Ruta Desai",
            "Michael Louis Iuzzolino",
            "Joseph Tighe",
            "Gedas Bertasius",
            "Satwik Kottur"
        ]
    },
    {
        "title": "LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM",
        "summary": "One of the main challenges in the Simultaneous Localization and Mapping\n(SLAM) loop closure problem is the recognition of previously visited places. In\nthis work, we tackle the two main problems of real-time SLAM systems: 1) loop\nclosure detection accuracy and 2) real-time computation constraints on the\nembedded hardware. Our LoopNet method is based on a multitasking variant of the\nclassical ResNet architecture, adapted for online retraining on a dynamic\nvisual dataset and optimized for embedded devices. The online retraining is\ndesigned using a few-shot learning approach. The architecture provides both an\nindex into the queried visual dataset, and a measurement of the prediction\nquality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,\nLoopNet surpasses the limitations of handcrafted features and traditional deep\nlearning methods, offering better performance under varying conditions. Code is\navailable at https://github.com/RovisLab/LoopNet. Additinally, we introduce a\nnew loop closure benchmarking dataset, coined LoopDB, which is available at\nhttps://github.com/RovisLab/LoopDB.",
        "url": "http://arxiv.org/abs/2507.15109v1",
        "published_date": "2025-07-20T20:11:37+00:00",
        "updated_date": "2025-07-20T20:11:37+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Mohammad-Maher Nakshbandi",
            "Ziad Sharawy",
            "Sorin Grigorescu"
        ]
    },
    {
        "title": "BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking",
        "summary": "Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses\nsignificant risks, demanding precise, real-time localization and continuous\nmonitoring of the bleeding source for effective hemostatic intervention. In\nparticular, endoscopists have to repeatedly flush to clear blood, allowing only\nmilliseconds to identify bleeding sources, an inefficient process that prolongs\noperations and elevates patient risks. However, current Artificial Intelligence\n(AI) methods primarily focus on bleeding region segmentation, overlooking the\ncritical need for accurate bleeding source detection and temporal tracking in\nthe challenging ESD environment, which is marked by frequent visual\nobstructions and dynamic scene changes. This gap is widened by the lack of\nspecialized datasets, hindering the development of robust AI-assisted guidance\nsystems. To address these challenges, we introduce BleedOrigin-Bench, the first\ncomprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated\nbleeding sources across 106,222 frames from 44 procedures, supplemented with\n39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6\nchallenging clinical scenarios. We also present BleedOrigin-Net, a novel\ndual-stage detection-tracking framework for the bleeding source localization in\nESD procedures, addressing the complete workflow from bleeding onset detection\nto continuous spatial tracking. We compare with widely-used object detection\nmodels (YOLOv11/v12), multimodal large language models, and point tracking\nmethods. Extensive evaluation demonstrates state-of-the-art performance,\nachieving 96.85% frame-level accuracy ($\\pm\\leq8$ frames) for bleeding onset\ndetection, 70.24% pixel-level accuracy ($\\leq100$ px) for initial source\ndetection, and 96.11% pixel-level accuracy ($\\leq100$ px) for point tracking.",
        "url": "http://arxiv.org/abs/2507.15094v1",
        "published_date": "2025-07-20T19:19:42+00:00",
        "updated_date": "2025-07-20T19:19:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mengya Xu",
            "Rulin Zhou",
            "An Wang",
            "Chaoyang Lyu",
            "Zhen Li",
            "Ning Zhong",
            "Hongliang Ren"
        ]
    },
    {
        "title": "Visual Place Recognition for Large-Scale UAV Applications",
        "summary": "Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial\nVehicle (UAV) navigation, enabling robust localization across diverse\nenvironments. Despite significant advancements, aerial vPR faces unique\nchallenges due to the limited availability of large-scale, high-altitude\ndatasets, which limits model generalization, along with the inherent rotational\nambiguity in UAV imagery. To address these challenges, we introduce LASED, a\nlarge-scale aerial dataset with approximately one million images,\nsystematically sampled from 170,000 unique locations throughout Estonia over a\ndecade, offering extensive geographic and temporal diversity. Its structured\ndesign ensures clear place separation significantly enhancing model training\nfor aerial scenarios. Furthermore, we propose the integration of steerable\nConvolutional Neural Networks (CNNs) to explicitly handle rotational variance,\nleveraging their inherent rotational equivariance to produce robust,\norientation-invariant feature representations. Our extensive benchmarking\ndemonstrates that models trained on LASED achieve significantly higher recall\ncompared to those trained on smaller, less diverse datasets, highlighting the\nbenefits of extensive geographic coverage and temporal diversity. Moreover,\nsteerable CNNs effectively address rotational ambiguity inherent in aerial\nimagery, consistently outperforming conventional convolutional architectures,\nachieving on average 12\\% recall improvement over the best-performing\nnon-steerable network. By combining structured, large-scale datasets with\nrotation-equivariant neural networks, our approach significantly enhances model\nrobustness and generalization for aerial vPR.",
        "url": "http://arxiv.org/abs/2507.15089v1",
        "published_date": "2025-07-20T19:02:15+00:00",
        "updated_date": "2025-07-20T19:02:15+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ioannis Tsampikos Papapetros",
            "Ioannis Kansizoglou",
            "Antonios Gasteratos"
        ]
    },
    {
        "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR",
        "summary": "Text image is a unique and crucial information medium that integrates visual\naesthetics and linguistic semantics in modern e-society. Due to their subtlety\nand complexity, the generation of text images represents a challenging and\nevolving frontier in the image generation field. The recent surge of\nspecialized image generators (\\emph{e.g.}, Flux-series) and unified generative\nmodels (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a\nnatural question: can they master the intricacies of text image generation and\nediting? Motivated by this, we assess current state-of-the-art generative\nmodels' capabilities in terms of text image generation and editing. We\nincorporate various typical optical character recognition (OCR) tasks into our\nevaluation and broaden the concept of text-based generation tasks into OCR\ngenerative tasks. We select 33 representative tasks and categorize them into\nfive categories: document, handwritten text, scene text, artistic text, and\ncomplex \\& layout-rich text. For comprehensive evaluation, we examine six\nmodels across both closed-source and open-source domains, using tailored,\nhigh-quality image inputs and prompts. Through this evaluation, we draw crucial\nobservations and identify the weaknesses of current generative models for OCR\ntasks. We argue that photorealistic text image generation and editing should be\ninternalized as foundational skills into general-domain generative models,\nrather than being delegated to specialized solutions, and we hope this\nempirical analysis can provide valuable insights for the community to achieve\nthis goal. This evaluation is online and will be continuously updated at our\nGitHub repository.",
        "url": "http://arxiv.org/abs/2507.15085v1",
        "published_date": "2025-07-20T18:43:09+00:00",
        "updated_date": "2025-07-20T18:43:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peirong Zhang",
            "Haowei Xu",
            "Jiaxin Zhang",
            "Guitao Xu",
            "Xuhan Zheng",
            "Zhenhua Yang",
            "Junle Liu",
            "Yuyi Zhang",
            "Lianwen Jin"
        ]
    },
    {
        "title": "PET Image Reconstruction Using Deep Diffusion Image Prior",
        "summary": "Diffusion models have shown great promise in medical image denoising and\nreconstruction, but their application to Positron Emission Tomography (PET)\nimaging remains limited by tracer-specific contrast variability and high\ncomputational demands. In this work, we proposed an anatomical prior-guided PET\nimage reconstruction method based on diffusion models, inspired by the deep\ndiffusion image prior (DDIP) framework. The proposed method alternated between\ndiffusion sampling and model fine-tuning guided by the PET sinogram, enabling\nthe reconstruction of high-quality images from various PET tracers using a\nscore function pretrained on a dataset of another tracer. To improve\ncomputational efficiency, the half-quadratic splitting (HQS) algorithm was\nadopted to decouple network optimization from iterative PET reconstruction. The\nproposed method was evaluated using one simulation and two clinical datasets.\nFor the simulation study, a model pretrained on [$^{18}$F]FDG data was tested\non amyloid-negative PET data to assess out-of-distribution (OOD) performance.\nFor the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one\n[$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from\nanother tracer. Experiment results show that the proposed PET reconstruction\nmethod can generalize robustly across tracer distributions and scanner types,\nproviding an efficient and versatile reconstruction framework for low-dose PET\nimaging.",
        "url": "http://arxiv.org/abs/2507.15078v1",
        "published_date": "2025-07-20T18:25:29+00:00",
        "updated_date": "2025-07-20T18:25:29+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "physics.med-ph"
        ],
        "authors": [
            "Fumio Hashimoto",
            "Kuang Gong"
        ]
    }
]