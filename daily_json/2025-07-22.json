[
    {
        "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context",
        "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .",
        "url": "http://arxiv.org/abs/2507.15807v1",
        "published_date": "2025-07-21T17:08:18+00:00",
        "updated_date": "2025-07-21T17:08:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shuo Chen",
            "Jianzhe Liu",
            "Zhen Han",
            "Yan Xia",
            "Daniel Cremers",
            "Philip Torr",
            "Volker Tresp",
            "Jindong Gu"
        ],
        "tldr": "This paper introduces Dynamic Attention Reallocation (DARA) to improve visual context utilization in Multimodal In-Context Learning (MICL) and proposes TrueMICL, a dataset designed to evaluate genuine multimodal understanding. They demonstrate improved performance by forcing the model to attend to the visual context.",
        "tldr_zh": "本文提出了一种动态注意力重新分配（DARA）方法，以提高多模态上下文学习（MICL）中视觉上下文的利用率，并提出了 TrueMICL 数据集，旨在评估真正的多模态理解。他们通过强制模型关注视觉上下文，展示了性能的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have made significant strides by\ncombining visual recognition and language understanding to generate content\nthat is both coherent and contextually accurate. However, MLLMs continue to\nstruggle with object hallucinations, where models produce seemingly plausible\nbut factually incorrect outputs, including objects that do not exist in the\nimage. Recent work has revealed that the prior knowledge in MLLMs significantly\nsuppresses visual information in deep layers, causing hallucinatory outputs.\nHowever, how these priors suppress visual information at the intermediate layer\nstage in MLLMs remains unclear. We observe that visual factual knowledge and\nthe differences between intermediate-layer prior/original probability\ndistributions show similar evolutionary trends in intermediate layers.\nMotivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a\nsimple, training-free method that dynamically selects intermediate layers with\nthe most significant visual factual information. By contrasting the output\ndistributions of the selected layer derived from the original input and\npure-text input, EVA extracts visual factual knowledge and proportionally\nincorporates it into the final layer to correct the output logits. Importantly,\nEVA is model-agnostic, seamlessly integrates with various classic decoding\nstrategies, and is applicable across different MLLMs. We validate EVA on\nwidely-used benchmarks, and the results show that it significantly reduces\nhallucination rates compared to baseline methods, underscoring its\neffectiveness in mitigating hallucinations.",
        "url": "http://arxiv.org/abs/2507.15652v1",
        "published_date": "2025-07-21T14:15:34+00:00",
        "updated_date": "2025-07-21T14:15:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Zhou",
            "Zihan Zhang",
            "Hao Chen"
        ],
        "tldr": "This paper introduces EVA, a training-free method that dynamically selects intermediate layers in MLLMs to extract visual facts and mitigate object hallucinations by correcting output logits.",
        "tldr_zh": "该论文介绍了一种名为EVA的免训练方法，通过动态选择多模态大语言模型中的中间层来提取视觉事实，并通过校正输出logits来减轻对象幻觉。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SIA: Enhancing Safety via Intent Awareness for Vision-Language Models",
        "summary": "As vision-language models (VLMs) are increasingly deployed in real-world\napplications, new safety risks arise from the subtle interplay between images\nand text. In particular, seemingly innocuous inputs can combine to reveal\nharmful intent, leading to unsafe model responses. Despite increasing attention\nto multimodal safety, previous approaches based on post hoc filtering or static\nrefusal prompts struggle to detect such latent risks, especially when\nharmfulness emerges only from the combination of inputs. We propose SIA (Safety\nvia Intent Awareness), a training-free prompt engineering framework that\nproactively detects and mitigates harmful intent in multimodal inputs. SIA\nemploys a three-stage reasoning process: (1) visual abstraction via captioning,\n(2) intent inference through few-shot chain-of-thought prompting, and (3)\nintent-conditioned response refinement. Rather than relying on predefined rules\nor classifiers, SIA dynamically adapts to the implicit intent inferred from the\nimage-text pair. Through extensive experiments on safety-critical benchmarks\nincluding SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves\nsubstantial safety improvements, outperforming prior methods. Although SIA\nshows a minor reduction in general reasoning accuracy on MMStar, the\ncorresponding safety gains highlight the value of intent-aware reasoning in\naligning VLMs with human-centric values.",
        "url": "http://arxiv.org/abs/2507.16856v1",
        "published_date": "2025-07-21T13:59:50+00:00",
        "updated_date": "2025-07-21T13:59:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Youngjin Na",
            "Sangheon Jeong",
            "Youngwan Lee"
        ],
        "tldr": "This paper introduces SIA, a training-free prompt engineering framework for Vision-Language Models that proactively detects and mitigates harmful intent by reasoning about visual content and text, leading to safer model responses.",
        "tldr_zh": "本文介绍了一种名为SIA的免训练提示工程框架，用于视觉-语言模型，通过推理视觉内容和文本，主动检测并减轻有害意图，从而实现更安全的模型响应。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos",
        "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.",
        "url": "http://arxiv.org/abs/2507.15597v1",
        "published_date": "2025-07-21T13:19:09+00:00",
        "updated_date": "2025-07-21T13:19:09+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Hao Luo",
            "Yicheng Feng",
            "Wanpeng Zhang",
            "Sipeng Zheng",
            "Ye Wang",
            "Haoqi Yuan",
            "Jiazheng Liu",
            "Chaoyi Xu",
            "Qin Jin",
            "Zongqing Lu"
        ],
        "tldr": "The paper introduces Being-H0, a Vision-Language-Action model pretrained on large-scale human videos using a novel physical instruction tuning paradigm for improved dexterity and generalization in robotic manipulation, addressing limitations of existing VLAs relying on synthetic or teleoperated data.",
        "tldr_zh": "该论文介绍了Being-H0，一个基于大规模人类视频预训练的视觉-语言-动作模型，使用了一种新的物理指令调优范式，以提高机器人操作的灵活性和泛化能力，解决了现有依赖合成或遥操作数据的视觉语言模型存在的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging",
        "summary": "Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}.",
        "url": "http://arxiv.org/abs/2507.15576v1",
        "published_date": "2025-07-21T12:57:49+00:00",
        "updated_date": "2025-07-21T12:57:49+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Nicolas Poggi",
            "Shashank Agnihotri",
            "Margret Keuper"
        ],
        "tldr": "This paper explores using Vision-Language Models (VLMs) with In-Context Learning (ICL) for THz image classification, showing improved performance in low-data scenarios without fine-tuning.",
        "tldr_zh": "本文探索了使用视觉语言模型（VLMs）和上下文学习（ICL）进行太赫兹（THz）图像分类，展示了在低数据场景下无需微调即可提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding",
        "summary": "In recent years, the introduction of Multi-modal Large Language Models\n(MLLMs) into video understanding tasks has become increasingly prevalent.\nHowever, how to effectively integrate temporal information remains a critical\nresearch focus. Traditional approaches treat spatial and temporal information\nseparately. Due to issues like motion blur, it is challenging to accurately\nrepresent the spatial information of rapidly moving objects. This can lead to\ntemporally important regions being underemphasized during spatial feature\nextraction, which in turn hinders accurate spatio-temporal interaction and\nvideo understanding. To address this limitation, we propose an innovative video\nrepresentation method called Dynamic-Image (DynImg). Specifically, we introduce\na set of non-key frames as temporal prompts to highlight the spatial areas\ncontaining fast-moving objects. During the process of visual feature\nextraction, these prompts guide the model to pay additional attention to the\nfine-grained spatial features corresponding to these regions. Moreover, to\nmaintain the correct sequence for DynImg, we employ a corresponding 4D video\nRotary Position Embedding. This retains both the temporal and spatial adjacency\nof DynImg, helping MLLM understand the spatio-temporal order within this\ncombined format. Experimental evaluations reveal that DynImg surpasses the\nstate-of-the-art methods by approximately 2% across multiple video\nunderstanding benchmarks, proving the effectiveness of our temporal prompts in\nenhancing video comprehension.",
        "url": "http://arxiv.org/abs/2507.15569v1",
        "published_date": "2025-07-21T12:50:49+00:00",
        "updated_date": "2025-07-21T12:50:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyi Bao",
            "Chenwei Xie",
            "Hao Tang",
            "Tingyu Weng",
            "Xiaofeng Wang",
            "Yun Zheng",
            "Xingang Wang"
        ],
        "tldr": "The paper introduces DynImg, a video representation method using non-key frames as temporal prompts to highlight fast-moving objects for improved spatio-temporal interaction and video understanding in MLLMs, achieving a 2% improvement over SOTA methods.",
        "tldr_zh": "该论文介绍了DynImg，一种视频表示方法，它使用非关键帧作为时间提示来突出快速移动的物体，从而改进MLLM中的时空交互和视频理解，并在SOTA方法上提高了2%。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation",
        "summary": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.",
        "url": "http://arxiv.org/abs/2507.15542v1",
        "published_date": "2025-07-21T12:15:27+00:00",
        "updated_date": "2025-07-21T12:15:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinqian Lei",
            "Bo Wang",
            "Robby T. Tan"
        ],
        "tldr": "The paper introduces HOLa, a novel zero-shot HOI detection method using low-rank decomposed VLM features and LLM-guided regularization to improve generalization and action distinction, achieving state-of-the-art results on HICO-DET.",
        "tldr_zh": "该论文介绍了HOLa，一种新颖的零样本HOI检测方法，它使用低秩分解的VLM特征和LLM引导的正则化来提高泛化能力和动作区分能力，并在HICO-DET上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner",
        "summary": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).",
        "url": "http://arxiv.org/abs/2507.15509v1",
        "published_date": "2025-07-21T11:22:17+00:00",
        "updated_date": "2025-07-21T11:22:17+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Lei Chen",
            "Xuanle Zhao",
            "Zhixiong Zeng",
            "Jing Huang",
            "Yufeng Zhong",
            "Lin Ma"
        ],
        "tldr": "The paper introduces Chart-R1, a vision-language model for chart reasoning, using a novel data synthesis technique and a two-stage training approach (Chart-COT and Chart-RFT) to achieve state-of-the-art performance.",
        "tldr_zh": "该论文介绍了Chart-R1，一个用于图表推理的视觉语言模型，它使用了一种新颖的数据合成技术和两阶段训练方法（Chart-COT 和 Chart-RFT），从而实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GR-3 Technical Report",
        "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.",
        "url": "http://arxiv.org/abs/2507.15493v2",
        "published_date": "2025-07-21T10:54:13+00:00",
        "updated_date": "2025-07-22T15:04:37+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chilam Cheang",
            "Sijin Chen",
            "Zhongren Cui",
            "Yingdong Hu",
            "Liqun Huang",
            "Tao Kong",
            "Hang Li",
            "Yifeng Li",
            "Yuxiao Liu",
            "Xiao Ma",
            "Hao Niu",
            "Wenxuan Ou",
            "Wanli Peng",
            "Zeyu Ren",
            "Haixin Shi",
            "Jiawen Tian",
            "Hongtao Wu",
            "Xin Xiao",
            "Yuyang Xiao",
            "Jiafeng Xu",
            "Yichu Yang"
        ],
        "tldr": "The paper introduces GR-3, a large-scale vision-language-action model for generalist robotics, demonstrating strong generalization, efficient fine-tuning, and adeptness at long-horizon, dexterous tasks, validated with a new bi-manual mobile robot, ByteMini.",
        "tldr_zh": "该论文介绍了 GR-3，一种用于通用机器人技术的大规模视觉-语言-动作模型，展示了强大的泛化能力、高效的微调能力以及对长程、灵巧任务的掌握能力，并通过一种新型的双手动移动机器人 ByteMini 进行了验证。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "One Last Attention for Your Vision-Language Model",
        "summary": "Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable\nzero-shot performance, yet their downstream potential hinges on effective\nfine-tuning. Most adaptation methods typically focus on refining representation\nfrom separate modalities (text or vision) but neglect the critical role of\ntheir fused representations in the decision-making process, \\emph{\\ie} rational\nmatrix that drives the final prediction. To bridge the gap, we propose a simple\nyet effective \\textbf{R}ational \\textbf{Ada}ptaion ({RAda}) to explicitly\nexploit the final fused representation during fine-tuning. RAda employs a\nlearned mask, obtained from a lightweight attention layer attached at the end\nof a VLM, to dynamically calibrate the contribution of each element in the\nrational matrix, enabling targeted adjustments to the final cross-modal\ninteractions without incurring costly modifications to intermediate features.\nExperiments in different settings (i.e., updating, or freezing pretrained\nencoders in adaptation, and test-time training that can only access the\nunlabeled test data) show that RAda serves as a versatile fine-tuning\ntechnique, improving the baseline with minimal code and performing comparably\nagainst current arts in most settings. Code is available at\n\\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.",
        "url": "http://arxiv.org/abs/2507.15480v2",
        "published_date": "2025-07-21T10:35:32+00:00",
        "updated_date": "2025-07-28T04:47:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liang Chen",
            "Ghazi Shazan Ahmad",
            "Tianjun Yao",
            "Lingqiao Liu",
            "Zhiqiang Shen"
        ],
        "tldr": "The paper introduces RAda, a lightweight attention-based fine-tuning method for Vision-Language Models that focuses on adapting the fused representations, improving performance with minimal code changes.",
        "tldr_zh": "该论文介绍了一种名为RAda的轻量级基于注意力机制的视觉-语言模型微调方法，该方法专注于调整融合表示，以最小的代码更改来提高性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems",
        "summary": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection.",
        "url": "http://arxiv.org/abs/2507.15285v1",
        "published_date": "2025-07-21T06:35:46+00:00",
        "updated_date": "2025-07-21T06:35:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lazaro Janier Gonzalez-Soler",
            "Maciej Salwowski",
            "Christoph Busch"
        ],
        "tldr": "This paper explores using in-context learning with Vision Language Models (VLMs) to detect physical and digital attacks on face recognition systems, demonstrating competitive performance compared to traditional CNNs without extensive training.",
        "tldr_zh": "该论文探索了使用视觉语言模型（VLM）的上下文学习来检测人脸识别系统中的物理和数字攻击，并展示了与传统卷积神经网络相比具有竞争力的性能，而无需进行大量的训练。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction",
        "summary": "Visual Planning for Assistance (VPA) aims to predict a sequence of user\nactions required to achieve a specified goal based on a video showing the\nuser's progress. Although recent advances in multimodal large language models\n(MLLMs) have shown promising results in video understanding, long-horizon\nvisual planning remains a challenging problem. We identify two challenges in\ntraining large MLLMs for video-based planning tasks: (1) scarcity of procedural\nannotations, limiting the model's ability to learn procedural task dynamics\neffectively, and (2) inefficiency of next-token prediction objective to\nexplicitly capture the structured action space for visual planning when\ncompared to free-form, natural language. To tackle data scarcity, we introduce\nAuxiliary Task Augmentation. We design and train our model on auxiliary tasks\nrelevant to long-horizon video-based planning (e.g., goal prediction) to\naugment the model's planning ability. To more explicitly model the structured\naction space unique to visual planning tasks, we leverage Multi-token\nPrediction, extending traditional next-token prediction by using multiple heads\nto predict multiple future tokens during training. Our approach, VideoPlan,\nachieves state-of-the-art VPA performance on the COIN and CrossTask datasets,\nsurpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3\nfuture actions. We further extend our method to the challenging Ego4D Long-term\nAction Anticipation task, and show that it is on par with the state-of-the-art\napproaches despite not using specialized egocentric features. Code will be made\navailable.",
        "url": "http://arxiv.org/abs/2507.15130v1",
        "published_date": "2025-07-20T21:39:05+00:00",
        "updated_date": "2025-07-20T21:39:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ce Zhang",
            "Yale Song",
            "Ruta Desai",
            "Michael Louis Iuzzolino",
            "Joseph Tighe",
            "Gedas Bertasius",
            "Satwik Kottur"
        ],
        "tldr": "The paper introduces VideoPlan, an approach that uses auxiliary task augmentation and multi-token prediction to improve visual planning performance on video-based action anticipation tasks, achieving state-of-the-art results on COIN and CrossTask datasets.",
        "tldr_zh": "本文介绍了VideoPlan，它采用辅助任务增强和多token预测来提高基于视频的动作预测任务中的视觉规划性能，并在COIN和CrossTask数据集上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction",
        "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.",
        "url": "http://arxiv.org/abs/2507.15852v2",
        "published_date": "2025-07-21T17:59:02+00:00",
        "updated_date": "2025-07-22T10:51:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhixiong Zhang",
            "Shuangrui Ding",
            "Xiaoyi Dong",
            "Songxin He",
            "Jianfan Lin",
            "Junsong Tang",
            "Yuhang Zang",
            "Yuhang Cao",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "tldr": "The paper introduces SeC, a concept-driven VOS framework leveraging LVLMs to improve segmentation robustness in complex video scenarios, and proposes a new benchmark, SeCVOS, to evaluate such capabilities.",
        "tldr_zh": "该论文介绍了SeC，一种概念驱动的视频对象分割框架，利用大型视觉语言模型来提高复杂视频场景中分割的鲁棒性，并提出了一个新的基准SeCVOS来评估这种能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment",
        "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.",
        "url": "http://arxiv.org/abs/2507.15680v3",
        "published_date": "2025-07-21T14:44:46+00:00",
        "updated_date": "2025-07-23T08:20:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongkang Hou",
            "Jiarun Song"
        ],
        "tldr": "This paper proposes a knowledge distillation method to transfer IQA capabilities from a fine-tuned CLIP model to a smaller student model, achieving better performance and reduced complexity.",
        "tldr_zh": "本文提出了一种知识蒸馏方法，将微调后的CLIP模型的图像质量评估能力迁移到更小的学生模型，从而实现更好的性能和更低的复杂度。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark",
        "summary": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.",
        "url": "http://arxiv.org/abs/2507.15655v1",
        "published_date": "2025-07-21T14:16:44+00:00",
        "updated_date": "2025-07-21T14:16:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aniket Pal",
            "Ajoy Mondal",
            "Minesh Mathew",
            "C. V. Jawahar"
        ],
        "tldr": "The paper introduces HW-MLVQA, a new multilingual VQA benchmark specifically designed for handwritten documents, addressing the limitations of existing MLVQA models in this domain.",
        "tldr_zh": "本文介绍了 HW-MLVQA，这是一个新的多语言 VQA 基准，专门为手写文档设计，旨在解决现有 MLVQA 模型在该领域的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval",
        "summary": "Enabling efficient text-video retrieval on edge-end devices is critical for\nreal-world applications. Yet, existing methods face a critical challenge in\nbalancing accuracy and computational efficiency: uniform frame sampling methods\nensure content coverage but incur prohibitive computational costs, while\nsalient-frame sampling methods reduce overhead but suffer from query-agnostic\nframe selection that biases retrieval results. To address this, we propose\nProCLIP, a user-centric framework that achieves state-of-the-art accuracy with\nsignificantly improved efficiency. We design a prompt-aware frame sampling\nstrategy that dynamically guides lightweight feature extractors using textual\nprompts to select semantically relevant frames, overcoming the limitations of\nexisting salient-frame sampling methods which rely on static, query-agnostic\nselection criteria. Moreover, we adopt a two-stage candidate pruning strategy\nthat combines rapid coarse filtering via a lightweight module with CLIP-powered\nfine-grained re-ranking, enhancing retrieval efficiency while preserving\naccuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency\nreduction versus baselines while maintaining competitive accuracy, i.e.,\nR@1=49.0 in MSR-VTT dataset. Code is available at\nhttps://github.com/tiffylong/ProCLIP.",
        "url": "http://arxiv.org/abs/2507.15491v1",
        "published_date": "2025-07-21T10:46:49+00:00",
        "updated_date": "2025-07-21T10:46:49+00:00",
        "categories": [
            "cs.MM",
            "cs.CV"
        ],
        "authors": [
            "Deyu Zhang",
            "Tingting Long",
            "Jinrui Zhang",
            "Ligeng Chen",
            "Ju Ren",
            "Yaoxue Zhang"
        ],
        "tldr": "The paper introduces ProCLIP, a prompt-aware frame sampling method for efficient text-video retrieval that dynamically selects semantically relevant frames using textual prompts and a two-stage candidate pruning strategy.",
        "tldr_zh": "该论文介绍了 ProCLIP，一种提示感知帧采样方法，用于高效的文本-视频检索。该方法使用文本提示动态选择语义相关的帧，并采用两阶段候选剪枝策略。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent",
        "summary": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning.",
        "url": "http://arxiv.org/abs/2507.15428v1",
        "published_date": "2025-07-21T09:27:45+00:00",
        "updated_date": "2025-07-21T09:27:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiaao Li",
            "Kaiyuan Li",
            "Chen Gao",
            "Yong Li",
            "Xinlei Chen"
        ],
        "tldr": "The paper introduces EgoPrune, a training-free token pruning method tailored for egomotion video reasoning, improving efficiency and reducing computational cost for embodied AI agents using egomotion videos.",
        "tldr_zh": "该论文介绍了一种名为EgoPrune的免训练令牌剪枝方法，专门用于自运动视频推理，通过减少计算成本来提高使用自运动视频的具身人工智能代理的效率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]