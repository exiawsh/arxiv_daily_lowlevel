[
    {
        "title": "LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content",
        "summary": "The increasing complexity of content rendering in modern games has led to a\nproblematic growth in the workload of the GPU. In this paper, we propose an\nAI-based low-complexity scaler (LCS) inspired by state-of-the-art efficient\nsuper-resolution (ESR) models which could offload the workload on the GPU to a\nlow-power device such as a neural processing unit (NPU). The LCS is trained on\nGameIR image pairs natively rendered at low and high resolution. We utilize\nadversarial training to encourage reconstruction of perceptually important\ndetails, and apply reparameterization and quantization techniques to reduce\nmodel complexity and size. In our comparative analysis we evaluate the LCS\nalongside the publicly available AMD hardware-based Edge Adaptive Scaling\nFunction (EASF) and AMD FidelityFX Super Resolution 1 (FSR1) on five different\nmetrics, and find that the LCS achieves better perceptual quality,\ndemonstrating the potential of ESR models for upscaling on resource-constrained\ndevices.",
        "url": "http://arxiv.org/abs/2507.22873v1",
        "published_date": "2025-07-30T17:47:25+00:00",
        "updated_date": "2025-07-30T17:47:25+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Simon Pochinda",
            "Momen K. Tageldeen",
            "Mark Thompson",
            "Tony Rinaldi",
            "Troy Giorshev",
            "Keith Lee",
            "Jie Zhou",
            "Frederick Walls"
        ],
        "tldr": "This paper proposes an AI-based low-complexity super-resolution scaler (LCS) for games, designed to offload GPU workload to a low-power NPU, achieving better perceptual quality than AMD's EASF and FSR1.",
        "tldr_zh": "该论文提出了一种基于人工智能的低复杂度超分辨率缩放器 (LCS)，专为游戏设计，旨在将 GPU 工作负载转移到低功耗 NPU 上，并实现了比 AMD 的 EASF 和 FSR1 更好的感知质量。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing",
        "summary": "Fashion design is a complex creative process that blends visual and textual\nexpressions. Designers convey ideas through sketches, which define spatial\nstructure and design elements, and textual descriptions, capturing material,\ntexture, and stylistic details. In this paper, we present LOcalized Text and\nSketch for fashion image generation (LOTS), an approach for compositional\nsketch-text based generation of complete fashion outlooks. LOTS leverages a\nglobal description with paired localized sketch + text information for\nconditioning and introduces a novel step-based merging strategy for diffusion\nadaptation. First, a Modularized Pair-Centric representation encodes sketches\nand text into a shared latent space while preserving independent localized\nfeatures; then, a Diffusion Pair Guidance phase integrates both local and\nglobal conditioning via attention-based guidance within the diffusion model's\nmulti-step denoising process. To validate our method, we build on Fashionpedia\nto release Sketchy, the first fashion dataset where multiple text-sketch pairs\nare provided per image. Quantitative results show LOTS achieves\nstate-of-the-art image generation performance on both global and localized\nmetrics, while qualitative examples and a human evaluation study highlight its\nunprecedented level of design customization.",
        "url": "http://arxiv.org/abs/2507.22627v1",
        "published_date": "2025-07-30T12:48:29+00:00",
        "updated_date": "2025-07-30T12:48:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Federico Girella",
            "Davide Talon",
            "Ziyue Liu",
            "Zanxi Ruan",
            "Yiming Wang",
            "Marco Cristani"
        ],
        "tldr": "The paper introduces LOTS, a novel approach for fashion image generation using paired sketches and text, achieving state-of-the-art results on a new dataset, Sketchy.",
        "tldr_zh": "该论文介绍了LOTS，一种使用配对草图和文本的时尚图像生成新方法，并在一个名为Sketchy的新数据集上实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Adverse Weather Removal via Spectral-based Spatial Grouping",
        "summary": "Adverse weather conditions cause diverse and complex degradation patterns,\ndriving the development of All-in-One (AiO) models. However, recent AiO\nsolutions still struggle to capture diverse degradations, since global\nfiltering methods like direct operations on the frequency domain fail to handle\nhighly variable and localized distortions. To address these issue, we propose\nSpectral-based Spatial Grouping Transformer (SSGformer), a novel approach that\nleverages spectral decomposition and group-wise attention for multi-weather\nimage restoration. SSGformer decomposes images into high-frequency edge\nfeatures using conventional edge detection and low-frequency information via\nSingular Value Decomposition. We utilize multi-head linear attention to\neffectively model the relationship between these features. The fused features\nare integrated with the input to generate a grouping-mask that clusters regions\nbased on the spatial similarity and image texture. To fully leverage this mask,\nwe introduce a group-wise attention mechanism, enabling robust adverse weather\nremoval and ensuring consistent performance across diverse weather conditions.\nWe also propose a Spatial Grouping Transformer Block that uses both channel\nattention and spatial attention, effectively balancing feature-wise\nrelationships and spatial dependencies. Extensive experiments show the\nsuperiority of our approach, validating its effectiveness in handling the\nvaried and intricate adverse weather degradations.",
        "url": "http://arxiv.org/abs/2507.22498v2",
        "published_date": "2025-07-30T09:08:34+00:00",
        "updated_date": "2025-07-31T10:38:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuhwan Jeong",
            "Yunseo Yang",
            "Youngho Yoon",
            "Kuk-Jin Yoon"
        ],
        "tldr": "The paper introduces SSGformer, a novel transformer-based architecture for robust adverse weather removal, leveraging spectral decomposition and group-wise attention to handle diverse and localized distortions in degraded images.",
        "tldr_zh": "本文提出了一种名为SSGformer的新型基于Transformer的架构，用于稳健的恶劣天气去除，利用频谱分解和分组注意力来处理降级图像中多样化和局部化的失真。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Exploiting Diffusion Prior for Task-driven Image Restoration",
        "summary": "Task-driven image restoration (TDIR) has recently emerged to address\nperformance drops in high-level vision tasks caused by low-quality (LQ) inputs.\nPrevious TDIR methods struggle to handle practical scenarios in which images\nare degraded by multiple complex factors, leaving minimal clues for\nrestoration. This motivates us to leverage the diffusion prior, one of the most\npowerful natural image priors. However, while the diffusion prior can help\ngenerate visually plausible results, using it to restore task-relevant details\nremains challenging, even when combined with recent TDIR methods. To address\nthis, we propose EDTR, which effectively harnesses the power of diffusion prior\nto restore task-relevant details. Specifically, we propose directly leveraging\nuseful clues from LQ images in the diffusion process by generating from\npixel-error-based pre-restored LQ images with mild noise added. Moreover, we\nemploy a small number of denoising steps to prevent the generation of redundant\ndetails that dilute crucial task-related information. We demonstrate that our\nmethod effectively utilizes diffusion prior for TDIR, significantly enhancing\ntask performance and visual quality across diverse tasks with multiple complex\ndegradations.",
        "url": "http://arxiv.org/abs/2507.22459v1",
        "published_date": "2025-07-30T08:05:49+00:00",
        "updated_date": "2025-07-30T08:05:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jaeha Kim",
            "Junghun Oh",
            "Kyoung Mu Lee"
        ],
        "tldr": "This paper introduces EDTR, a novel approach that leverages diffusion priors for task-driven image restoration (TDIR), effectively restoring task-relevant details even with complex degradations by incorporating clues from low-quality images and limiting denoising steps.",
        "tldr_zh": "本文介绍了一种名为EDTR的新方法，该方法利用扩散先验进行任务驱动的图像恢复（TDIR），通过结合低质量图像中的线索并限制去噪步骤，有效地恢复即使在复杂退化情况下的任务相关细节。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Subtyping Breast Lesions via Generative Augmentation based Long-tailed Recognition in Ultrasound",
        "summary": "Accurate identification of breast lesion subtypes can facilitate personalized\ntreatment and interventions. Ultrasound (US), as a safe and accessible imaging\nmodality, is extensively employed in breast abnormality screening and\ndiagnosis. However, the incidence of different subtypes exhibits a skewed\nlong-tailed distribution, posing significant challenges for automated\nrecognition. Generative augmentation provides a promising solution to rectify\ndata distribution. Inspired by this, we propose a dual-phase framework for\nlong-tailed classification that mitigates distributional bias through\nhigh-fidelity data synthesis while avoiding overuse that corrupts holistic\nperformance. The framework incorporates a reinforcement learning-driven\nadaptive sampler, dynamically calibrating synthetic-real data ratios by\ntraining a strategic multi-agent to compensate for scarcities of real data\nwhile ensuring stable discriminative capability. Furthermore, our\nclass-controllable synthetic network integrates a sketch-grounded perception\nbranch that harnesses anatomical priors to maintain distinctive class features\nwhile enabling annotation-free inference. Extensive experiments on an in-house\nlong-tailed and a public imbalanced breast US datasets demonstrate that our\nmethod achieves promising performance compared to state-of-the-art approaches.\nMore synthetic images can be found at\nhttps://github.com/Stinalalala/Breast-LT-GenAug.",
        "url": "http://arxiv.org/abs/2507.22568v1",
        "published_date": "2025-07-30T10:50:41+00:00",
        "updated_date": "2025-07-30T10:50:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shijing Chen",
            "Xinrui Zhou",
            "Yuhao Wang",
            "Yuhao Huang",
            "Ao Chang",
            "Dong Ni",
            "Ruobing Huang"
        ],
        "tldr": "This paper presents a dual-phase framework for long-tailed breast lesion subtype classification in ultrasound images, using generative augmentation and reinforcement learning to balance data distribution and improve performance.",
        "tldr_zh": "该论文提出了一种双阶段框架，用于超声图像中长尾乳腺病灶亚型分类，利用生成式增强和强化学习来平衡数据分布并提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Moiré Zero: An Efficient and High-Performance Neural Architecture for Moiré Removal",
        "summary": "Moir\\'e patterns, caused by frequency aliasing between fine repetitive\nstructures and a camera sensor's sampling process, have been a significant\nobstacle in various real-world applications, such as consumer photography and\nindustrial defect inspection. With the advancements in deep learning\nalgorithms, numerous studies-predominantly based on convolutional neural\nnetworks-have suggested various solutions to address this issue. Despite these\nefforts, existing approaches still struggle to effectively eliminate artifacts\ndue to the diverse scales, orientations, and color shifts of moir\\'e patterns,\nprimarily because the constrained receptive field of CNN-based architectures\nlimits their ability to capture the complex characteristics of moir\\'e\npatterns. In this paper, we propose MZNet, a U-shaped network designed to bring\nimages closer to a 'Moire-Zero' state by effectively removing moir\\'e patterns.\nIt integrates three specialized components: Multi-Scale Dual Attention Block\n(MSDAB) for extracting and refining multi-scale features, Multi-Shape Large\nKernel Convolution Block (MSLKB) for capturing diverse moir\\'e structures, and\nFeature Fusion-Based Skip Connection for enhancing information flow. Together,\nthese components enhance local texture restoration and large-scale artifact\nsuppression. Experiments on benchmark datasets demonstrate that MZNet achieves\nstate-of-the-art performance on high-resolution datasets and delivers\ncompetitive results on lower-resolution dataset, while maintaining a low\ncomputational cost, suggesting that it is an efficient and practical solution\nfor real-world applications. Project page:\nhttps://sngryonglee.github.io/MoireZero",
        "url": "http://arxiv.org/abs/2507.22407v1",
        "published_date": "2025-07-30T06:16:35+00:00",
        "updated_date": "2025-07-30T06:16:35+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Seungryong Lee",
            "Woojeong Baek",
            "Younghyun Kim",
            "Eunwoo Kim",
            "Haru Moon",
            "Donggon Yoo",
            "Eunbyung Park"
        ],
        "tldr": "The paper introduces MZNet, a U-shaped neural network for moiré pattern removal, using multi-scale attention, large kernel convolutions, and feature fusion to achieve state-of-the-art performance with low computational cost.",
        "tldr_zh": "该论文介绍了一种用于消除摩尔纹的U型神经网络MZNet，它使用多尺度注意力、大核卷积和特征融合，以低计算成本实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MINR: Implicit Neural Representations with Masked Image Modelling",
        "summary": "Self-supervised learning methods like masked autoencoders (MAE) have shown\nsignificant promise in learning robust feature representations, particularly in\nimage reconstruction-based pretraining task. However, their performance is\noften strongly dependent on the masking strategies used during training and can\ndegrade when applied to out-of-distribution data. To address these limitations,\nwe introduce the masked implicit neural representations (MINR) framework that\nsynergizes implicit neural representations with masked image modeling. MINR\nlearns a continuous function to represent images, enabling more robust and\ngeneralizable reconstructions irrespective of masking strategies. Our\nexperiments demonstrate that MINR not only outperforms MAE in in-domain\nscenarios but also in out-of-distribution settings, while reducing model\ncomplexity. The versatility of MINR extends to various self-supervised learning\napplications, confirming its utility as a robust and efficient alternative to\nexisting frameworks.",
        "url": "http://arxiv.org/abs/2507.22404v1",
        "published_date": "2025-07-30T06:12:57+00:00",
        "updated_date": "2025-07-30T06:12:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sua Lee",
            "Joonhun Lee",
            "Myungjoo Kang"
        ],
        "tldr": "The paper introduces MINR, a framework combining implicit neural representations with masked image modeling, demonstrating improved robustness and generalization over MAE, especially in out-of-distribution scenarios, while reducing model complexity.",
        "tldr_zh": "该论文介绍了一种名为MINR的框架，它将隐式神经表示与掩码图像建模相结合，展示了比MAE更好的鲁棒性和泛化能力，尤其是在分布外场景中，同时降低了模型复杂度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LAMA-Net: A Convergent Network Architecture for Dual-Domain Reconstruction",
        "summary": "We propose a learnable variational model that learns the features and\nleverages complementary information from both image and measurement domains for\nimage reconstruction. In particular, we introduce a learned alternating\nminimization algorithm (LAMA) from our prior work, which tackles two-block\nnonconvex and nonsmooth optimization problems by incorporating a residual\nlearning architecture in a proximal alternating framework. In this work, our\ngoal is to provide a complete and rigorous convergence proof of LAMA and show\nthat all accumulation points of a specified subsequence of LAMA must be Clarke\nstationary points of the problem. LAMA directly yields a highly interpretable\nneural network architecture called LAMA-Net. Notably, in addition to the\nresults shown in our prior work, we demonstrate that the convergence property\nof LAMA yields outstanding stability and robustness of LAMA-Net in this work.\nWe also show that the performance of LAMA-Net can be further improved by\nintegrating a properly designed network that generates suitable initials, which\nwe call iLAMA-Net. To evaluate LAMA-Net/iLAMA-Net, we conduct several\nexperiments and compare them with several state-of-the-art methods on popular\nbenchmark datasets for Sparse-View Computed Tomography.",
        "url": "http://arxiv.org/abs/2507.22316v1",
        "published_date": "2025-07-30T01:16:38+00:00",
        "updated_date": "2025-07-30T01:16:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chi Ding",
            "Qingchao Zhang",
            "Ge Wang",
            "Xiaojing Ye",
            "Yunmei Chen"
        ],
        "tldr": "The paper introduces LAMA-Net, a neural network architecture derived from a learned alternating minimization algorithm (LAMA) for image reconstruction, along with a convergence proof and experimental results on Sparse-View Computed Tomography.",
        "tldr_zh": "该论文介绍了一种名为LAMA-Net的神经网络架构，它源自学习到的交替最小化算法（LAMA），用于图像重建。论文还提供了收敛性证明，并在稀疏视图计算机断层扫描上进行了实验。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]