[
    {
        "title": "Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation",
        "summary": "Referring audio-visual segmentation (RAVS) has recently seen significant\nadvancements, yet challenges remain in integrating multimodal information and\ndeeply understanding and reasoning about audiovisual content. To extend the\nboundaries of RAVS and facilitate future research in this field, we propose\nOmnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset\ncontaining 2,104 videos and 61,095 multimodal referring expressions. OmniAVS\nstands out with three key innovations: (1) 8 types of multimodal expressions\nthat flexibly combine text, speech, sound, and visual cues; (2) an emphasis on\nunderstanding audio content beyond just detecting their presence; and (3) the\ninclusion of complex reasoning and world knowledge in expressions. Furthermore,\nwe introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the\nchallenges of multimodal reasoning and fine-grained understanding of\naudiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and\nperform reasoning-based segmentation. Extensive experiments show that OISA\noutperforms existing methods on OmniAVS and achieves competitive results on\nother related tasks.",
        "url": "http://arxiv.org/abs/2507.22886v2",
        "published_date": "2025-07-30T17:59:31+00:00",
        "updated_date": "2025-07-31T03:28:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaining Ying",
            "Henghui Ding",
            "Guangquan Jie",
            "Yu-Gang Jiang"
        ],
        "tldr": "This paper introduces a new dataset, OmniAVS, for referring audio-visual segmentation with complex multimodal expressions and reasoning, and proposes a model, OISA, that outperforms existing methods on this dataset.",
        "tldr_zh": "本文介绍了一个新的数据集 OmniAVS，用于具有复杂多模态表达和推理的指代性音视频分割，并提出了一个模型 OISA，该模型在该数据集上优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents",
        "summary": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.",
        "url": "http://arxiv.org/abs/2507.22827v1",
        "published_date": "2025-07-30T16:41:21+00:00",
        "updated_date": "2025-07-30T16:41:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yilei Jiang",
            "Yaozhi Zheng",
            "Yuxuan Wan",
            "Jiaming Han",
            "Qunzhong Wang",
            "Michael R. Lyu",
            "Xiangyu Yue"
        ],
        "tldr": "The paper introduces ScreenCoder, a modular multi-agent framework for UI-to-code generation, featuring grounding, planning, and generation stages, achieving state-of-the-art performance by fine-tuning a VLM with synthetic data.",
        "tldr_zh": "本文介绍了一种名为ScreenCoder的模块化多智能体框架，用于将UI设计转换为代码，该框架包括基础认知、规划和生成阶段。通过使用合成数据对VLM进行微调，实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention",
        "summary": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA.",
        "url": "http://arxiv.org/abs/2507.22805v1",
        "published_date": "2025-07-30T16:15:22+00:00",
        "updated_date": "2025-07-30T16:15:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuqi Pang",
            "Bowen Yang",
            "Yun Cao",
            "Fan Rong",
            "Xiaoyu Li",
            "Chen He"
        ],
        "tldr": "The paper introduces MoCHA, a vision-language framework that integrates multiple vision backbones with a Mixture of Experts Connector and Hierarchical Group Attention mechanism to improve visual reasoning and performance, outperforming state-of-the-art open-weight models on several benchmarks.",
        "tldr_zh": "该论文介绍了MoCHA，一个视觉-语言框架，它集成了多个视觉骨干网络、混合专家连接器和分层组注意力机制，以提高视觉推理性能，并在多个基准测试中优于最先进的开源模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visual Language Models as Zero-Shot Deepfake Detectors",
        "summary": "The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models\nfor face swapping, presents a substantial and evolving threat in digital media,\nidentity verification, and a multitude of other systems. The majority of\nexisting methods for detecting deepfakes rely on training specialized\nclassifiers to distinguish between genuine and manipulated images, focusing\nonly on the image domain without incorporating any auxiliary tasks that could\nenhance robustness. In this paper, inspired by the zero-shot capabilities of\nVision Language Models, we propose a novel VLM-based approach to image\nclassification and then evaluate it for deepfake detection. Specifically, we\nutilize a new high-quality deepfake dataset comprising 60,000 images, on which\nour zero-shot models demonstrate superior performance to almost all existing\nmethods. Subsequently, we compare the performance of the best-performing\narchitecture, InstructBLIP, on the popular deepfake dataset DFDC-P against\ntraditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our\nresults demonstrate the superiority of VLMs over traditional classifiers.",
        "url": "http://arxiv.org/abs/2507.22469v1",
        "published_date": "2025-07-30T08:20:02+00:00",
        "updated_date": "2025-07-30T08:20:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Viacheslav Pirogov"
        ],
        "tldr": "This paper explores using Vision Language Models (VLMs) in a zero-shot setting for deepfake detection, demonstrating superior performance compared to traditional methods on a new and existing deepfake dataset.",
        "tldr_zh": "本文探索了在零样本设置中使用视觉语言模型（VLM）进行深度伪造检测，并在新的和现有的深度伪造数据集上展示了优于传统方法的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models",
        "summary": "Large-scale but noisy image-text pair data have paved the way for the success\nof Contrastive Language-Image Pretraining (CLIP). As the foundation vision\nencoder, CLIP in turn serves as the cornerstone for most large vision-language\nmodels (LVLMs). This interdependence naturally raises an interesting question:\nCan we reciprocally leverage LVLMs to enhance the quality of image-text pair\ndata, thereby opening the possibility of a self-reinforcing cycle for\ncontinuous improvement? In this work, we take a significant step toward this\nvision by introducing an LVLM-driven data refinement pipeline. Our framework\nleverages LVLMs to process images and their raw alt-text, generating four\ncomplementary textual formulas: long positive descriptions, long negative\ndescriptions, short positive tags, and short negative tags. Applying this\npipeline to the curated DFN-Large dataset yields VLM-150M, a refined dataset\nenriched with multi-grained annotations. Based on this dataset, we further\npropose a training paradigm that extends conventional contrastive learning by\nincorporating negative descriptions and short tags as additional supervised\nsignals. The resulting model, namely HQ-CLIP, demonstrates remarkable\nimprovements across diverse benchmarks. Within a comparable training data\nscale, our approach achieves state-of-the-art performance in zero-shot\nclassification, cross-modal retrieval, and fine-grained visual understanding\ntasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models\ntrained on the DFN-2B dataset, which contains 10$\\times$ more training data\nthan ours. All code, data, and models are available at\nhttps://zxwei.site/hqclip.",
        "url": "http://arxiv.org/abs/2507.22431v1",
        "published_date": "2025-07-30T07:21:36+00:00",
        "updated_date": "2025-07-30T07:21:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhixiang Wei",
            "Guangting Wang",
            "Xiaoxiao Ma",
            "Ke Mei",
            "Huaian Chen",
            "Yi Jin",
            "Fengyun Rao"
        ],
        "tldr": "This paper introduces HQ-CLIP, a method that leverages large vision-language models (LVLMs) to refine image-text datasets, creating a self-reinforcing cycle for continuous improvement in CLIP models, achieving state-of-the-art performance with a smaller dataset than standard CLIP.",
        "tldr_zh": "该论文介绍了HQ-CLIP，一种利用大型视觉语言模型 (LVLM) 来改进图像-文本数据集的方法，从而为 CLIP 模型的持续改进创造了一个自我强化循环，并以比标准 CLIP 更小的数据集实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations",
        "summary": "Vision-Language Models (VLMs) are increasingly used as perceptual modules for\nvisual content reasoning, including through captioning and DeepFake detection.\nIn this work, we expose a critical vulnerability of VLMs when exposed to\nsubtle, structured perturbations in the frequency domain. Specifically, we\nhighlight how these feature transformations undermine authenticity/DeepFake\ndetection and automated image captioning tasks. We design targeted image\ntransformations, operating in the frequency domain to systematically adjust VLM\noutputs when exposed to frequency-perturbed real and synthetic images. We\ndemonstrate that the perturbation injection method generalizes across five\nstate-of-the-art VLMs which includes different-parameter Qwen2/2.5 and BLIP\nmodels. Experimenting across ten real and generated image datasets reveals that\nVLM judgments are sensitive to frequency-based cues and may not wholly align\nwith semantic content. Crucially, we show that visually-imperceptible spatial\nfrequency transformations expose the fragility of VLMs deployed for automated\nimage captioning and authenticity detection tasks. Our findings under\nrealistic, black-box constraints challenge the reliability of VLMs,\nunderscoring the need for robust multimodal perception systems.",
        "url": "http://arxiv.org/abs/2507.22398v1",
        "published_date": "2025-07-30T05:41:29+00:00",
        "updated_date": "2025-07-30T05:41:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jordan Vice",
            "Naveed Akhtar",
            "Yansong Gao",
            "Richard Hartley",
            "Ajmal Mian"
        ],
        "tldr": "This paper demonstrates that Vision-Language Models (VLMs) are vulnerable to subtle, frequency-domain perturbations, impacting their reliability in image captioning and DeepFake detection tasks. The authors show this vulnerability across multiple VLMs and datasets under realistic black-box constraints.",
        "tldr_zh": "该论文证明了视觉语言模型（VLMs）容易受到细微的频域扰动的影响，从而影响其在图像字幕和DeepFake检测任务中的可靠性。作者在多个VLMs和数据集上，在实际黑盒约束下展示了这种漏洞。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception",
        "summary": "Accurate interpretation of land-cover changes in multi-temporal satellite\nimagery is critical for real-world scenarios. However, existing methods\ntypically provide only one-shot change masks or static captions, limiting their\nability to support interactive, query-driven analysis. In this work, we\nintroduce remote sensing image change analysis (RSICA) as a new paradigm that\ncombines the strengths of change detection and visual question answering to\nenable multi-turn, instruction-guided exploration of changes in bi-temporal\nremote sensing images. To support this task, we construct ChangeChat-105k, a\nlarge-scale instruction-following dataset, generated through a hybrid\nrule-based and GPT-assisted process, covering six interaction types: change\ncaptioning, classification, quantification, localization, open-ended question\nanswering, and multi-turn dialogues. Building on this dataset, we propose\nDeltaVLM, an end-to-end architecture tailored for interactive RSICA. DeltaVLM\nfeatures three innovations: (1) a fine-tuned bi-temporal vision encoder to\ncapture temporal differences; (2) a visual difference perception module with a\ncross-semantic relation measuring (CSRM) mechanism to interpret changes; and\n(3) an instruction-guided Q-former to effectively extract query-relevant\ndifference information from visual changes, aligning them with textual\ninstructions. We train DeltaVLM on ChangeChat-105k using a frozen large\nlanguage model, adapting only the vision and alignment modules to optimize\nefficiency. Extensive experiments and ablation studies demonstrate that\nDeltaVLM achieves state-of-the-art performance on both single-turn captioning\nand multi-turn interactive change analysis, outperforming existing multimodal\nlarge language models and remote sensing vision-language models. Code, dataset\nand pre-trained weights are available at https://github.com/hanlinwu/DeltaVLM.",
        "url": "http://arxiv.org/abs/2507.22346v1",
        "published_date": "2025-07-30T03:14:27+00:00",
        "updated_date": "2025-07-30T03:14:27+00:00",
        "categories": [
            "cs.CV",
            "I.2.10; I.4.8; I.5.4"
        ],
        "authors": [
            "Pei Deng",
            "Wenqian Zhou",
            "Hanlin Wu"
        ],
        "tldr": "The paper introduces a new task, Remote Sensing Image Change Analysis (RSICA), and a corresponding dataset, ChangeChat-105k, for instruction-guided exploration of changes in bi-temporal remote sensing images. They also propose DeltaVLM, a tailored VLM architecture that achieves state-of-the-art performance on this task.",
        "tldr_zh": "本文介绍了一种新的任务，即遥感图像变化分析（RSICA），以及相应的 ChangeChat-105k 数据集，用于在双时相遥感图像中进行指令引导的变化探索。他们还提出了 DeltaVLM，这是一种定制的 VLM 架构，在该任务上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees",
        "summary": "Contrastive Language-Image Pre-training (CLIP)~\\citep{radford2021learning}\nhas emerged as a pivotal model in computer vision and multimodal learning,\nachieving state-of-the-art performance at aligning visual and textual\nrepresentations through contrastive learning. However, CLIP struggles with\npotential information misalignment in many image-text datasets and suffers from\nentangled representation. On the one hand, short captions for a single image in\ndatasets like MSCOCO may describe disjoint regions in the image, leaving the\nmodel uncertain about which visual features to retain or disregard. On the\nother hand, directly aligning long captions with images can lead to the\nretention of entangled details, preventing the model from learning\ndisentangled, atomic concepts -- ultimately limiting its generalization on\ncertain downstream tasks involving short prompts.\n  In this paper, we establish theoretical conditions that enable flexible\nalignment between textual and visual representations across varying levels of\ngranularity. Specifically, our framework ensures that a model can not only\n\\emph{preserve} cross-modal semantic information in its entirety but also\n\\emph{disentangle} visual representations to capture fine-grained textual\nconcepts. Building on this foundation, we introduce \\ours, a novel approach\nthat identifies and aligns the most relevant visual and textual representations\nin a modular manner. Superior performance across various tasks demonstrates its\ncapability to handle information misalignment and supports our identification\ntheory. The code is available at https://github.com/Mid-Push/SmartCLIP.",
        "url": "http://arxiv.org/abs/2507.22264v1",
        "published_date": "2025-07-29T22:26:20+00:00",
        "updated_date": "2025-07-29T22:26:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shaoan Xie",
            "Lingjing Kong",
            "Yujia Zheng",
            "Yu Yao",
            "Zeyu Tang",
            "Eric P. Xing",
            "Guangyi Chen",
            "Kun Zhang"
        ],
        "tldr": "SmartCLIP addresses information misalignment in CLIP by proposing a modular approach that identifies and aligns relevant visual and textual representations, supported by theoretical guarantees and empirical results.",
        "tldr_zh": "SmartCLIP 通过提出一种模块化方法来解决 CLIP 中的信息不对齐问题，该方法识别并对齐相关的视觉和文本表示，并由理论保证和经验结果支持。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models",
        "summary": "As Vision-Language Models (VLMs) are increasingly deployed in split-DNN\nconfigurations--with visual encoders (e.g., ResNet, ViT) operating on user\ndevices and sending intermediate features to the cloud--there is a growing\nprivacy risk from semantic information leakage. Existing approaches to\nreconstructing images from these intermediate features often result in blurry,\nsemantically ambiguous images. To directly address semantic leakage, we propose\nCapRecover, a cross-modality inversion framework that recovers high-level\nsemantic content, such as labels or captions, directly from intermediate\nfeatures without image reconstruction.\n  We evaluate CapRecover on multiple datasets and victim models, demonstrating\nstrong performance in semantic recovery. Specifically, CapRecover achieves up\nto 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from\nResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis\nfurther reveals that deeper convolutional layers encode significantly more\nsemantic information compared to shallow layers. To mitigate semantic leakage,\nwe introduce a simple yet effective protection method: adding random noise to\nintermediate features at each layer and removing the noise in the next layer.\nExperimental results show that this approach prevents semantic leakage without\nadditional training costs.",
        "url": "http://arxiv.org/abs/2507.22828v1",
        "published_date": "2025-07-30T16:42:02+00:00",
        "updated_date": "2025-07-30T16:42:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kedong Xiu",
            "Saiqian Zhang"
        ],
        "tldr": "The paper introduces CapRecover, a cross-modality feature inversion attack framework that recovers semantic information (labels, captions) from intermediate features of Vision-Language Models in split-DNN configurations, and proposes a simple defense mechanism.",
        "tldr_zh": "该论文介绍了CapRecover，一个跨模态特征反演攻击框架，用于从分割式DNN配置中视觉语言模型的中间特征中恢复语义信息（标签、字幕），并提出了一种简单的防御机制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings",
        "summary": "Accurate fetal biometric measurements, such as abdominal circumference, play\na vital role in prenatal care. However, obtaining high-quality ultrasound\nimages for these measurements heavily depends on the expertise of sonographers,\nposing a significant challenge in low-income countries due to the scarcity of\ntrained personnel. To address this issue, we leverage FetalCLIP, a\nvision-language model pretrained on a curated dataset of over 210,000 fetal\nultrasound image-caption pairs, to perform automated fetal ultrasound image\nquality assessment (IQA) on blind-sweep ultrasound data. We introduce\nFetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank\nAdaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN\nand Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of\n0.757. Moreover, we show that an adapted segmentation model, when repurposed\nfor classification, further improves performance, achieving an F1 score of\n0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal\nultrasound foundation models can enable task-specific adaptations, advancing\nprenatal care in resource-limited settings. The experimental code is available\nat: https://github.com/donglihe-hub/FetalCLIP-IQA.",
        "url": "http://arxiv.org/abs/2507.22802v1",
        "published_date": "2025-07-30T16:09:29+00:00",
        "updated_date": "2025-07-30T16:09:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dongli He",
            "Hu Wang",
            "Mohammad Yaqub"
        ],
        "tldr": "The paper fine-tunes a vision-language model, FetalCLIP, for fetal ultrasound image quality assessment, showing improved F1 scores in a low-resource setting. It demonstrates parameter-efficient adaptation for task-specific applications in prenatal care.",
        "tldr_zh": "该论文针对胎儿超声图像质量评估微调了一个视觉语言模型 FetalCLIP，在资源匮乏的环境下展示了改进的 F1 分数。它演示了参数高效的自适应性，可用于产前护理中的特定任务应用。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning",
        "summary": "Reinforcement learning has proven its effectiveness in enhancing the\nreasoning capabilities of large language models. Recent research efforts have\nprogressively extended this paradigm to multimodal reasoning tasks. Due to the\ninherent complexity and diversity of multimodal tasks, especially in semantic\ncontent and problem formulations, existing models often exhibit unstable\nperformance across various domains and difficulty levels. To address these\nlimitations, we propose VL-Cogito, an advanced multimodal reasoning model\ntrained via a novel multi-stage Progressive Curriculum Reinforcement Learning\n(PCuRL) framework. PCuRL systematically guides the model through tasks of\ngradually increasing difficulty, substantially improving its reasoning\nabilities across diverse multimodal contexts. The framework introduces two key\ninnovations: (1) an online difficulty soft weighting mechanism, dynamically\nadjusting training difficulty across successive RL training stages; and (2) a\ndynamic length reward mechanism, which encourages the model to adaptively\nregulate its reasoning path length according to task complexity, thus balancing\nreasoning efficiency with correctness. Experimental evaluations demonstrate\nthat VL-Cogito consistently matches or surpasses existing reasoning-oriented\nmodels across mainstream multimodal benchmarks spanning mathematics, science,\nlogic, and general understanding, validating the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2507.22607v2",
        "published_date": "2025-07-30T12:23:21+00:00",
        "updated_date": "2025-07-31T09:09:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Ruifeng Yuan",
            "Chenghao Xiao",
            "Sicong Leng",
            "Jianyu Wang",
            "Long Li",
            "Weiwen Xu",
            "Hou Pong Chan",
            "Deli Zhao",
            "Tingyang Xu",
            "Zhongyu Wei",
            "Hao Zhang",
            "Yu Rong"
        ],
        "tldr": "This paper introduces VL-Cogito, a multimodal reasoning model trained with a progressive curriculum reinforcement learning framework that uses dynamic difficulty weighting and reward mechanisms to improve reasoning abilities across diverse multimodal tasks.",
        "tldr_zh": "本文介绍了一种多模态推理模型VL-Cogito，该模型采用渐进式课程强化学习框架进行训练，利用动态难度加权和奖励机制来提高在各种多模态任务中的推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP",
        "summary": "Out-of-distribution (OOD) detection is an important building block in\ntrustworthy image recognition systems as unknown classes may arise at\ntest-time. OOD detection methods typically revolve around a single classifier,\nleading to a split in the research field between the classical supervised\nsetting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot\nsetting (class names fed as prompts to CLIP). In both cases, an overarching\nchallenge is that the OOD detection performance is implicitly constrained by\nthe classifier's capabilities on in-distribution (ID) data. In this work, we\nshow that given a little open-mindedness from both ends, remarkable OOD\ndetection can be achieved by instead creating a heterogeneous ensemble - COOkeD\ncombines the predictions of a closed-world classifier trained end-to-end on a\nspecific dataset, a zero-shot CLIP classifier, and a linear probe classifier\ntrained on CLIP image features. While bulky at first sight, this approach is\nmodular, post-hoc and leverages the availability of pre-trained VLMs, thus\nintroduces little overhead compared to training a single standard classifier.\nWe evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also\nconsider more challenging, realistic settings ranging from training-time label\nnoise, to test-time covariate shift, to zero-shot shift which has been\npreviously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art\nperformance and greater robustness compared to both classical and CLIP-based\nOOD detection methods. Code is available at https://github.com/glhr/COOkeD",
        "url": "http://arxiv.org/abs/2507.22576v1",
        "published_date": "2025-07-30T11:02:38+00:00",
        "updated_date": "2025-07-30T11:02:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Galadrielle Humblot-Renaux",
            "Gianni Franchi",
            "Sergio Escalera",
            "Thomas B. Moeslund"
        ],
        "tldr": "The paper introduces COOkeD, a novel ensemble-based OOD detection method combining a closed-world classifier, a zero-shot CLIP classifier, and a linear probe, achieving SOTA performance and robustness in various challenging settings. It's efficient and modular, leveraging pre-trained VLMs.",
        "tldr_zh": "该论文介绍了COOkeD，一种新颖的基于集成的OOD检测方法，结合了闭世界分类器、零样本CLIP分类器和线性探针，在各种具有挑战性的设置中实现了SOTA性能和鲁棒性。它高效且模块化，充分利用了预训练的VLM。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam",
        "summary": "This paper introduces a novel benchmark, EGE-Math Solutions Assessment\nBenchmark, for evaluating Vision-Language Models (VLMs) on their ability to\nassess hand-written mathematical solutions. Unlike existing benchmarks that\nfocus on problem solving, our approach centres on understanding student\nsolutions, identifying mistakes, and assigning grades according to fixed\ncriteria. We compile 122 scanned solutions from the Russian Unified State Exam\n(EGE) together with official expert grades, and evaluate seven modern VLMs from\nGoogle, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The\nresults reveal current limitations in mathematical reasoning and human-rubric\nalignment, opening new research avenues in AI-assisted assessment. You can find\ncode in https://github.com/Karifannaa/Auto-check-EGE-math",
        "url": "http://arxiv.org/abs/2507.22958v1",
        "published_date": "2025-07-29T23:46:45+00:00",
        "updated_date": "2025-07-29T23:46:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "68T07, 97D50",
            "I.2.7; I.4; K.3.1"
        ],
        "authors": [
            "Ruslan Khrulev"
        ],
        "tldr": "The paper introduces a new benchmark, EGE-Math, for evaluating VLMs on their ability to assess handwritten mathematical solutions from the Russian Unified State Exam, revealing limitations in mathematical reasoning and human-rubric alignment.",
        "tldr_zh": "该论文介绍了一个新的基准，EGE-Math，用于评估视觉语言模型在评估俄罗斯国家统一考试手写数学解答方面的能力，揭示了数学推理和人工评分标准对齐方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7
    },
    {
        "title": "Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework",
        "summary": "Video signals are vulnerable in multimedia communication and storage systems,\nas even slight bitstream-domain corruption can lead to significant pixel-domain\ndegradation. To recover faithful spatio-temporal content from corrupted inputs,\nbitstream-corrupted video recovery has recently emerged as a challenging and\nunderstudied task. However, existing methods require time-consuming and\nlabor-intensive annotation of corrupted regions for each corrupted video frame,\nresulting in a large workload in practice. In addition, high-quality recovery\nremains difficult as part of the local residual information in corrupted frames\nmay mislead feature completion and successive content recovery. In this paper,\nwe propose the first blind bitstream-corrupted video recovery framework that\nintegrates visual foundation models with a recovery model, which is adapted to\ndifferent types of corruption and bitstream-level prompts. Within the\nframework, the proposed Detect Any Corruption (DAC) model leverages the rich\npriors of the visual foundation model while incorporating bitstream and\ncorruption knowledge to enhance corruption localization and blind recovery.\nAdditionally, we introduce a novel Corruption-aware Feature Completion (CFC)\nmodule, which adaptively processes residual contributions based on high-level\ncorruption understanding. With VFM-guided hierarchical feature augmentation and\nhigh-level coordination in a mixture-of-residual-experts (MoRE) structure, our\nmethod suppresses artifacts and enhances informative residuals. Comprehensive\nevaluations show that the proposed method achieves outstanding performance in\nbitstream-corrupted video recovery without requiring a manually labeled mask\nsequence. The demonstrated effectiveness will help to realize improved user\nexperience, wider application scenarios, and more reliable multimedia\ncommunication and storage systems.",
        "url": "http://arxiv.org/abs/2507.22481v1",
        "published_date": "2025-07-30T08:31:54+00:00",
        "updated_date": "2025-07-30T08:31:54+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Tianyi Liu",
            "Kejun Wu",
            "Chen Cai",
            "Yi Wang",
            "Kim-Hui Yap",
            "Lap-Pui Chau"
        ],
        "tldr": "This paper introduces a novel framework for blind bitstream-corrupted video recovery using visual foundation models (VFMs), achieving state-of-the-art performance without manual annotation.",
        "tldr_zh": "本文提出了一种新颖的框架，利用视觉基础模型（VFM）进行盲比特流损坏视频恢复，无需手动标注即可实现最先进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    },
    {
        "title": "Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring",
        "summary": "Classroom behavior monitoring is a critical aspect of educational research,\nwith significant implications for student engagement and learning outcomes.\nRecent advancements in Visual Question Answering (VQA) models offer promising\ntools for automatically analyzing complex classroom interactions from video\nrecordings. In this paper, we investigate the applicability of several\nstate-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, and\nNVILA, in the context of classroom behavior analysis. To facilitate rigorous\nevaluation, we introduce our BAV-Classroom-VQA dataset derived from real-world\nclassroom video recordings at the Banking Academy of Vietnam. We present the\nmethodology for data collection, annotation, and benchmark the performance of\nthe selected VQA models on this dataset. Our initial experimental results\ndemonstrate that all four models achieve promising performance levels in\nanswering behavior-related visual questions, showcasing their potential in\nfuture classroom analytics and intervention systems.",
        "url": "http://arxiv.org/abs/2507.22369v1",
        "published_date": "2025-07-30T04:25:14+00:00",
        "updated_date": "2025-07-30T04:25:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sinh Trong Vu",
            "Hieu Trung Pham",
            "Dung Manh Nguyen",
            "Hieu Minh Hoang",
            "Nhu Hoang Le",
            "Thu Ha Pham",
            "Tai Tan Mai"
        ],
        "tldr": "This paper explores the use of open-source VQA models (LLaMA2, LLaMA3, QWEN3, NVILA) for classroom activity monitoring, introducing a new BAV-Classroom-VQA dataset and demonstrating promising initial performance.",
        "tldr_zh": "本文探讨了使用开源 VQA 模型 (LLaMA2, LLaMA3, QWEN3, NVILA) 进行课堂活动监控，引入了一个新的 BAV-Classroom-VQA 数据集，并展示了有希望的初始性能。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]