[
    {
        "title": "Team of One: Cracking Complex Video QA with Model Synergy",
        "summary": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
        "url": "http://arxiv.org/abs/2507.13820v1",
        "published_date": "2025-07-18T11:12:44+00:00",
        "updated_date": "2025-07-18T11:12:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jun Xie",
            "Zhaoran Zhao",
            "Xiongjun Guan",
            "Yingjian Zhu",
            "Hongzhu Yi",
            "Xinming Wang",
            "Feng Chen",
            "Zhepeng Wang"
        ],
        "tldr": "The paper introduces a framework for video question answering that coordinates multiple Video-Language Models (VLMs) using a prompting-and-response integration mechanism, with an external LLM serving as an evaluator and integrator, achieving superior performance on the CVRR-ES dataset.",
        "tldr_zh": "该论文提出了一个视频问答框架，通过提示和响应集成机制协调多个视频语言模型（VLM），并使用外部LLM作为评估器和集成器，在CVRR-ES数据集上实现了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
        "summary": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
        "url": "http://arxiv.org/abs/2507.14024v1",
        "published_date": "2025-07-18T15:52:39+00:00",
        "updated_date": "2025-07-18T15:52:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiarong Ye",
            "Sharon X. Huang"
        ],
        "tldr": "The paper introduces Moodifier, an MLLM-enhanced, training-free image editing model that translates abstract emotions into specific visual attributes, leveraging a newly created 8M+ image dataset, MoodArchive, and a fine-tuned vision-language model, MoodifyCLIP.",
        "tldr_zh": "该论文介绍了 Moodifier，一个 MLLM 增强的，无需训练的图像编辑模型，它将抽象的情感转化为特定的视觉属性，利用了一个新创建的包含超过 800 万张图像的数据集 MoodArchive 和一个微调的视觉语言模型 MoodifyCLIP。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
        "summary": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
        "url": "http://arxiv.org/abs/2507.13993v2",
        "published_date": "2025-07-18T15:01:44+00:00",
        "updated_date": "2025-07-26T06:47:55+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ningyong Wu",
            "Jinzhi Wang",
            "Wenhong Zhao",
            "Chenzhan Yu",
            "Zhigang Xiu",
            "Duwei Dai"
        ],
        "tldr": "OrthoInsight is a multi-modal deep learning framework leveraging YOLOv9, medical knowledge graphs, and a fine-tuned LLaVA model for rib fracture diagnosis and report generation, outperforming GPT-4 and Claude-3 in diagnostic accuracy and report quality.",
        "tldr_zh": "OrthoInsight是一个多模态深度学习框架，利用YOLOv9、医学知识图谱和微调的LLaVA模型进行肋骨骨折诊断和报告生成，在诊断准确性和报告质量方面优于GPT-4和Claude-3。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
        "summary": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
        "url": "http://arxiv.org/abs/2507.13868v1",
        "published_date": "2025-07-18T12:42:30+00:00",
        "updated_date": "2025-07-18T12:42:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Francesco Ortu",
            "Zhijing Jin",
            "Diego Doimo",
            "Alberto Cazzaniga"
        ],
        "tldr": "This paper investigates how vision-language models resolve conflicts between their internal knowledge and external visual information, identifying specific attention heads responsible for visual overrides and demonstrating the ability to steer model behavior by modifying these heads.",
        "tldr_zh": "本文研究了视觉-语言模型如何解决其内部知识与外部视觉信息之间的冲突，识别出负责视觉覆盖的特定注意力头，并展示了通过修改这些头来引导模型行为的能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions",
        "summary": "In visual question answering (VQA) context, users often pose ambiguous\nquestions to visual language models (VLMs) due to varying expression habits.\nExisting research addresses such ambiguities primarily by rephrasing questions.\nThese approaches neglect the inherently interactive nature of user interactions\nwith VLMs, where ambiguities can be clarified through user feedback. However,\nresearch on interactive clarification faces two major challenges: (1)\nBenchmarks are absent to assess VLMs' capacity for resolving ambiguities\nthrough interaction; (2) VLMs are trained to prefer answering rather than\nasking, preventing them from seeking clarification. To overcome these\nchallenges, we introduce \\textbf{ClearVQA} benchmark, which targets three\ncommon categories of ambiguity in VQA context, and encompasses various VQA\nscenarios.",
        "url": "http://arxiv.org/abs/2507.13773v1",
        "published_date": "2025-07-18T09:31:43+00:00",
        "updated_date": "2025-07-18T09:31:43+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Pu Jian",
            "Donglei Yu",
            "Wen Yang",
            "Shuo Ren",
            "Jiajun Zhang"
        ],
        "tldr": "This paper introduces ClearVQA, a new benchmark for evaluating VLMs' ability to resolve ambiguities in visual question answering through interactive clarification, addressing the lack of existing benchmarks and VLMs' preference for answering over asking.",
        "tldr_zh": "本文介绍了ClearVQA，一个新的基准，用于评估视觉语言模型通过交互式澄清来解决视觉问答中歧义的能力，解决了现有基准的缺乏以及视觉语言模型倾向于回答而不是提问的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks",
        "summary": "Despite recent progress in video large language models (VideoLLMs), a key\nopen challenge remains: how to equip models with chain-of-thought (CoT)\nreasoning abilities grounded in fine-grained object-level video understanding.\nExisting instruction-tuned models, such as the Qwen and LLaVA series, are\ntrained on high-level video-text pairs, often lacking structured annotations\nnecessary for compositional, step-by-step reasoning. We propose CoTasks:\nChain-of-Thought based Video Instruction Tuning Tasks, a new framework that\ndecomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)\ninto four entity-level foundational tasks: frame localization, entity tracking,\nspatial and temporal relation extraction. By embedding these intermediate\nCoT-style reasoning steps into the input, CoTasks enables models to explicitly\nperform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA\nbenchmark show that CoTasks significantly enhance inference performance:\nLLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and\nQwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal\n(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the\neffectiveness of CoTasks as a structured CoT-style supervision framework for\nimproving compositional video reasoning.",
        "url": "http://arxiv.org/abs/2507.13609v1",
        "published_date": "2025-07-18T02:29:19+00:00",
        "updated_date": "2025-07-18T02:29:19+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yanan Wang",
            "Julio Vizcarra",
            "Zhi Li",
            "Hao Niu",
            "Mori Kurokawa"
        ],
        "tldr": "The paper introduces CoTasks, a framework that decomposes complex video questions into entity-level tasks to enhance chain-of-thought reasoning in VideoLLMs, demonstrating significant performance improvements on NeXT-QA.",
        "tldr_zh": "该论文介绍了CoTasks，一个将复杂的视频问题分解为实体级别任务的框架，旨在增强视频大型语言模型（VideoLLMs）中的思维链推理能力，并在NeXT-QA上展示了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning",
        "summary": "Continual learning for vision-language models has achieved remarkable\nperformance through synthetic replay, where samples are generated using Stable\nDiffusion to regularize during finetuning and retain knowledge. However,\nreal-world downstream applications often exhibit domain-specific nuances and\nfine-grained semantics not captured by generators, causing synthetic-replay\nmethods to produce misaligned samples that misguide finetuning and undermine\nretention of prior knowledge. In this work, we propose a LoRA-enhanced\nsynthetic-replay framework that injects task-specific low-rank adapters into a\nfrozen Stable Diffusion model, efficiently capturing each new task's unique\nvisual and semantic patterns. Specifically, we introduce a two-stage,\nconfidence-based sample selection: we first rank real task data by\npost-finetuning VLM confidence to focus LoRA finetuning on the most\nrepresentative examples, then generate synthetic samples and again select them\nby confidence for distillation. Our approach integrates seamlessly with\nexisting replay pipelines-simply swap in the adapted generator to boost replay\nfidelity. Extensive experiments on the Multi-domain Task Incremental Learning\n(MTIL) benchmark show that our method outperforms previous synthetic-replay\ntechniques, achieving an optimal balance among plasticity, stability, and\nzero-shot capability. These results demonstrate the effectiveness of generator\nadaptation via LoRA for robust continual learning in VLMs.",
        "url": "http://arxiv.org/abs/2507.13568v2",
        "published_date": "2025-07-17T23:08:29+00:00",
        "updated_date": "2025-07-29T03:29:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaihong Wang",
            "Donghyun Kim",
            "Margrit Betke"
        ],
        "tldr": "The paper introduces LoRA-Loop, a LoRA-enhanced synthetic replay framework for continual vision-language model learning that adapts a frozen Stable Diffusion model to generate more representative samples for knowledge retention during finetuning, improving performance on multi-domain incremental learning.",
        "tldr_zh": "该论文提出了LoRA-Loop，一个LoRA增强的合成回放框架，用于持续视觉语言模型学习。该框架通过调整冻结的Stable Diffusion模型来生成更具代表性的样本，以在微调期间保持知识，从而提高在多域增量学习中的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
        "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
        "url": "http://arxiv.org/abs/2507.13956v1",
        "published_date": "2025-07-18T14:21:24+00:00",
        "updated_date": "2025-07-18T14:21:24+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yutao Jin",
            "Haowen Xiao",
            "Jielei Chu",
            "Fengmao Lv",
            "Yuxiao Li",
            "Tianrui Li"
        ],
        "tldr": "The paper introduces ADPC, a novel visual-language causal intervention framework leveraging LLMs and multi-modal data (MRI, fMRI, text) to improve Alzheimer's Disease diagnosis by mitigating confounders through causal intervention, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了一种名为ADPC的新型视觉-语言因果干预框架，该框架利用大型语言模型和多模态数据（MRI、fMRI、文本），通过因果干预来减轻混杂因素，从而提高阿尔茨海默病的诊断水平，并取得了最先进的成果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement",
        "summary": "Recent advancements in text-to-image diffusion models have achieved\nremarkable success in generating realistic and diverse visual content. A\ncritical factor in this process is the model's ability to accurately interpret\ntextual prompts. However, these models often struggle with creative\nexpressions, particularly those involving complex, abstract, or highly\ndescriptive language. In this work, we introduce a novel training-free approach\ntailored to improve image generation for a unique form of creative language:\npoetic verse, which frequently features layered, abstract, and dual meanings.\nOur proposed PoemTale Diffusion approach aims to minimise the information that\nis lost during poetic text-to-image conversion by integrating a multi stage\nprompt refinement loop into Language Models to enhance the interpretability of\npoetic texts. To support this, we adapt existing state-of-the-art diffusion\nmodels by modifying their self-attention mechanisms with a consistent\nself-attention technique to generate multiple consistent images, which are then\ncollectively used to convey the poem's meaning. Moreover, to encourage research\nin the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting\nof 1111 poems sourced from multiple online and offline resources. We engaged a\npanel of poetry experts for qualitative assessments. The results from both\nhuman and quantitative evaluations validate the efficacy of our method and\ncontribute a novel perspective to poem-to-image generation with enhanced\ninformation capture in the generated images.",
        "url": "http://arxiv.org/abs/2507.13708v2",
        "published_date": "2025-07-18T07:33:08+00:00",
        "updated_date": "2025-07-23T13:00:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sofia Jamil",
            "Bollampalli Areen Reddy",
            "Raghvendra Kumar",
            "Sriparna Saha",
            "Koustava Goswami",
            "K. J. Joseph"
        ],
        "tldr": "The paper introduces PoemTale Diffusion, a training-free approach that uses multi-stage prompt refinement and modified self-attention in diffusion models to improve poem-to-image generation, along with a new P4I dataset for poetry-image research.",
        "tldr_zh": "该论文介绍了 PoemTale Diffusion，这是一种无需训练的方法，它使用多阶段提示细化和修改后的扩散模型中的自注意力来改进诗歌到图像的生成，并提供了一个新的 P4I 数据集用于诗歌图像研究。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Salience Adjustment for Context-Based Emotion Recognition",
        "summary": "Emotion recognition in dynamic social contexts requires an understanding of\nthe complex interaction between facial expressions and situational cues. This\npaper presents a salience-adjusted framework for context-aware emotion\nrecognition with Bayesian Cue Integration (BCI) and Visual-Language Models\n(VLMs) to dynamically weight facial and contextual information based on the\nexpressivity of facial cues. We evaluate this approach using human annotations\nand automatic emotion recognition systems in prisoner's dilemma scenarios,\nwhich are designed to evoke emotional reactions. Our findings demonstrate that\nincorporating salience adjustment enhances emotion recognition performance,\noffering promising directions for future research to extend this framework to\nbroader social contexts and multimodal applications.",
        "url": "http://arxiv.org/abs/2507.15878v1",
        "published_date": "2025-07-17T20:55:20+00:00",
        "updated_date": "2025-07-17T20:55:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Bin Han",
            "Jonathan Gratch"
        ],
        "tldr": "This paper introduces a salience-adjusted framework using Bayesian Cue Integration and Visual-Language Models to dynamically weight facial and contextual cues for improved context-aware emotion recognition in social scenarios.",
        "tldr_zh": "本文介绍了一个基于贝叶斯线索整合和视觉语言模型的显著性调整框架，用于动态地权衡面部和上下文线索，从而提高社会场景中上下文感知的面部情绪识别。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
        "summary": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
        "url": "http://arxiv.org/abs/2507.13812v1",
        "published_date": "2025-07-18T10:44:22+00:00",
        "updated_date": "2025-07-18T10:44:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingying Zhang",
            "Lixiang Ru",
            "Kang Wu",
            "Lei Yu",
            "Lei Liang",
            "Yansheng Li",
            "Jingdong Chen"
        ],
        "tldr": "SkySense V2 is a unified multi-modal remote sensing foundation model using a single transformer backbone and a novel self-supervised learning strategy tailored for remote sensing data, achieving improved performance across various Earth observation tasks.",
        "tldr_zh": "SkySense V2是一个统一的多模态遥感基础模型，它使用单个transformer骨干网络和一种为遥感数据定制的新型自监督学习策略，在各种地球观测任务中实现了性能的提升。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]