[
    {
        "title": "Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process",
        "summary": "With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.",
        "url": "http://arxiv.org/abs/2601.09410v1",
        "published_date": "2026-01-14T11:57:15+00:00",
        "updated_date": "2026-01-14T11:57:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sangjun Han",
            "Youngmi Hur"
        ],
        "tldr": "This paper proposes a Laplacian pyramid-based detail loss and repeated upscaling/downscaling process to enhance high-frequency details in super-resolution images, achieving state-of-the-art results for CNN-based models and improvements for attention-based models.",
        "tldr_zh": "本文提出了一种基于拉普拉斯金字塔的细节损失和重复升/降尺度过程，以增强超分辨率图像中的高频细节。该方法在基于CNN的模型上取得了最先进的结果，并改进了基于注意力机制的模型。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model",
        "summary": "Longitudinal brain MRI is essential for lifespan study, yet high attrition rates often lead to missing data, complicating analysis. Deep generative models have been explored, but most rely solely on image intensity, leading to two key limitations: 1) the fidelity or trustworthiness of the generated brain images are limited, making downstream studies questionable; 2) the usage flexibility is restricted due to fixed guidance rooted in the model structure, restricting full ability to versatile application scenarios. To address these challenges, we introduce DF-DiffCom, a Kolmogorov-Arnold Networks (KAN)-enhanced diffusion model that smartly leverages deformation fields for trustworthy longitudinal brain image completion. Trained on OASIS-3, DF-DiffCom outperforms state-of-the-art methods, improving PSNR by 5.6% and SSIM by 0.12. More importantly, its modality-agnostic nature allows smooth extension to varied MRI modalities, even to attribute maps such as brain tissue segmentation results.",
        "url": "http://arxiv.org/abs/2601.09572v1",
        "published_date": "2026-01-14T15:41:40+00:00",
        "updated_date": "2026-01-14T15:41:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianli Tao",
            "Ziyang Wang",
            "Delong Yang",
            "Han Zhang",
            "Le Zhang"
        ],
        "tldr": "The paper introduces DF-DiffCom, a KAN-enhanced diffusion model that leverages deformation fields for trustworthy completion of longitudinal brain MRI, addressing limitations of existing methods regarding fidelity and flexibility. It achieves state-of-the-art performance and modality-agnostic extension.",
        "tldr_zh": "该论文介绍了一种名为DF-DiffCom的KAN增强扩散模型，该模型利用变形场实现可信的纵向脑部MRI补全，解决了现有方法在保真度和灵活性方面的局限性。它实现了最先进的性能和模态无关的扩展。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Knowledge-Embedded and Hypernetwork-Guided Few-Shot Substation Meter Defect Image Generation Method",
        "summary": "Substation meters play a critical role in monitoring and ensuring the stable operation of power grids, yet their detection of cracks and other physical defects is often hampered by a severe scarcity of annotated samples. To address this few-shot generation challenge, we propose a novel framework that integrates Knowledge Embedding and Hypernetwork-Guided Conditional Control into a Stable Diffusion pipeline, enabling realistic and controllable synthesis of defect images from limited data.\n  First, we bridge the substantial domain gap between natural-image pre-trained models and industrial equipment by fine-tuning a Stable Diffusion backbone using DreamBooth-style knowledge embedding. This process encodes the unique structural and textural priors of substation meters, ensuring generated images retain authentic meter characteristics.\n  Second, we introduce a geometric crack modeling module that parameterizes defect attributes--such as location, length, curvature, and branching pattern--to produce spatially constrained control maps. These maps provide precise, pixel-level guidance during generation.\n  Third, we design a lightweight hypernetwork that dynamically modulates the denoising process of the diffusion model in response to the control maps and high-level defect descriptors, achieving a flexible balance between generation fidelity and controllability.\n  Extensive experiments on a real-world substation meter dataset demonstrate that our method substantially outperforms existing augmentation and generation baselines. It reduces Frechet Inception Distance (FID) by 32.7%, increases diversity metrics, and--most importantly--boosts the mAP of a downstream defect detector by 15.3% when trained on augmented data. The framework offers a practical, high-quality data synthesis solution for industrial inspection systems where defect samples are rare.",
        "url": "http://arxiv.org/abs/2601.09238v1",
        "published_date": "2026-01-14T07:21:57+00:00",
        "updated_date": "2026-01-14T07:21:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jackie Alex",
            "Justin Petter"
        ],
        "tldr": "This paper introduces a novel Stable Diffusion-based framework for generating synthetic defect images of substation meters in few-shot settings, using knowledge embedding and hypernetwork-guided conditional control, demonstrably improving downstream defect detection performance.",
        "tldr_zh": "本文提出了一种基于Stable Diffusion的新框架，用于在少样本环境中生成变电站仪表的合成缺陷图像，该框架采用知识嵌入和超网络引导的条件控制，并显著提高了下游缺陷检测的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion",
        "summary": "Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.\n  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.",
        "url": "http://arxiv.org/abs/2601.09213v1",
        "published_date": "2026-01-14T06:38:12+00:00",
        "updated_date": "2026-01-14T06:38:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jialu Li",
            "Taiyan Zhou"
        ],
        "tldr": "The paper introduces SpikeVAEDiff, a two-stage framework using VDVAE and Versatile Diffusion to reconstruct high-resolution images from neural spike data, achieving superior spatiotemporal resolution compared to fMRI-based methods.",
        "tldr_zh": "本文介绍了一种名为SpikeVAEDiff的两阶段框架，该框架使用VDVAE和Versatile Diffusion从神经脉冲数据重建高分辨率图像，与基于fMRI的方法相比，实现了卓越的时空分辨率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation",
        "summary": "Despite significant progress in autoregressive image generation, inference remains slow due to the sequential nature of AR models and the ambiguity of image tokens, even when using speculative decoding. Recent works attempt to address this with relaxed speculative decoding but lack theoretical grounding. In this paper, we establish the theoretical basis of relaxed SD and propose COOL-SD, an annealed relaxation of speculative decoding built on two key insights. The first analyzes the total variation (TV) distance between the target model and relaxed speculative decoding and yields an optimal resampling distribution that minimizes an upper bound of the distance. The second uses perturbation analysis to reveal an annealing behaviour in relaxed speculative decoding, motivating our annealed design. Together, these insights enable COOL-SD to generate images faster with comparable quality, or achieve better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.",
        "url": "http://arxiv.org/abs/2601.09212v1",
        "published_date": "2026-01-14T06:35:21+00:00",
        "updated_date": "2026-01-14T06:35:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xingyao Li",
            "Fengzhuo Zhang",
            "Cunxiao Du",
            "Hui Ji"
        ],
        "tldr": "The paper introduces COOL-SD, an annealed relaxation of speculative decoding for faster autoregressive image generation, with theoretical grounding and experimental validation showing improvements in speed-quality trade-offs.",
        "tldr_zh": "本文提出了一种用于加速自回归图像生成的退火松弛推测解码方法COOL-SD，该方法具有理论基础并通过实验验证，表明其在速度-质量权衡方面有所改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "S3-CLIP: Video Super Resolution for Person-ReID",
        "summary": "Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.",
        "url": "http://arxiv.org/abs/2601.08807v1",
        "published_date": "2026-01-13T18:46:37+00:00",
        "updated_date": "2026-01-13T18:46:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tamas Endrei",
            "Gyorgy Cserey"
        ],
        "tldr": "The paper introduces S3-CLIP, a video super-resolution framework integrated with CLIP for person re-identification (ReID), specifically addressing challenges in low-quality tracklets and cross-view scenarios, showing improved performance in ground-to-aerial ranking accuracy.",
        "tldr_zh": "本文介绍了S3-CLIP，一个结合了CLIP的视频超分辨率框架，用于行人重识别（ReID），特别解决了低质量轨迹和跨视角场景中的挑战，并在地面到空中的排名准确率方面表现出改进。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Motion Attribution for Video Generation",
        "summary": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
        "url": "http://arxiv.org/abs/2601.08828v1",
        "published_date": "2026-01-13T18:59:09+00:00",
        "updated_date": "2026-01-13T18:59:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.RO"
        ],
        "authors": [
            "Xindi Wu",
            "Despoina Paschalidou",
            "Jun Gao",
            "Antonio Torralba",
            "Laura Leal-Taixé",
            "Olga Russakovsky",
            "Sanja Fidler",
            "Jonathan Lorraine"
        ],
        "tldr": "This paper introduces Motive, a framework for attributing motion in video generation models, which can identify influential clips for data curation and improve temporal consistency and physical plausibility, achieving significant human preference wins.",
        "tldr_zh": "本文介绍了一种名为Motive的框架，用于在视频生成模型中进行运动归因，可以识别用于数据管理的关键片段，并提高时间一致性和物理合理性，从而在人类偏好上取得了显著的胜利。",
        "relevance_score": 4,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]