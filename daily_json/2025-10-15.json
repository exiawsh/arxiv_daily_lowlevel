[
    {
        "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
        "summary": "Although recent advances in visual generation have been remarkable, most\nexisting architectures still depend on distinct encoders for images and text.\nThis separation constrains diffusion models' ability to perform cross-modal\nreasoning and knowledge transfer. Prior attempts to bridge this gap often use\nthe last layer information from VLM, employ multiple visual encoders, or train\nlarge unified models jointly for text and image generation, which demands\nsubstantial computational resources and large-scale data, limiting its\naccessibility.We present UniFusion, a diffusion-based generative model\nconditioned on a frozen large vision-language model (VLM) that serves as a\nunified multimodal encoder. At the core of UniFusion is the Layerwise Attention\nPooling (LAP) mechanism that extracts both high level semantics and low level\ndetails from text and visual tokens of a frozen VLM to condition a diffusion\ngenerative model. We demonstrate that LAP outperforms other shallow fusion\narchitectures on text-image alignment for generation and faithful transfer of\nvisual information from VLM to the diffusion model which is key for editing. We\npropose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),\nwhich conditions a diffusion transformer (DiT) only on the text tokens\ngenerated by the VLM during in-model prompt rewriting. VERIFI combines the\nalignment of the conditioning distribution with the VLM's reasoning\ncapabilities for increased capabilities and flexibility at inference. In\naddition, finetuning on editing task not only improves text-image alignment for\ngeneration, indicative of cross-modality knowledge transfer, but also exhibits\ntremendous generalization capabilities. Our model when trained on single image\nediting, zero-shot generalizes to multiple image references further motivating\nthe unified encoder design of UniFusion.",
        "url": "http://arxiv.org/abs/2510.12789v1",
        "published_date": "2025-10-14T17:57:56+00:00",
        "updated_date": "2025-10-14T17:57:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Kevin Li",
            "Manuel Brack",
            "Sudeep Katakol",
            "Hareesh Ravi",
            "Ajinkya Kale"
        ],
        "tldr": "UniFusion introduces a diffusion-based image generation model using a frozen VLM as a unified encoder with a novel Layerwise Attention Pooling mechanism and prompt rewriting method, VERIFI, to improve cross-modal reasoning and editing capabilities.",
        "tldr_zh": "UniFusion 提出了一种基于扩散的图像生成模型，它使用冻结的 VLM 作为统一编码器，并结合了新的 Layerwise Attention Pooling 机制和提示重写方法 VERIFI，以提高跨模态推理和编辑能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency Consistency",
        "summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based\ndenoisers, enabling the denoising of unseen single images. Nonetheless,\nexisting zero-shot methods suffer from long training times and rely on the\nassumption of noise independence and a zero-mean property, limiting their\neffectiveness in real-world denoising scenarios where noise characteristics are\nmore complicated. This paper proposes an efficient and effective method for\nreal-world denoising, the Zero-Shot denoiser based on Cross-Frequency\nConsistency (ZSCFC), which enables training and denoising with a single noisy\nimage and does not rely on assumptions about noise distribution. Specifically,\nimage textures exhibit position similarity and content consistency across\ndifferent frequency bands, while noise does not. Based on this property, we\ndeveloped cross-frequency consistency loss and an ultralight network to realize\nimage denoising. Experiments on various real-world image datasets demonstrate\nthat our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of\ncomputational efficiency and denoising performance.",
        "url": "http://arxiv.org/abs/2510.12646v1",
        "published_date": "2025-10-14T15:35:59+00:00",
        "updated_date": "2025-10-14T15:35:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanlin Jiang",
            "Yuchen Liu",
            "Mingren Liu"
        ],
        "tldr": "This paper introduces a zero-shot image denoising method, ZSCFC, which leverages cross-frequency consistency to denoise real-world images efficiently without relying on noise distribution assumptions, outperforming existing zero-shot methods.",
        "tldr_zh": "本文提出了一种零样本图像去噪方法ZSCFC，该方法利用跨频率一致性高效地去除真实世界图像的噪声，且不依赖于噪声分布假设，性能优于现有的零样本方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training",
        "summary": "Pixel-space generative models are often more difficult to train and generally\nunderperform compared to their latent-space counterparts, leaving a persistent\nperformance and efficiency gap. In this paper, we introduce a novel two-stage\ntraining framework that closes this gap for pixel-space diffusion and\nconsistency models. In the first stage, we pre-train encoders to capture\nmeaningful semantics from clean images while aligning them with points along\nthe same deterministic sampling trajectory, which evolves points from the prior\nto the data distribution. In the second stage, we integrate the encoder with a\nrandomly initialized decoder and fine-tune the complete model end-to-end for\nboth diffusion and consistency models. Our training framework demonstrates\nstrong empirical performance on ImageNet dataset. Specifically, our diffusion\nmodel reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75\nnumber of function evaluations (NFE), surpassing prior pixel-space methods by a\nlarge margin in both generation quality and efficiency while rivaling leading\nVAE-based models at comparable training cost. Furthermore, on ImageNet-256, our\nconsistency model achieves an impressive FID of 8.82 in a single sampling step,\nsignificantly surpassing its latent-space counterpart. To the best of our\nknowledge, this marks the first successful training of a consistency model\ndirectly on high-resolution images without relying on pre-trained VAEs or\ndiffusion models.",
        "url": "http://arxiv.org/abs/2510.12586v1",
        "published_date": "2025-10-14T14:41:16+00:00",
        "updated_date": "2025-10-14T14:41:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiachen Lei",
            "Keli Liu",
            "Julius Berner",
            "Haiming Yu",
            "Hongkai Zheng",
            "Jiahong Wu",
            "Xiangxiang Chu"
        ],
        "tldr": "This paper introduces a two-stage training framework with self-supervised pre-training to improve the performance and efficiency of pixel-space generative models (diffusion and consistency models), achieving state-of-the-art FID scores on ImageNet.",
        "tldr_zh": "本文提出了一种两阶段训练框架，通过自监督预训练来提升像素空间生成模型（扩散模型和一致性模型）的性能和效率，并在ImageNet上取得了最先进的FID分数。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LayerSync: Self-aligning Intermediate Layers",
        "summary": "We propose LayerSync, a domain-agnostic approach for improving the generation\nquality and the training efficiency of diffusion models. Prior studies have\nhighlighted the connection between the quality of generation and the\nrepresentations learned by diffusion models, showing that external guidance on\nmodel intermediate representations accelerates training. We reconceptualize\nthis paradigm by regularizing diffusion models with their own intermediate\nrepresentations. Building on the observation that representation quality varies\nacross diffusion model layers, we show that the most semantically rich\nrepresentations can act as an intrinsic guidance for weaker ones, reducing the\nneed for external supervision. Our approach, LayerSync, is a self-sufficient,\nplug-and-play regularizer term with no overhead on diffusion model training and\ngeneralizes beyond the visual domain to other modalities. LayerSync requires no\npretrained models nor additional data. We extensively evaluate the method on\nimage generation and demonstrate its applicability to other domains such as\naudio, video, and motion generation. We show that it consistently improves the\ngeneration quality and the training efficiency. For example, we speed up the\ntraining of flow-based transformer by over 8.75x on ImageNet dataset and\nimproved the generation quality by 23.6%. The code is available at\nhttps://github.com/vita-epfl/LayerSync.",
        "url": "http://arxiv.org/abs/2510.12581v1",
        "published_date": "2025-10-14T14:39:14+00:00",
        "updated_date": "2025-10-14T14:39:14+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yasaman Haghighi",
            "Bastien van Delft",
            "Mariam Hassan",
            "Alexandre Alahi"
        ],
        "tldr": "LayerSync is a plug-and-play regularization method that leverages a diffusion model's own semantically rich layers to guide weaker ones, improving generation quality and training efficiency without external data or models. It shows strong results across multiple modalities.",
        "tldr_zh": "LayerSync 是一种即插即用的正则化方法，它利用扩散模型自身语义丰富的层来引导较弱的层，从而提高生成质量和训练效率，无需外部数据或模型。它在多种模态上显示出强大的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model",
        "summary": "This paper introduces a novel framework for image quality transfer based on\nconditional flow matching (CFM). Unlike conventional generative models that\nrely on iterative sampling or adversarial objectives, CFM learns a continuous\nflow between a noise distribution and target data distributions through the\ndirect regression of an optimal velocity field. We evaluate this approach in\nthe context of low-field magnetic resonance imaging (LF-MRI), a rapidly\nemerging modality that offers affordable and portable scanning but suffers from\ninherently low signal-to-noise ratio and reduced diagnostic quality. Our\nframework is designed to reconstruct high-field-like MR images from their\ncorresponding low-field inputs, thereby bridging the quality gap without\nrequiring expensive infrastructure. Experiments demonstrate that CFM not only\nachieves state-of-the-art performance, but also generalizes robustly to both\nin-distribution and out-of-distribution data. Importantly, it does so while\nutilizing significantly fewer parameters than competing deep learning methods.\nThese results underline the potential of CFM as a powerful and scalable tool\nfor MRI reconstruction, particularly in resource-limited clinical environments.",
        "url": "http://arxiv.org/abs/2510.12408v1",
        "published_date": "2025-10-14T11:41:27+00:00",
        "updated_date": "2025-10-14T11:41:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Huu Tien Nguyen",
            "Ahmed Karam Eldaly"
        ],
        "tldr": "This paper introduces a conditional flow matching (CFM) model for enhancing low-field MRI image quality, achieving state-of-the-art performance with fewer parameters and robust generalization.",
        "tldr_zh": "本文介绍了一种基于条件流匹配（CFM）的模型，用于提高低场MRI图像质量，以更少的参数和强大的泛化能力实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BIGFix: Bidirectional Image Generation with Token Fixing",
        "summary": "Recent advances in image and video generation have raised significant\ninterest from both academia and industry. A key challenge in this field is\nimproving inference efficiency, as model size and the number of inference steps\ndirectly impact the commercial viability of generative models while also posing\nfundamental scientific challenges. A promising direction involves combining\nauto-regressive sequential token modeling with multi-token prediction per step,\nreducing inference time by up to an order of magnitude. However, predicting\nmultiple tokens in parallel can introduce structural inconsistencies due to\ntoken incompatibilities, as capturing complex joint dependencies during\ntraining remains challenging. Traditionally, once tokens are sampled, there is\nno mechanism to backtrack and refine erroneous predictions. We propose a method\nfor self-correcting image generation by iteratively refining sampled tokens. We\nachieve this with a novel training scheme that injects random tokens in the\ncontext, improving robustness and enabling token fixing during sampling. Our\nmethod preserves the efficiency benefits of parallel token prediction while\nsignificantly enhancing generation quality. We evaluate our approach on image\ngeneration using the ImageNet-256 and CIFAR-10 datasets, as well as on video\ngeneration with UCF-101 and NuScenes, demonstrating substantial improvements\nacross both modalities.",
        "url": "http://arxiv.org/abs/2510.12231v1",
        "published_date": "2025-10-14T07:34:44+00:00",
        "updated_date": "2025-10-14T07:34:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Victor Besnier",
            "David Hurych",
            "Andrei Bursuc",
            "Eduardo Valle"
        ],
        "tldr": "The paper introduces BIGFix, a method for self-correcting image and video generation that iteratively refines sampled tokens, improving generation quality while maintaining inference efficiency through parallel token prediction. This is achieved by a novel training scheme that injects random tokens to improve robustness.",
        "tldr_zh": "该论文介绍了 BIGFix，一种用于自我修正图像和视频生成的方法，通过迭代地改进采样到的 tokens 来提高生成质量，同时通过并行 token 预测保持推理效率。 这通过一种新颖的训练方案来实现，该方案注入随机 tokens 以提高鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration",
        "summary": "Old-photo face restoration poses significant challenges due to compounded\ndegradations such as breakage, fading, and severe blur. Existing pre-trained\ndiffusion-guided methods either rely on explicit degradation priors or global\nstatistical guidance, which struggle with localized artifacts or face color. We\npropose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages\npseudo-reference faces generated by a pre-trained diffusion model under weak\nguidance. These pseudo-labels exhibit structurally aligned contours and natural\ncolors, enabling region-specific restoration via staged supervision: structural\nguidance applied throughout the denoising process and color refinement in later\nsteps, aligned with the coarse-to-fine nature of diffusion. By incorporating\nface parsing maps and scratch masks, our method selectively restores breakage\nregions while avoiding identity mismatch. We further construct VintageFace, a\n300-image benchmark of real old face photos with varying degradation levels.\nSSDiff outperforms existing GAN-based and diffusion-based methods in perceptual\nquality, fidelity, and regional controllability. Code link:\nhttps://github.com/PRIS-CV/SSDiff.",
        "url": "http://arxiv.org/abs/2510.12114v1",
        "published_date": "2025-10-14T03:34:15+00:00",
        "updated_date": "2025-10-14T03:34:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjie Li",
            "Xiangyi Wang",
            "Heng Guo",
            "Guangwei Gao",
            "Zhanyu Ma"
        ],
        "tldr": "The paper proposes a self-supervised selective-guided diffusion model (SSDiff) for old-photo face restoration, using pseudo-reference faces for region-specific restoration and a new benchmark dataset (VintageFace). It claims to outperform existing methods.",
        "tldr_zh": "该论文提出了一种自监督选择引导扩散模型 (SSDiff) 用于老照片面部修复，使用伪参考面进行区域特定修复，并提供了一个新的基准数据集 (VintageFace)。它声称优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring",
        "summary": "Unlike general image deblurring that prioritizes perceptual quality, QR code\ndeblurring focuses on ensuring successful decoding. QR codes are characterized\nby highly structured patterns with sharp edges, a robust prior for restoration.\nYet existing deep learning methods rarely exploit these priors explicitly. To\naddress this gap, we propose the Edge-Guided Attention Block (EGAB), which\nembeds explicit edge priors into a Transformer architecture. Based on EGAB, we\ndevelop Edge-Guided Restormer (EG-Restormer), an effective network that\nsignificantly boosts the decoding rate of severely blurred QR codes. For mildly\nblurred inputs, we design the Lightweight and Efficient Network (LENet) for\nfast deblurring. We further integrate these two networks into an Adaptive\nDual-network (ADNet), which dynamically selects the suitable network based on\ninput blur severity, making it ideal for resource-constrained mobile devices.\nExtensive experiments show that our EG-Restormer and ADNet achieve\nstate-of-the-art performance with a competitive speed. Project page:\nhttps://github.com/leejianping/ADNet",
        "url": "http://arxiv.org/abs/2510.12098v1",
        "published_date": "2025-10-14T03:03:47+00:00",
        "updated_date": "2025-10-14T03:03:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianping Li",
            "Dongyang Guo",
            "Wenjie Li",
            "Wei Zhao"
        ],
        "tldr": "This paper introduces an adaptive dual-network framework (ADNet) for QR code deblurring, leveraging edge priors with an edge-guided attention block (EGAB) and a lightweight network (LENet) to achieve state-of-the-art performance on resource-constrained devices.",
        "tldr_zh": "本文提出了一种自适应双网络框架（ADNet）用于QR码去模糊，利用边缘先验知识通过边缘引导注意力模块（EGAB）和一个轻量级网络（LENet）在资源受限设备上实现最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier",
        "summary": "Generative classifiers, which leverage conditional generative models for\nclassification, have recently demonstrated desirable properties such as\nrobustness to distribution shifts. However, recent progress in this area has\nbeen largely driven by diffusion-based models, whose substantial computational\ncost severely limits scalability. This exclusive focus on diffusion-based\nmethods has also constrained our understanding of generative classifiers. In\nthis work, we propose a novel generative classifier built on recent advances in\nvisual autoregressive (VAR) modeling, which offers a new perspective for\nstudying generative classifiers. To further enhance its performance, we\nintroduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a\nsuperior trade-off between accuracy and inference speed, thereby significantly\nimproving practical applicability. Moreover, we show that the VAR-based method\nexhibits fundamentally different properties from diffusion-based methods. In\nparticular, due to its tractable likelihood, the VAR-based classifier enables\nvisual explainability via token-wise mutual information and demonstrates\ninherent resistance to catastrophic forgetting in class-incremental learning\ntasks.",
        "url": "http://arxiv.org/abs/2510.12060v1",
        "published_date": "2025-10-14T01:59:01+00:00",
        "updated_date": "2025-10-14T01:59:01+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yi-Chung Chen",
            "David I. Inouye",
            "Jing Gao"
        ],
        "tldr": "This paper introduces a novel generative classifier, Adaptive VAR Classifier$^+$ (A-VARC$^+$), based on visual autoregressive (VAR) modeling, that offers improved accuracy, inference speed, explainability, and resistance to catastrophic forgetting compared to diffusion-based methods.",
        "tldr_zh": "本文介绍了一种新的生成分类器，即基于视觉自回归（VAR）建模的自适应VAR分类器$^+$（A-VARC$^+$），与基于扩散的方法相比，它提供了更高的准确性、推理速度、可解释性以及对灾难性遗忘的抵抗力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]