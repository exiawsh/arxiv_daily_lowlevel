[
    {
        "title": "FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA",
        "summary": "Recent advances in diffusion-based generative models have demonstrated\nsignificant potential in augmenting scarce datasets for object detection tasks.\nNevertheless, most recent models rely on resource-intensive full fine-tuning of\nlarge-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA\nV100) and thousands of synthetic images. To address these limitations, we\npropose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation\npipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned\nexclusively through Low-Rank Adaptation (LoRA). This dramatically reduces\ncomputational requirements, enabling synthetic dataset generation with a\nconsumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our\napproach on seven diverse object detection datasets. Our results demonstrate\nthat training object detectors with just 500 synthetic images generated by our\napproach yields superior detection performance compared to models trained on\n5000 synthetic images from the ODGEN baseline, achieving improvements of up to\n21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass\nstate-of-the-art performance with far greater efficiency, as FLORA achieves\nsuperior results using only 10% of the data and a fraction of the computational\ncost. This work demonstrates that a quality and efficiency-focused approach is\nmore effective than brute-force generation, making advanced synthetic data\ncreation more practical and accessible for real-world scenarios.",
        "url": "http://arxiv.org/abs/2508.21712v1",
        "published_date": "2025-08-29T15:29:06+00:00",
        "updated_date": "2025-08-29T15:29:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alvaro Patricio",
            "Atabak Dehban",
            "Rodrigo Ventura"
        ],
        "tldr": "The paper introduces FLORA, a LoRA-based fine-tuning approach for diffusion models, enabling efficient synthetic data generation for object detection in low-data regimes, achieving superior performance with significantly reduced computational resources compared to full fine-tuning methods.",
        "tldr_zh": "该论文介绍了FLORA，一种基于LoRA微调的扩散模型方法，可以在低数据情况下高效生成用于目标检测的合成数据，与完全微调方法相比，以显著减少的计算资源实现了卓越的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models",
        "summary": "Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.",
        "url": "http://arxiv.org/abs/2508.21542v1",
        "published_date": "2025-08-29T11:55:47+00:00",
        "updated_date": "2025-08-29T11:55:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Ziwei Liao",
            "Mohamed Sayed",
            "Steven L. Waslander",
            "Sara Vicente",
            "Daniyar Turmukhambetov",
            "Michael Firman"
        ],
        "tldr": "This paper introduces a diffusion model-based approach to generate complete 3D Gaussian splat representations from single images, effectively handling occlusions and unobserved regions through a generative framework and self-supervised training.",
        "tldr_zh": "本文提出了一种基于扩散模型的方法，从单张图像生成完整的3D高斯溅射表示，通过生成式框架和自监督训练有效地处理遮挡和未观察到的区域。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]