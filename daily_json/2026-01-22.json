[
    {
        "title": "Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD",
        "summary": "Traditional data masking techniques such as anonymization cannot achieve the expected privacy protection while ensuring data utility for privacy-preserving machine learning. Synthetic data plays an increasingly important role as it generates a large number of training samples and prevents information leakage in real data. The existing methods suffer from the repeating trade-off processes between privacy and utility. We propose a novel framework for differential privacy generation, which employs an Error Feedback Stochastic Gradient Descent(EFSGD) method and introduces a reconstruction loss and noise injection mechanism into the training process. We generate images with higher quality and usability under the same privacy budget as the related work. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both grayscale and RGB images. We achieve state-of-the-art results over almost all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.",
        "url": "http://arxiv.org/abs/2601.15061v1",
        "published_date": "2026-01-21T15:07:33+00:00",
        "updated_date": "2026-01-21T15:07:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Qiwei Ma",
            "Jun Zhang"
        ],
        "tldr": "This paper introduces a novel differentially private image generation framework using Error Feedback SGD, reconstruction loss, and noise injection, achieving state-of-the-art results on MNIST, Fashion-MNIST, and CelebA.",
        "tldr_zh": "本文提出了一种新的差分隐私图像生成框架，该框架使用误差反馈SGD、重构损失和噪声注入，并在MNIST、Fashion-MNIST和CelebA上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Text-to-Image Generation via End-Edge Collaborative Hybrid Super-Resolution",
        "summary": "Artificial Intelligence-Generated Content (AIGC) has made significant strides, with high-resolution text-to-image (T2I) generation becoming increasingly critical for improving users' Quality of Experience (QoE). Although resource-constrained edge computing adequately supports fast low-resolution T2I generations, achieving high-resolution output still faces the challenge of ensuring image fidelity at the cost of latency. To address this, we first investigate the performance of super-resolution (SR) methods for image enhancement, confirming a fundamental trade-off that lightweight learning-based SR struggles to recover fine details, while diffusion-based SR achieves higher fidelity at a substantial computational cost. Motivated by these observations, we propose an end-edge collaborative generation-enhancement framework. Upon receiving a T2I generation task, the system first generates a low-resolution image based on adaptively selected denoising steps and super-resolution scales at the edge side, which is then partitioned into patches and processed by a region-aware hybrid SR policy. This policy applies a diffusion-based SR model to foreground patches for detail recovery and a lightweight learning-based SR model to background patches for efficient upscaling, ultimately stitching the enhanced ones into the high-resolution image. Experiments show that our system reduces service latency by 33% compared with baselines while maintaining competitive image quality.",
        "url": "http://arxiv.org/abs/2601.14741v1",
        "published_date": "2026-01-21T07:55:37+00:00",
        "updated_date": "2026-01-21T07:55:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chongbin Yi",
            "Yuxin Liang",
            "Ziqi Zhou",
            "Peng Yang"
        ],
        "tldr": "The paper proposes an end-edge collaborative framework for high-resolution text-to-image generation, using a hybrid super-resolution policy that combines diffusion-based and learning-based SR models to balance image quality and latency.",
        "tldr_zh": "该论文提出了一种用于高分辨率文本到图像生成的端边协同框架，采用混合超分辨率策略，结合基于扩散和基于学习的SR模型，以平衡图像质量和延迟。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "3D Space as a Scratchpad for Editable Text-to-Image Generation",
        "summary": "Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/",
        "url": "http://arxiv.org/abs/2601.14602v1",
        "published_date": "2026-01-21T02:40:19+00:00",
        "updated_date": "2026-01-21T02:40:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Oindrila Saha",
            "Vojtech Krs",
            "Radomir Mech",
            "Subhransu Maji",
            "Matheus Gadelha",
            "Kevin Blackburn-Matzen"
        ],
        "tldr": "This paper introduces a 3D spatial scratchpad for text-to-image generation, allowing for explicit 3D reasoning and editing, leading to improved text alignment and spatial consistency compared to 2D layout methods.",
        "tldr_zh": "本文介绍了一种用于文本到图像生成的3D空间草稿板，它允许显式的3D推理和编辑，与2D布局方法相比，提高了文本对齐和空间一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Soft Tail-dropping for Adaptive Visual Tokenization",
        "summary": "We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.",
        "url": "http://arxiv.org/abs/2601.14246v1",
        "published_date": "2026-01-20T18:57:19+00:00",
        "updated_date": "2026-01-20T18:57:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyuan Chen",
            "Kai Zhang",
            "Zhuowen Tu",
            "Yuanjun Xiong"
        ],
        "tldr": "The paper introduces STAT, a novel 1D visual tokenizer that adaptively generates a variable number of tokens per image based on its complexity, enabling efficient and high-quality autoregressive image generation.",
        "tldr_zh": "该论文介绍了STAT，一种新型的1D视觉标记器，它可以根据图像的复杂性自适应地生成可变数量的标记，从而实现高效且高质量的自回归图像生成。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation",
        "summary": "Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2601.15221v1",
        "published_date": "2026-01-21T17:53:21+00:00",
        "updated_date": "2026-01-21T17:53:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanlei Guo",
            "Jiahao Shao",
            "Xinya Chen",
            "Xiyang Tan",
            "Sheng Miao",
            "Yujun Shen",
            "Yiyi Liao"
        ],
        "tldr": "The paper introduces ScenDi, a novel urban scene generation method that combines 3D latent diffusion for coarse scene creation and 2D video diffusion for detail enhancement, achieving camera controllability and realism on datasets like Waymo and KITTI-360.",
        "tldr_zh": "该论文介绍了ScenDi，一种新颖的城市场景生成方法，它结合了用于粗略场景创建的3D潜在扩散和用于细节增强的2D视频扩散，从而在Waymo和KITTI-360等数据集上实现了相机可控性和真实感。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]