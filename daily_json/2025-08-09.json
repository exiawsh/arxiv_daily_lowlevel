[
    {
        "title": "Multivariate Fields of Experts",
        "summary": "We introduce the multivariate fields of experts, a new framework for the\nlearning of image priors. Our model generalizes existing fields of experts\nmethods by incorporating multivariate potential functions constructed via\nMoreau envelopes of the $\\ell_\\infty$-norm. We demonstrate the effectiveness of\nour proposal across a range of inverse problems that include image denoising,\ndeblurring, compressed-sensing magnetic-resonance imaging, and computed\ntomography. The proposed approach outperforms comparable univariate models and\nachieves performance close to that of deep-learning-based regularizers while\nbeing significantly faster, requiring fewer parameters, and being trained on\nsubstantially fewer data. In addition, our model retains a relatively high\nlevel of interpretability due to its structured design.",
        "url": "http://arxiv.org/abs/2508.06490v1",
        "published_date": "2025-08-08T17:58:25+00:00",
        "updated_date": "2025-08-08T17:58:25+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "eess.SP"
        ],
        "authors": [
            "Stanislas Ducotterd",
            "Michael Unser"
        ],
        "tldr": "The paper introduces a new image prior learning framework called Multivariate Fields of Experts, which generalizes existing methods using multivariate potential functions and demonstrates effectiveness across various inverse problems, achieving performance close to deep learning with advantages in speed, parameters, and interpretability.",
        "tldr_zh": "该论文介绍了一种新的图像先验学习框架，名为多元专家场，它通过使用多元势函数推广了现有方法，并在各种逆问题中展示了有效性，其性能接近深度学习，但在速度、参数和可解释性方面具有优势。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery",
        "summary": "High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy.",
        "url": "http://arxiv.org/abs/2508.06407v1",
        "published_date": "2025-08-08T15:50:40+00:00",
        "updated_date": "2025-08-08T15:50:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "eess.IV"
        ],
        "authors": [
            "Ch Muhammad Awais",
            "Marco Reggiannini",
            "Davide Moroni",
            "Oktay Karakus"
        ],
        "tldr": "This paper proposes a novel super-resolution method for SAR imagery that incorporates classification objectives directly into the super-resolution process, aiming to improve both image quality and classification accuracy.",
        "tldr_zh": "本文提出了一种新的SAR图像超分辨率方法，该方法将分类目标直接融入超分辨率过程，旨在提高图像质量和分类精度。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?",
        "summary": "Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain\nshift due to variations in imaging devices and acquisition protocols. This\nchallenge limits the deployment of trained AI models in real-world scenarios,\nwhere performance degrades on unseen domains. Traditional solutions involve\nincreasing the size of the dataset through ad-hoc image augmentation or\nadditional online training/transfer learning, which have several limitations.\nSynthetic data offers a promising alternative, but anatomical/structural\nconsistency constraints limit the effectiveness of generative models in\ncreating image-label pairs. To address this, we propose a diffusion model (DM)\ntrained on a source domain that generates synthetic cardiac MR images that\nresemble a given reference. The synthetic data maintains spatial and structural\nfidelity, ensuring similarity to the source domain and compatibility with the\nsegmentation mask. We assess the utility of our generative approach in\nmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and\nvanilla U-Net segmentation networks. We explore domain generalisation, where,\ndomain-invariant segmentation models are trained on synthetic source domain\ndata, and domain adaptation, where, we shift target domain data towards the\nsource domain using the DM. Both strategies significantly improved segmentation\nperformance on data from an unseen target domain, in terms of surface-based\nmetrics (Welch's t-test, p < 0.01), compared to training segmentation models on\nreal data alone. The proposed method ameliorates the need for transfer learning\nor online training to address domain shift challenges in cardiac MR image\nanalysis, especially useful in data-scarce settings.",
        "url": "http://arxiv.org/abs/2508.06327v1",
        "published_date": "2025-08-08T13:57:48+00:00",
        "updated_date": "2025-08-08T13:57:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Ci Wong",
            "Duygu Sarikaya",
            "Kieran Zucker",
            "Marc De Kamps",
            "Nishant Ravikumar"
        ],
        "tldr": "This paper proposes using a diffusion model to generate synthetic cardiac MR images to bridge the domain gap in multi-center cardiac MR segmentation, showing significant improvement in segmentation performance on unseen target domains compared to training on real data alone.",
        "tldr_zh": "本文提出了一种使用扩散模型生成合成心脏磁共振图像的方法，以弥合多中心心脏磁共振分割中的领域差距。与仅在真实数据上训练相比，该方法在未见过的目标域上的分割性能有了显著提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder",
        "summary": "Image deblurring, removing blurring artifacts from images, is a fundamental\ntask in computational photography and low-level computer vision. Existing\napproaches focus on specialized solutions tailored to particular blur types,\nthus, these solutions lack generalization. This limitation in current methods\nimplies requiring multiple models to cover several blur types, which is not\npractical in many real scenarios. In this paper, we introduce the first\nall-in-one deblurring method capable of efficiently restoring images affected\nby diverse blur degradations, including global motion, local motion, blur in\nlow-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)\ndecoding module, which dynamically routes image features based on the\nrecognized blur degradation, enabling precise and efficient restoration in an\nend-to-end manner. Our unified approach not only achieves performance\ncomparable to dedicated task-specific models, but also demonstrates remarkable\nrobustness and generalization capabilities on unseen blur degradation\nscenarios.",
        "url": "http://arxiv.org/abs/2508.06228v1",
        "published_date": "2025-08-08T11:15:45+00:00",
        "updated_date": "2025-08-08T11:15:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Feijoo",
            "Paula Garrido-Mellado",
            "Jaesung Rim",
            "Alvaro Garcia",
            "Marcos V. Conde"
        ],
        "tldr": "This paper introduces a unified image deblurring method using a Mixture-of-Experts decoder, capable of handling diverse blur types with comparable performance to specialized models and improved generalization.",
        "tldr_zh": "本文介绍了一种使用混合专家解码器的统一图像去模糊方法，该方法能够处理多种模糊类型，其性能与专用模型相当，并具有更好的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Clinically-guided Data Synthesis for Laryngeal Lesion Detection",
        "summary": "Although computer-aided diagnosis (CADx) and detection (CADe) systems have\nmade significant progress in various medical domains, their application is\nstill limited in specialized fields such as otorhinolaryngology. In the latter,\ncurrent assessment methods heavily depend on operator expertise, and the high\nheterogeneity of lesions complicates diagnosis, with biopsy persisting as the\ngold standard despite its substantial costs and risks. A critical bottleneck\nfor specialized endoscopic CADx/e systems is the lack of well-annotated\ndatasets with sufficient variability for real-world generalization. This study\nintroduces a novel approach that exploits a Latent Diffusion Model (LDM)\ncoupled with a ControlNet adapter to generate laryngeal endoscopic\nimage-annotation pairs, guided by clinical observations. The method addresses\ndata scarcity by conditioning the diffusion process to produce realistic,\nhigh-quality, and clinically relevant image features that capture diverse\nanatomical conditions. The proposed approach can be leveraged to expand\ntraining datasets for CADx/e models, empowering the assessment process in\nlaryngology. Indeed, during a downstream task of detection, the addition of\nonly 10% synthetic data improved the detection rate of laryngeal lesions by 9%\nwhen the model was internally tested and 22.1% on out-of-domain external data.\nAdditionally, the realism of the generated images was evaluated by asking 5\nexpert otorhinolaryngologists with varying expertise to rate their confidence\nin distinguishing synthetic from real images. This work has the potential to\naccelerate the development of automated tools for laryngeal disease diagnosis,\noffering a solution to data scarcity and demonstrating the applicability of\nsynthetic data in real-world scenarios.",
        "url": "http://arxiv.org/abs/2508.06182v1",
        "published_date": "2025-08-08T09:55:54+00:00",
        "updated_date": "2025-08-08T09:55:54+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Chiara Baldini",
            "Kaisar Kushibar",
            "Richard Osuala",
            "Simone Balocco",
            "Oliver Diaz",
            "Karim Lekadir",
            "Leonardo S. Mattos"
        ],
        "tldr": "This paper introduces a clinically-guided Latent Diffusion Model (LDM) with a ControlNet adapter to generate synthetic laryngeal endoscopic images for training CADx/e systems, showing improved lesion detection rates and high realism as evaluated by expert otorhinolaryngologists.",
        "tldr_zh": "本文介绍了一种临床指导的潜在扩散模型 (LDM)，带有 ControlNet 适配器，用于生成合成的喉内窥镜图像，以训练 CADx/e 系统。结果表明，该方法提高了病灶检测率，并且经专家耳鼻喉科医生评估，具有很高的真实感。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment",
        "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
        "url": "http://arxiv.org/abs/2508.06160v1",
        "published_date": "2025-08-08T09:29:37+00:00",
        "updated_date": "2025-08-08T09:29:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenbang Du",
            "Yonggan Fu",
            "Lifu Wang",
            "Jiayi Qian",
            "Xiao Luo",
            "Yingyan",
            "Lin"
        ],
        "tldr": "This paper introduces PostDiff, a training-free framework for accelerating diffusion models by optimizing the trade-off between denoising steps and per-step inference cost. The method involves mixed-resolution denoising and hybrid module caching to improve efficiency while maintaining fidelity.",
        "tldr_zh": "本文介绍了PostDiff，一个无需训练的框架，通过优化去噪步骤和每步推理成本之间的权衡来加速扩散模型。该方法涉及混合分辨率去噪和混合模块缓存，以提高效率并保持保真度。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models",
        "summary": "In oral cancer diagnostics, the limited availability of annotated datasets\nfrequently constrains the performance of diagnostic models, particularly due to\nthe variability and insufficiency of training data. To address these\nchallenges, this study proposed a novel approach to enhance diagnostic accuracy\nby synthesizing realistic oral cancer lesions using an inpainting technique\nwith a fine-tuned diffusion model. We compiled a comprehensive dataset from\nmultiple sources, featuring a variety of oral cancer images. Our method\ngenerated synthetic lesions that exhibit a high degree of visual fidelity to\nactual lesions, thereby significantly enhancing the performance of diagnostic\nalgorithms. The results show that our classification model achieved a\ndiagnostic accuracy of 0.97 in differentiating between cancerous and\nnon-cancerous tissues, while our detection model accurately identified lesion\nlocations with 0.85 accuracy. This method validates the potential for synthetic\nimage generation in medical diagnostics and paves the way for further research\ninto extending these methods to other types of cancer diagnostics.",
        "url": "http://arxiv.org/abs/2508.06151v1",
        "published_date": "2025-08-08T09:15:02+00:00",
        "updated_date": "2025-08-08T09:15:02+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Yong Oh Lee",
            "JeeEun Kim",
            "Jung Woo Lee"
        ],
        "tldr": "This paper introduces a diffusion model-based inpainting technique to generate synthetic oral cancer lesions, improving the accuracy of diagnostic models for oral cancer detection and classification.",
        "tldr_zh": "本文介绍了一种基于扩散模型的图像修复技术，用于生成合成的口腔癌病灶，从而提高口腔癌检测和分类诊断模型的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment",
        "summary": "Diffusion-based or flow-based models have achieved significant progress in\nvideo synthesis but require multiple iterative sampling steps, which incurs\nsubstantial computational overhead. While many distillation methods that are\nsolely based on trajectory-preserving or distribution-matching have been\ndeveloped to accelerate video generation models, these approaches often suffer\nfrom performance breakdown or increased artifacts under few-step settings. To\naddress these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and\nstable distillation framework that combines the advantages of\ntrajectory-preserving and distribution-matching strategies. Our approach\nintroduces continuous-time consistency distillation to ensure precise\npreservation of ODE trajectories. Subsequently, we propose a dual-perspective\nalignment that includes distribution alignment between synthetic and real data\nalong with trajectory alignment across different inference steps. Our method\nmaintains high-quality video generation while substantially reducing the number\nof inference steps. Quantitative evaluations on the OpenVid-1M benchmark\ndemonstrate that our method significantly outperforms existing approaches in\nfew-step video generation.",
        "url": "http://arxiv.org/abs/2508.06082v1",
        "published_date": "2025-08-08T07:26:34+00:00",
        "updated_date": "2025-08-08T07:26:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanxiao Sun",
            "Jiafu Wu",
            "Yun Cao",
            "Chengming Xu",
            "Yabiao Wang",
            "Weijian Cao",
            "Donghao Luo",
            "Chengjie Wang",
            "Yanwei Fu"
        ],
        "tldr": "SwiftVideo is proposed as a unified distillation framework that combines trajectory-preserving and distribution-matching strategies for few-step video generation, achieving state-of-the-art results on OpenVid-1M.",
        "tldr_zh": "SwiftVideo 提出了一个统一的蒸馏框架，结合了轨迹保持和分布匹配策略，用于少步视频生成，在 OpenVid-1M 上取得了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis",
        "summary": "Sub-visible particle analysis using flow imaging microscopy combined with\ndeep learning has proven effective in identifying particle types, enabling the\ndistinction of harmless components such as silicone oil from protein particles.\nHowever, the scarcity of available data and severe imbalance between particle\ntypes within datasets remain substantial hurdles when applying multi-class\nclassifiers to such problems, often forcing researchers to rely on less\neffective methods. The aforementioned issue is particularly challenging for\nparticle types that appear unintentionally and in lower numbers, such as\nsilicone oil and air bubbles, as opposed to protein particles, where obtaining\nlarge numbers of images through controlled settings is comparatively\nstraightforward. In this work, we develop a state-of-the-art diffusion model to\naddress data imbalance by generating high-fidelity images that can augment\ntraining datasets, enabling the effective training of multi-class deep neural\nnetworks. We validate this approach by demonstrating that the generated samples\nclosely resemble real particle images in terms of visual quality and structure.\nTo assess the effectiveness of using diffusion-generated images in training\ndatasets, we conduct large-scale experiments on a validation dataset comprising\n500,000 protein particle images and demonstrate that this approach improves\nclassification performance with no negligible downside. Finally, to promote\nopen research and reproducibility, we publicly release both our diffusion\nmodels and the trained multi-class deep neural network classifiers, along with\na straightforward interface for easy integration into future studies, at\nhttps://github.com/utkuozbulak/svp-generative-ai.",
        "url": "http://arxiv.org/abs/2508.06021v1",
        "published_date": "2025-08-08T05:15:02+00:00",
        "updated_date": "2025-08-08T05:15:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Utku Ozbulak",
            "Michaela Cohrs",
            "Hristo L. Svilenov",
            "Joris Vankerschaver",
            "Wesley De Neve"
        ],
        "tldr": "The paper addresses data imbalance in flow imaging microscopy for sub-visible particle classification by using a diffusion model to generate synthetic training data, improving the performance of multi-class classifiers. The models and code are publicly available.",
        "tldr_zh": "本文通过使用扩散模型生成合成训练数据，解决了流动成像显微镜中亚可见颗粒分类的数据不平衡问题，从而提高了多类分类器的性能。模型和代码已公开发布。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents",
        "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.",
        "url": "http://arxiv.org/abs/2508.05954v1",
        "published_date": "2025-08-08T02:38:47+00:00",
        "updated_date": "2025-08-08T02:38:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Han Lin",
            "Jaemin Cho",
            "Amir Zadeh",
            "Chuan Li",
            "Mohit Bansal"
        ],
        "tldr": "Bifrost-1 bridges pre-trained multimodal LLMs and diffusion models using patch-level CLIP embeddings, achieving efficient high-fidelity controllable image generation while preserving LLM reasoning capabilities. It offers a computationally cheaper alternative to training LLMs on image data.",
        "tldr_zh": "Bifrost-1 使用 patch-level CLIP 嵌入连接预训练的多模态 LLM 和扩散模型，在保留 LLM 推理能力的同时，实现了高效的高保真可控图像生成。它为训练 LLM 处理图像数据提供了一种计算成本更低的替代方案。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss",
        "summary": "Medical image synthesis is an important topic for both clinical and research\napplications. Recently, diffusion models have become a leading approach in this\narea. Despite their strengths, many existing methods struggle with (1) limited\ngeneralizability that only work for specific body regions or voxel spacings,\n(2) slow inference, which is a common issue for diffusion models, and (3) weak\nalignment with input conditions, which is a critical issue for medical imaging.\nMAISI, a previously proposed framework, addresses generalizability issues but\nstill suffers from slow inference and limited condition consistency. In this\nwork, we present MAISI-v2, the first accelerated 3D medical image synthesis\nframework that integrates rectified flow to enable fast and high quality\ngeneration. To further enhance condition fidelity, we introduce a novel\nregion-specific contrastive loss to enhance the sensitivity to region of\ninterest. Our experiments show that MAISI-v2 can achieve SOTA image quality\nwith $33 \\times$ acceleration for latent diffusion model. We also conducted a\ndownstream segmentation experiment to show that the synthetic images can be\nused for data augmentation. We release our code, training details, model\nweights, and a GUI demo to facilitate reproducibility and promote further\ndevelopment within the community.",
        "url": "http://arxiv.org/abs/2508.05772v1",
        "published_date": "2025-08-07T18:39:45+00:00",
        "updated_date": "2025-08-07T18:39:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Can Zhao",
            "Pengfei Guo",
            "Dong Yang",
            "Yucheng Tang",
            "Yufan He",
            "Benjamin Simon",
            "Mason Belue",
            "Stephanie Harmon",
            "Baris Turkbey",
            "Daguang Xu"
        ],
        "tldr": "MAISI-v2 accelerates 3D medical image synthesis using rectified flow and region-specific contrastive loss, achieving significant speedups and improved condition fidelity compared to previous diffusion models.",
        "tldr_zh": "MAISI-v2 使用修正流和特定区域对比损失加速了 3D 医学图像合成，与之前的扩散模型相比，实现了显著的加速和改进的条件保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion",
        "summary": "Urbanization, climate change, and agricultural stress are increasing the\ndemand for precise and timely environmental monitoring. Land Surface\nTemperature (LST) is a key variable in this context and is retrieved from\nremote sensing satellites. However, these systems face a trade-off between\nspatial and temporal resolution. While spatio-temporal fusion methods offer\npromising solutions, few have addressed the estimation of daily LST at 10 m\nresolution. In this study, we present WGAST, a Weakly-Supervised Generative\nNetwork for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra\nMODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning\nframework designed for this task. It adopts a conditional generative\nadversarial architecture, with a generator composed of four stages: feature\nextraction, fusion, LST reconstruction, and noise suppression. The first stage\nemploys a set of encoders to extract multi-level latent representations from\nthe inputs, which are then fused in the second stage using cosine similarity,\nnormalization, and temporal attention mechanisms. The third stage decodes the\nfused features into high-resolution LST, followed by a Gaussian filter to\nsuppress high-frequency noise. Training follows a weakly supervised strategy\nbased on physical averaging principles and reinforced by a PatchGAN\ndiscriminator. Experiments demonstrate that WGAST outperforms existing methods\nin both quantitative and qualitative evaluations. Compared to the\nbest-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves\nSSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and\neffectively captures fine-scale thermal patterns, as validated against 33\nground-based sensors. The code is available at\nhttps://github.com/Sofianebouaziz1/WGAST.git.",
        "url": "http://arxiv.org/abs/2508.06485v1",
        "published_date": "2025-08-08T17:49:46+00:00",
        "updated_date": "2025-08-08T17:49:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sofiane Bouaziz",
            "Adel Hafiane",
            "Raphael Canals",
            "Rachid Nedjai"
        ],
        "tldr": "The paper presents WGAST, a weakly-supervised generative network that fuses Terra MODIS, Landsat 8, and Sentinel-2 data to estimate daily 10m Land Surface Temperature (LST), showing improved accuracy and robustness compared to existing methods.",
        "tldr_zh": "该论文提出了WGAST，一个弱监督生成网络，融合Terra MODIS、Landsat 8和Sentinel-2数据，用于估计每日10米分辨率的地表温度(LST)，与现有方法相比，显示出更高的准确性和鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow",
        "summary": "We propose a fast text-guided image editing method called InstantEdit based\non the RectifiedFlow framework, which is structured as a few-step editing\nprocess that preserves critical content while following closely to textual\ninstructions. Our approach leverages the straight sampling trajectories of\nRectifiedFlow by introducing a specialized inversion strategy called PerRFI. To\nmaintain consistent while editable results for RectifiedFlow model, we further\npropose a novel regeneration method, Inversion Latent Injection, which\neffectively reuses latent information obtained during inversion to facilitate\nmore coherent and detailed regeneration. Additionally, we propose a\nDisentangled Prompt Guidance technique to balance editability with detail\npreservation, and integrate a Canny-conditioned ControlNet to incorporate\nstructural cues and suppress artifacts. Evaluation on the PIE image editing\ndataset demonstrates that InstantEdit is not only fast but also achieves better\nqualitative and quantitative results compared to state-of-the-art few-step\nediting methods.",
        "url": "http://arxiv.org/abs/2508.06033v1",
        "published_date": "2025-08-08T05:38:17+00:00",
        "updated_date": "2025-08-08T05:38:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Gong",
            "Zhen Zhu",
            "Minjia Zhang"
        ],
        "tldr": "InstantEdit is a novel text-guided image editing method based on RectifiedFlow, offering fast, few-step editing with improved content preservation and editability via specialized inversion and regeneration techniques.",
        "tldr_zh": "InstantEdit 是一种新的基于 RectifiedFlow 的文本引导图像编辑方法，通过专门的反演和再生技术，提供快速、少步骤的编辑，并改善了内容保留和可编辑性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]