[
    {
        "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition",
        "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.",
        "url": "http://arxiv.org/abs/2507.14686v2",
        "published_date": "2025-07-19T16:29:02+00:00",
        "updated_date": "2025-07-29T16:42:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Cai",
            "Tianyi Liu",
            "Jianjun Gao",
            "Wenyang Liu",
            "Kejun Wu",
            "Ruoyu Wang",
            "Yi Wang",
            "Soo Chin Liew"
        ],
        "tldr": "This paper introduces Multimodal Interactive Prompt Distillation (MIPD) to transfer knowledge from a large MLLM to a smaller GSR model for improved open-vocabulary grounded situation recognition, enhancing generalization and zero-shot abilities in unseen and rare situations. It achieves superior performance on the Ov-SWiG dataset.",
        "tldr_zh": "本文介绍了一种多模态交互式提示蒸馏 (MIPD) 方法，用于将知识从大型 MLLM 迁移到较小的 GSR 模型，以改进开放词汇的 grounded situation recognition，从而增强在未见和罕见情况下的泛化能力和零样本能力。在 Ov-SWiG 数据集上实现了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis",
        "summary": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel\ntissue analysis across various pathological tasks. While recent advancements in\nmulti-modal large language models (MLLMs) allow multi-task WSI analysis through\nnatural language, they often underperform compared to task-specific models.\nCollaborative multi-agent systems have emerged as a promising solution to\nbalance versatility and accuracy in healthcare, yet their potential remains\nunderexplored in pathology-specific domains. To address these issues, we\npropose WSI-Agents, a novel collaborative multi-agent system for multi-modal\nWSI analysis. WSI-Agents integrates specialized functional agents with robust\ntask allocation and verification mechanisms to enhance both task-specific\naccuracy and multi-task versatility through three components: (1) a task\nallocation module assigning tasks to expert agents using a model zoo of patch\nand WSI level MLLMs, (2) a verification mechanism ensuring accuracy through\ninternal consistency checks and external validation using pathology knowledge\nbases and domain-specific models, and (3) a summary module synthesizing the\nfinal summary with visual interpretation maps. Extensive experiments on\nmulti-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs\nand medical agent frameworks across diverse tasks.",
        "url": "http://arxiv.org/abs/2507.14680v1",
        "published_date": "2025-07-19T16:11:03+00:00",
        "updated_date": "2025-07-19T16:11:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T07, 92C55",
            "I.2.7; I.4.8; J.3"
        ],
        "authors": [
            "Xinheng Lyu",
            "Yuci Liang",
            "Wenting Chen",
            "Meidan Ding",
            "Jiaqi Yang",
            "Guolin Huang",
            "Daokun Zhang",
            "Xiangjian He",
            "Linlin Shen"
        ],
        "tldr": "The paper introduces WSI-Agents, a collaborative multi-agent system for multi-modal whole slide image analysis, which integrates task allocation, verification, and summary modules to improve accuracy and versatility compared to existing WSI MLLMs.",
        "tldr_zh": "该论文介绍了 WSI-Agents，一个用于多模态全切片图像分析的协作式多智能体系统。该系统集成了任务分配、验证和总结模块，与现有的 WSI MLLM 相比，提高了准确性和通用性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
        "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot",
        "url": "http://arxiv.org/abs/2507.14675v1",
        "published_date": "2025-07-19T16:03:34+00:00",
        "updated_date": "2025-07-19T16:03:34+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yuchen Duan",
            "Zhe Chen",
            "Yusong Hu",
            "Weiyun Wang",
            "Shenglong Ye",
            "Botian Shi",
            "Lewei Lu",
            "Qibin Hou",
            "Tong Lu",
            "Hongsheng Li",
            "Jifeng Dai",
            "Wenhai Wang"
        ],
        "tldr": "This paper introduces Docopilot, a multimodal model and associated dataset (Doc-750K) designed for enhanced document-level understanding, addressing limitations of current MLLMs and RAG-based approaches.",
        "tldr_zh": "该论文介绍了Docopilot，一种多模态模型和相关数据集 (Doc-750K)，旨在增强文档级别的理解，解决了当前 MLLM 和基于 RAG 的方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM",
        "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.",
        "url": "http://arxiv.org/abs/2507.14632v2",
        "published_date": "2025-07-19T14:05:33+00:00",
        "updated_date": "2025-07-31T12:03:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haiquan Wen",
            "Tianxiao Li",
            "Zhenglin Huang",
            "Yiwei He",
            "Guangliang Cheng"
        ],
        "tldr": "The paper introduces BusterX++, a novel framework for cross-modal AI-generated content detection and explanation, using reinforcement learning and a new benchmark dataset, GenBuster++.",
        "tldr_zh": "该论文介绍了 BusterX++，一个用于跨模态 AI 生成内容检测和解释的新框架，它使用强化学习和一个新的基准数据集 GenBuster++。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions",
        "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.",
        "url": "http://arxiv.org/abs/2507.14555v1",
        "published_date": "2025-07-19T09:19:16+00:00",
        "updated_date": "2025-07-19T09:19:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jintang Xue",
            "Ganning Zhao",
            "Jie-En Yao",
            "Hong-En Chen",
            "Yue Hu",
            "Meida Chen",
            "Suya You",
            "C. -C. Jay Kuo"
        ],
        "tldr": "Descrip3D enhances 3D scene understanding by incorporating object-level text descriptions that capture both intrinsic attributes and contextual relationships, improving performance across various tasks.",
        "tldr_zh": "Descrip3D通过结合对象级别的文本描述来增强3D场景理解，这些描述捕捉了内在属性和上下文关系，从而提高了各种任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding",
        "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.",
        "url": "http://arxiv.org/abs/2507.14533v1",
        "published_date": "2025-07-19T08:27:21+00:00",
        "updated_date": "2025-07-19T08:27:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuo Cao",
            "Nan Ma",
            "Jiayang Li",
            "Xiaohui Li",
            "Lihao Shao",
            "Kaiwen Zhu",
            "Yu Zhou",
            "Yuandong Pu",
            "Jiarui Wu",
            "Jiaquan Wang",
            "Bo Qu",
            "Wenhai Wang",
            "Yu Qiao",
            "Dajuin Yao",
            "Yihao Liu"
        ],
        "tldr": "The paper introduces ArtiMuse, an MLLM-based Image Aesthetics Assessment model with joint scoring and expert-level understanding, along with the ArtiMuse-10K dataset, which is expert-curated and contains fine-grained aesthetic attributes.",
        "tldr_zh": "该论文介绍了 ArtiMuse，一个基于 MLLM 的图像美学评估模型，具有联合评分和专家级理解能力，以及 ArtiMuse-10K 数据集，该数据集由专家策划，包含细粒度的美学属性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Whole Slide Pathology VQA via Token Compression",
        "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.",
        "url": "http://arxiv.org/abs/2507.14497v1",
        "published_date": "2025-07-19T06:04:25+00:00",
        "updated_date": "2025-07-19T06:04:25+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Weimin Lyu",
            "Qingqiao Hu",
            "Kehan Qi",
            "Zhan Shi",
            "Wentao Huang",
            "Saumya Gupta",
            "Chao Chen"
        ],
        "tldr": "The paper introduces TCP-LLaVA, a novel MLLM architecture for whole-slide pathology VQA that uses token compression to reduce computational cost and improve accuracy compared to existing methods.",
        "tldr_zh": "该论文介绍了一种名为TCP-LLaVA的新型MLLM架构，用于全切片病理VQA，它使用token压缩来降低计算成本，并提高与现有方法相比的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark",
        "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.",
        "url": "http://arxiv.org/abs/2507.14449v1",
        "published_date": "2025-07-19T02:53:01+00:00",
        "updated_date": "2025-07-19T02:53:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Cao",
            "Jin Zhang",
            "Ruiheng Zhang"
        ],
        "tldr": "The paper introduces IRGPT, a multimodal LLM for real-world infrared images, trained on a new large-scale IR-TD dataset with a bi-cross-modal curriculum learning strategy, achieving SOTA performance on various tasks.",
        "tldr_zh": "该论文介绍了IRGPT，一个用于真实红外图像的多模态LLM，它是在一个新的大规模IR-TD数据集上训练的，并采用了一种双向跨模态课程学习策略，在各种任务上实现了SOTA性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark",
        "summary": "The proliferation of multimodal Large Language Models has significantly\nadvanced the ability to analyze and understand complex data inputs from\ndifferent modalities. However, the processing of long documents remains\nunder-explored, largely due to a lack of suitable benchmarks. To address this,\nwe introduce Document Haystack, a comprehensive benchmark designed to evaluate\nthe performance of Vision Language Models (VLMs) on long, visually complex\ndocuments. Document Haystack features documents ranging from 5 to 200 pages and\nstrategically inserts pure text or multimodal text+image \"needles\" at various\ndepths within the documents to challenge VLMs' retrieval capabilities.\nComprising 400 document variants and a total of 8,250 questions, it is\nsupported by an objective, automated evaluation framework. We detail the\nconstruction and characteristics of the Document Haystack dataset, present\nresults from prominent VLMs and discuss potential research avenues in this\narea.",
        "url": "http://arxiv.org/abs/2507.15882v1",
        "published_date": "2025-07-18T19:33:15+00:00",
        "updated_date": "2025-07-18T19:33:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Goeric Huybrechts",
            "Srikanth Ronanki",
            "Sai Muralidhar Jayanthi",
            "Jack Fitzgerald",
            "Srinivasan Veeravanallur"
        ],
        "tldr": "The paper introduces Document Haystack, a new benchmark for evaluating Vision Language Models (VLMs) on long, visually complex documents, designed to test their retrieval capabilities within long contexts.",
        "tldr_zh": "该论文介绍了Document Haystack，这是一个新的基准，用于评估视觉语言模型（VLM）在长而视觉上复杂的文档上的性能，旨在测试它们在长上下文中的检索能力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation",
        "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.",
        "url": "http://arxiv.org/abs/2507.14312v1",
        "published_date": "2025-07-18T18:32:17+00:00",
        "updated_date": "2025-07-18T18:32:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Marc Lafon",
            "Gustavo Adolfo Vargas Hakim",
            "Clément Rambour",
            "Christian Desrosier",
            "Nicolas Thome"
        ],
        "tldr": "This paper introduces CLIPTTA, a novel test-time adaptation method for VLMs that uses a soft contrastive loss aligned with CLIP's pre-training objective to improve robustness under distribution shifts, outperforming existing entropy-based methods.",
        "tldr_zh": "本文介绍了一种名为CLIPTTA的新型VLM测试时自适应方法，该方法使用与CLIP预训练目标对齐的软对比损失，以提高在分布偏移下的鲁棒性，优于现有的基于熵的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding",
        "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.",
        "url": "http://arxiv.org/abs/2507.14298v1",
        "published_date": "2025-07-18T18:15:09+00:00",
        "updated_date": "2025-07-18T18:15:09+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Wan-Cyuan Fan",
            "Yen-Chun Chen",
            "Mengchen Liu",
            "Alexander Jacobson",
            "Lu Yuan",
            "Leonid Sigal"
        ],
        "tldr": "This paper introduces ChartScope, a pre-trained LVLM customized for comprehensive chart understanding using a novel data generation pipeline, Dual-Path training, and a new benchmark, ChartDQA, showing significant performance improvements across diverse chart types.",
        "tldr_zh": "本文介绍了ChartScope，一个为全面图表理解而定制的预训练LVLM，它使用了一种新的数据生成流程、双路径训练和一个新的基准ChartDQA，在各种图表类型中显示出显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025",
        "summary": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA\n2025 Challenge, which targets visual question answering (VQA) for\ngastrointestinal endoscopy. We adopt the Florence model-a large-scale\nmultimodal foundation model-as the backbone of our VQA pipeline, pairing a\npowerful vision encoder with a text encoder to interpret endoscopic images and\nproduce clinically relevant answers. To improve generalization, we apply\ndomain-specific augmentations that preserve medical features while increasing\ntraining diversity. Experiments on the KASVIR dataset show that fine-tuning\nFlorence yields accurate responses on the official challenge metrics. Our\nresults highlight the potential of large multimodal models in medical VQA and\nprovide a strong baseline for future work on explainability, robustness, and\nclinical integration. The code is publicly available at:\nhttps://github.com/TiwariLaxuu/VQA-Florence.git",
        "url": "http://arxiv.org/abs/2507.14544v1",
        "published_date": "2025-07-19T09:04:13+00:00",
        "updated_date": "2025-07-19T09:04:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "68T45 (Machine vision and scene understanding)",
            "I.2.10; I.4.8; H.3.1"
        ],
        "authors": [
            "Sujata Gaihre",
            "Amir Thapa Magar",
            "Prasuna Pokharel",
            "Laxmi Tiwari"
        ],
        "tldr": "This paper explores using the Florence multimodal model for visual question answering on gastrointestinal endoscopy images, achieving accurate results on the KASVIR dataset. It demonstrates the potential of large multimodal models in medical VQA.",
        "tldr_zh": "本文探讨了使用 Florence 多模态模型进行胃肠内窥镜图像的视觉问答，并在 KASVIR 数据集上取得了准确的结果。它展示了大型多模态模型在医学 VQA 中的潜力。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution",
        "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.",
        "url": "http://arxiv.org/abs/2507.14367v1",
        "published_date": "2025-07-18T21:13:50+00:00",
        "updated_date": "2025-07-18T21:13:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiming Ren",
            "Raghav Goyal",
            "Zhiming Hu",
            "Tristan Ty Aumentado-Armstrong",
            "Iqbal Mohomed",
            "Alex Levinshtein"
        ],
        "tldr": "This paper introduces a \"Hallucination Score\" using MLLMs to measure and mitigate hallucinations in generative super-resolution (GSR) images, which are not well-captured by existing metrics, and proposes using deep features as differentiable reward functions to align GSR models.",
        "tldr_zh": "该论文介绍了一种使用多模态大型语言模型（MLLM）的“幻觉分数”，用于测量和减轻生成式超分辨率（GSR）图像中的幻觉，这些幻觉无法被现有指标很好地捕捉。文章还提出使用深度特征作为可微奖励函数来对齐GSR模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2",
        "summary": "Recent advances in medical image segmentation have been driven by deep\nlearning; however, most existing methods remain limited by modality-specific\ndesigns and exhibit poor adaptability to dynamic medical imaging scenarios. The\nSegment Anything Model 2 (SAM2) and its related variants, which introduce a\nstreaming memory mechanism for real-time video segmentation, present new\nopportunities for prompt-based, generalizable solutions. Nevertheless, adapting\nthese models to medical video scenarios typically requires large-scale datasets\nfor retraining or transfer learning, leading to high computational costs and\nthe risk of catastrophic forgetting. To address these challenges, we propose\nDD-SAM2, an efficient adaptation framework for SAM2 that incorporates a\nDepthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature\nextraction with minimal parameter overhead. This design enables effective\nfine-tuning of SAM2 on medical videos with limited training data. Unlike\nexisting adapter-based methods focused solely on static images, DD-SAM2 fully\nexploits SAM2's streaming memory for medical video object tracking and\nsegmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)\nand EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior\nperformance, achieving Dice scores of 0.93 and 0.97, respectively. To the best\nof our knowledge, this work provides an initial attempt at systematically\nexploring adapter-based SAM2 fine-tuning for medical video segmentation and\ntracking. Code, datasets, and models will be publicly available at\nhttps://github.com/apple1986/DD-SAM2.",
        "url": "http://arxiv.org/abs/2507.14613v1",
        "published_date": "2025-07-19T13:19:55+00:00",
        "updated_date": "2025-07-19T13:19:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guoping Xu",
            "Christopher Kabat",
            "You Zhang"
        ],
        "tldr": "The paper introduces DD-SAM2, an efficient adaptation framework for SAM2 using Depthwise-Dilated Adapters, achieving superior performance in medical video object tracking and segmentation with limited training data, as demonstrated on TrackRad2025 and EchoNet-Dynamic datasets.",
        "tldr_zh": "该论文介绍了DD-SAM2，一个基于深度可分离扩张适配器的SAM2高效适配框架，在有限的训练数据下，在医学视频对象跟踪和分割方面表现出色，并在TrackRad2025和EchoNet-Dynamic数据集上得到了验证。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "WebGuard: Building a Generalizable Guardrail for Web Agents",
        "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.",
        "url": "http://arxiv.org/abs/2507.14293v1",
        "published_date": "2025-07-18T18:06:27+00:00",
        "updated_date": "2025-07-18T18:06:27+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Boyuan Zheng",
            "Zeyi Liao",
            "Scott Salisbury",
            "Zeyuan Liu",
            "Michael Lin",
            "Qinyuan Zheng",
            "Zifan Wang",
            "Xiang Deng",
            "Dawn Song",
            "Huan Sun",
            "Yu Su"
        ],
        "tldr": "The paper introduces WebGuard, a dataset for assessing web agent action risks, and demonstrates the need for specialized guardrails as current LLMs perform poorly in predicting action outcomes, even after fine-tuning.",
        "tldr_zh": "本文介绍了WebGuard，一个用于评估Web代理行为风险的数据集，并证明了对专用防护栏的需求，因为即使经过微调，当前的大型语言模型在预测行为结果方面表现也很差。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]