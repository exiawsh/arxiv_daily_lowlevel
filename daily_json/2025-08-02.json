[
    {
        "title": "FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems",
        "summary": "We present FMPlug, a novel plug-in framework that enhances foundation\nflow-matching (FM) priors for solving ill-posed inverse problems. Unlike\ntraditional approaches that rely on domain-specific or untrained priors, FMPlug\nsmartly leverages two simple but powerful insights: the similarity between\nobserved and desired objects and the Gaussianity of generative flows. By\nintroducing a time-adaptive warm-up strategy and sharp Gaussianity\nregularization, FMPlug unlocks the true potential of domain-agnostic foundation\nmodels. Our method beats state-of-the-art methods that use foundation FM priors\nby significant margins, on image super-resolution and Gaussian deblurring.",
        "url": "http://arxiv.org/abs/2508.00721v1",
        "published_date": "2025-08-01T15:40:37+00:00",
        "updated_date": "2025-08-01T15:40:37+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG",
            "eess.SP"
        ],
        "authors": [
            "Yuxiang Wan",
            "Ryan Devera",
            "Wenjie Zhang",
            "Ju Sun"
        ],
        "tldr": "FMPlug improves foundation flow-matching priors for solving inverse problems by leveraging the similarity between observed and desired objects and the Gaussianity of generative flows, achieving state-of-the-art results in image super-resolution and deblurring.",
        "tldr_zh": "FMPlug通过利用观察对象和期望对象之间的相似性以及生成流的高斯性，改进了用于解决逆问题的基础流匹配先验，并在图像超分辨率和去模糊方面取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation",
        "summary": "Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.",
        "url": "http://arxiv.org/abs/2508.00750v1",
        "published_date": "2025-08-01T16:25:21+00:00",
        "updated_date": "2025-08-01T16:25:21+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Prerana Ramkumar"
        ],
        "tldr": "The paper introduces SU-ESRGAN, a novel super-resolution framework for satellite and drone imagery that integrates semantic information and uncertainty estimation, showing comparable performance to baseline ESRGAN and good cross-domain adaptability with fine-tuning.",
        "tldr_zh": "本文介绍了一种名为SU-ESRGAN的新型超分辨率框架，用于卫星和无人机图像，该框架集成了语义信息和不确定性估计，在性能上与基线ESRGAN相当，并通过微调表现出良好的跨域适应性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)",
        "summary": "Artificial Night-Time Light (NTL) remote sensing is a vital proxy for\nquantifying the intensity and spatial distribution of human activities.\nAlthough the NPP-VIIRS sensor provides high-quality NTL observations, its\ntemporal coverage, which begins in 2012, restricts long-term time-series\nstudies that extend to earlier periods. Despite the progress in extending\nVIIRS-like NTL time-series, current methods still suffer from two significant\nshortcomings: the underestimation of light intensity and the structural\nomission. To overcome these limitations, we propose a novel reconstruction\nframework consisting of a two-stage process: construction and refinement. The\nconstruction stage features a Hierarchical Fusion Decoder (HFD) designed to\nenhance the fidelity of the initial reconstruction. The refinement stage\nemploys a Dual Feature Refiner (DFR), which leverages high-resolution\nimpervious surface masks to guide and enhance fine-grained structural details.\nBased on this framework, we developed the Extended VIIRS-like Artificial\nNighttime Light (EVAL) product for China, extending the standard data record\nbackwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL\nsignificantly outperforms existing state-of-the-art products, boosting the\n$\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.\nFurthermore, EVAL exhibits excellent temporal consistency and maintains a high\ncorrelation with socioeconomic parameters, confirming its reliability for\nlong-term analysis. The resulting EVAL dataset provides a valuable new resource\nfor the research community and is publicly available at\nhttps://doi.org/10.11888/HumanNat.tpdc.302930.",
        "url": "http://arxiv.org/abs/2508.00590v1",
        "published_date": "2025-08-01T12:44:55+00:00",
        "updated_date": "2025-08-01T12:44:55+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Yihe Tian",
            "Kwan Man Cheng",
            "Zhengbo Zhang",
            "Tao Zhang",
            "Suju Li",
            "Dongmei Yan",
            "Bing Xu"
        ],
        "tldr": "This paper introduces a novel two-stage framework for reconstructing high-quality, long-term artificial nighttime light images (1986-2024), addressing limitations of existing methods by improving light intensity estimation and structural details, and provides a publicly available dataset.",
        "tldr_zh": "本文介绍了一种新颖的两阶段框架，用于重建高质量、长期的夜间人工照明图像（1986-2024）。该框架通过改进光照强度估计和结构细节，解决了现有方法的局限性，并提供了一个公开可用的数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer",
        "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.",
        "url": "http://arxiv.org/abs/2508.00477v1",
        "published_date": "2025-08-01T09:51:54+00:00",
        "updated_date": "2025-08-01T09:51:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuzhuo Chen",
            "Zehua Ma",
            "Jianhua Wang",
            "Kai Kang",
            "Shunyu Yao",
            "Weiming Zhang"
        ],
        "tldr": "The paper introduces LAMIC, a training-free framework for layout-aware multi-image composition using diffusion models, achieving state-of-the-art performance in identity keeping, background preservation, and layout control.",
        "tldr_zh": "该论文介绍了LAMIC，一个无需训练的布局感知多图像合成框架，它使用扩散模型，在身份保持、背景保留和布局控制方面实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution",
        "summary": "Recent advancements in video super-resolution (VSR) models have demonstrated\nimpressive results in enhancing low-resolution videos. However, due to\nlimitations in adequately controlling the generation process, achieving high\nfidelity alignment with the low-resolution input while maintaining temporal\nconsistency across frames remains a significant challenge. In this work, we\npropose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel\napproach that incorporates both semantic and temporal-spatio guidance in the\nlatent diffusion space to address these challenges. By incorporating high-level\nsemantic information and integrating spatial and temporal information, our\napproach achieves a seamless balance between recovering intricate details and\nensuring temporal coherence. Our method not only preserves high-reality visual\ncontent but also significantly enhances fidelity. Extensive experiments\ndemonstrate that SeTe-VSR outperforms existing methods in terms of detail\nrecovery and perceptual quality, highlighting its effectiveness for complex\nvideo super-resolution tasks.",
        "url": "http://arxiv.org/abs/2508.00471v1",
        "published_date": "2025-08-01T09:47:35+00:00",
        "updated_date": "2025-08-01T09:47:35+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Yiwen Wang",
            "Xinning Chai",
            "Yuhong Zhang",
            "Zhengxue Cheng",
            "Jun Zhao",
            "Rong Xie",
            "Li Song"
        ],
        "tldr": "The paper introduces SeTe-VSR, a novel video super-resolution method utilizing semantic and temporal guidance within the latent diffusion space to improve fidelity and temporal consistency.",
        "tldr_zh": "该论文介绍了SeTe-VSR，一种新的视频超分辨率方法，它利用潜在扩散空间中的语义和时间引导来提高保真度和时间一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space",
        "summary": "We present DC-AE 1.5, a new family of deep compression autoencoders for\nhigh-resolution diffusion models. Increasing the autoencoder's latent channel\nnumber is a highly effective approach for improving its reconstruction quality.\nHowever, it results in slow convergence for diffusion models, leading to poorer\ngeneration quality despite better reconstruction quality. This issue limits the\nquality upper bound of latent diffusion models and hinders the employment of\nautoencoders with higher spatial compression ratios. We introduce two key\ninnovations to address this challenge: i) Structured Latent Space, a\ntraining-based approach to impose a desired channel-wise structure on the\nlatent space with front latent channels capturing object structures and latter\nlatent channels capturing image details; ii) Augmented Diffusion Training, an\naugmented diffusion training strategy with additional diffusion training\nobjectives on object latent channels to accelerate convergence. With these\ntechniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling\nresults than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better\nimage generation quality than DC-AE-f32c32 while being 4x faster. Code:\nhttps://github.com/dc-ai-projects/DC-Gen.",
        "url": "http://arxiv.org/abs/2508.00413v1",
        "published_date": "2025-08-01T08:11:07+00:00",
        "updated_date": "2025-08-01T08:11:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Junyu Chen",
            "Dongyun Zou",
            "Wenkun He",
            "Junsong Chen",
            "Enze Xie",
            "Song Han",
            "Han Cai"
        ],
        "tldr": "The paper introduces DC-AE 1.5, a new autoencoder architecture for diffusion models that improves convergence speed and image generation quality through structured latent space and augmented diffusion training.",
        "tldr_zh": "该论文介绍了DC-AE 1.5，一种用于扩散模型的新型自动编码器架构，通过结构化潜在空间和增强的扩散训练，提高了收敛速度和图像生成质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos",
        "summary": "Geometric distortions and blurring caused by atmospheric turbulence degrade\nthe quality of long-range dynamic scene videos. Existing methods struggle with\nrestoring edge details and eliminating mixed distortions, especially under\nconditions of strong turbulence and complex dynamics. To address these\nchallenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines\nturbulence intensity, optical flow, and proportions of dynamic regions to\naccurately quantify video dynamic intensity under varying turbulence conditions\nand provide a high-dynamic turbulence training dataset. Additionally, we\npropose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework\nthat consists of three stages: \\textbf{de-tilting} for geometric stabilization,\n\\textbf{motion segmentation enhancement} for dynamic region refinement, and\n\\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight\nbackbones and stage-wise joint training to ensure both efficiency and high\nrestoration quality. Experimental results demonstrate that the proposed method\neffectively suppresses motion trailing artifacts, restores edge details and\nexhibits strong generalization capability, especially in real-world scenarios\ncharacterized by high-turbulence and complex dynamics. We will make the code\nand datasets openly available.",
        "url": "http://arxiv.org/abs/2508.00406v1",
        "published_date": "2025-08-01T08:06:41+00:00",
        "updated_date": "2025-08-01T08:06:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Wu",
            "Jingyuan Ye",
            "Ying Fu"
        ],
        "tldr": "The paper introduces a physical model-driven multi-stage video restoration framework (PMR) for turbulent dynamic videos, addressing geometric distortions and blurring. It includes a Dynamic Efficiency Index (DEI) for quantifying video dynamic intensity and achieves state-of-the-art results with strong generalization capability.",
        "tldr_zh": "该论文介绍了一种物理模型驱动的多阶段视频恢复框架 (PMR)，用于处理湍流动态视频，解决了几何失真和模糊问题。 它包括一个动态效率指数 (DEI)，用于量化视频动态强度，并实现了最先进的结果，具有强大的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network",
        "summary": "Depth map super-resolution technology aims to improve the spatial resolution\nof low-resolution depth maps and effectively restore high-frequency detail\ninformation. Traditional convolutional neural network has limitations in\ndealing with long-range dependencies and are unable to fully model the global\ncontextual information in depth maps. Although transformer can model global\ndependencies, its computational complexity and memory consumption are\nquadratic, which significantly limits its ability to process high-resolution\ndepth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba\n(MSF-UM) model, a novel guided depth map super-resolution framework. The core\ninnovation of this model is to integrate Mamba's efficient state-space modeling\ncapabilities into a multi-scale U-shaped fusion structure guided by a color\nimage. The structure combining the residual dense channel attention block and\nthe Mamba state space module is designed, which combines the local feature\nextraction capability of the convolutional layer with the modeling advantage of\nthe state space model for long-distance dependencies. At the same time, the\nmodel adopts a multi-scale cross-modal fusion strategy to make full use of the\nhigh-frequency texture information from the color image to guide the\nsuper-resolution process of the depth map. Compared with existing mainstream\nmethods, the proposed MSF-UM significantly reduces the number of model\nparameters while achieving better reconstruction accuracy. Extensive\nexperiments on multiple publicly available datasets validate the effectiveness\nof the model, especially showing excellent generalization ability in the task\nof large-scale depth map super-resolution.",
        "url": "http://arxiv.org/abs/2508.00248v1",
        "published_date": "2025-08-01T01:24:34+00:00",
        "updated_date": "2025-08-01T01:24:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenggang Guo",
            "Hao Xu",
            "XianMing Wan"
        ],
        "tldr": "This paper introduces a novel Mamba-based U-shaped network (MSF-UM) for guided depth map super-resolution, leveraging multi-scale fusion and color image guidance to improve reconstruction accuracy and reduce model parameters.",
        "tldr_zh": "本文提出了一种新的基于Mamba的U型网络（MSF-UM），用于引导深度图超分辨率，利用多尺度融合和彩色图像引导来提高重建精度并减少模型参数。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection",
        "summary": "Coronary stenosis is a major risk factor for ischemic heart events leading to\nincreased mortality, and medical treatments for this condition require\nmeticulous, labor-intensive analysis. Coronary angiography provides critical\nvisual cues for assessing stenosis, supporting clinicians in making informed\ndecisions for diagnosis and treatment. Recent advances in deep learning have\nshown great potential for automated localization and severity measurement of\nstenosis. In real-world scenarios, however, the success of these competent\napproaches is often hindered by challenges such as limited labeled data and\nclass imbalance. In this study, we propose a novel data augmentation approach\nthat uses an inpainting method based on a diffusion model to generate realistic\nlesions, allowing user-guided control of severity. Extensive evaluation on\nlesion detection and severity classification across various synthetic dataset\nsizes shows superior performance of our method on both a large-scale in-house\ndataset and a public coronary angiography dataset. Furthermore, our approach\nmaintains high detection and classification performance even when trained with\nlimited data, highlighting its clinical importance in improving the assessment\nof severity of stenosis and optimizing data utilization for more reliable\ndecision support.",
        "url": "http://arxiv.org/abs/2508.00438v1",
        "published_date": "2025-08-01T08:52:43+00:00",
        "updated_date": "2025-08-01T08:52:43+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Sumin Seo",
            "In Kyu Lee",
            "Hyun-Woo Kim",
            "Jaesik Min",
            "Chung-Hwan Jung"
        ],
        "tldr": "This paper proposes a diffusion-based data augmentation method with user-guided control of lesion severity for improving coronary stenosis detection and classification, especially when labeled data is limited.",
        "tldr_zh": "本文提出了一种基于扩散模型的数据增强方法，该方法可以通过用户引导控制病变程度，从而提高冠状动脉狭窄的检测和分类，特别是在标记数据有限的情况下。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting",
        "summary": "Amodal completion, which is the process of inferring the full appearance of\nobjects despite partial occlusions, is crucial for understanding complex\nhuman-object interactions (HOI) in computer vision and robotics. Existing\nmethods, such as those that use pre-trained diffusion models, often struggle to\ngenerate plausible completions in dynamic scenarios because they have a limited\nunderstanding of HOI. To solve this problem, we've developed a new approach\nthat uses physical prior knowledge along with a specialized multi-regional\ninpainting technique designed for HOI. By incorporating physical constraints\nfrom human topology and contact information, we define two distinct regions:\nthe primary region, where occluded object parts are most likely to be, and the\nsecondary region, where occlusions are less probable. Our multi-regional\ninpainting method uses customized denoising strategies across these regions\nwithin a diffusion model. This improves the accuracy and realism of the\ngenerated completions in both their shape and visual detail. Our experimental\nresults show that our approach significantly outperforms existing methods in\nHOI scenarios, moving machine perception closer to a more human-like\nunderstanding of dynamic environments. We also show that our pipeline is robust\neven without ground-truth contact annotations, which broadens its applicability\nto tasks like 3D reconstruction and novel view/pose synthesis.",
        "url": "http://arxiv.org/abs/2508.00427v1",
        "published_date": "2025-08-01T08:33:45+00:00",
        "updated_date": "2025-08-01T08:33:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Seunggeun Chi",
            "Enna Sachdeva",
            "Pin-Hao Huang",
            "Kwonjoon Lee"
        ],
        "tldr": "The paper introduces a contact-aware amodal completion method using a multi-regional inpainting technique within a diffusion model to improve the accuracy and realism of object completion in human-object interaction scenarios.",
        "tldr_zh": "该论文介绍了一种基于扩散模型的接触感知非模态补全方法，该方法使用多区域修复技术来提高人与物体交互场景中物体补全的准确性和真实感。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models",
        "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.",
        "url": "http://arxiv.org/abs/2508.00319v1",
        "published_date": "2025-08-01T05:02:26+00:00",
        "updated_date": "2025-08-01T05:02:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Sunghyun Park",
            "Seokeon Choi",
            "Hyoungwoo Park",
            "Sungrack Yun"
        ],
        "tldr": "This paper introduces 'personalization guidance,' a method for improving personalized text-to-image diffusion models by dynamically controlling the influence of an unlearned weak model, enhancing both text alignment and target fidelity without additional computational cost.",
        "tldr_zh": "本文介绍了一种名为“个性化引导”的方法，通过动态控制未学习的弱模型的影响，来改善个性化的文本到图像扩散模型，从而在不增加额外计算成本的情况下，增强文本对齐和目标保真度。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement",
        "summary": "The event camera, benefiting from its high dynamic range and low latency,\nprovides performance gain for low-light image enhancement. Unlike frame-based\ncameras, it records intensity changes with extremely high temporal resolution,\ncapturing sufficient structure information. Currently, existing event-based\nmethods feed a frame and events directly into a single model without fully\nexploiting modality-specific advantages, which limits their performance.\nTherefore, by analyzing the role of each sensing modality, the enhancement\npipeline is decoupled into two stages: visibility restoration and structure\nrefinement. In the first stage, we design a visibility restoration network with\namplitude-phase entanglement by rethinking the relationship between amplitude\nand phase components in Fourier space. In the second stage, a fusion strategy\nwith dynamic alignment is proposed to mitigate the spatial mismatch caused by\nthe temporal resolution discrepancy between two sensing modalities, aiming to\nrefine the structure information of the image enhanced by the visibility\nrestoration network. In addition, we utilize spatial-frequency interpolation to\nsimulate negative samples with diverse illumination, noise and artifact\ndegradations, thereby developing a contrastive loss that encourages the model\nto learn discriminative representations. Experiments demonstrate that the\nproposed method outperforms state-of-the-art models.",
        "url": "http://arxiv.org/abs/2508.00308v1",
        "published_date": "2025-08-01T04:25:00+00:00",
        "updated_date": "2025-08-01T04:25:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chunyan She",
            "Fujun Han",
            "Chengyu Fang",
            "Shukai Duan",
            "Lidan Wang"
        ],
        "tldr": "This paper proposes a two-stage low-light image enhancement method using event cameras and frame-based cameras, decoupling the process into visibility restoration with a Fourier-based network and structure refinement using dynamic alignment to fuse the modalities.",
        "tldr_zh": "本文提出了一种使用事件相机和帧相机相结合的两阶段低光图像增强方法，将过程分解为基于傅里叶网络的可见性恢复和使用动态对齐融合模态的结构细化。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Jet Image Generation in High Energy Physics Using Diffusion Models",
        "summary": "This article presents, for the first time, the application of diffusion\nmodels for generating jet images corresponding to proton-proton collision\nevents at the Large Hadron Collider (LHC). The kinematic variables of quark,\ngluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset\nare mapped to two-dimensional image representations. Diffusion models are\ntrained on these images to learn the spatial distribution of jet constituents.\nWe compare the performance of score-based diffusion models and consistency\nmodels in accurately generating class-conditional jet images. Unlike approaches\nbased on latent distributions, our method operates directly in image space. The\nfidelity of the generated images is evaluated using several metrics, including\nthe Fr\\'echet Inception Distance (FID), which demonstrates that consistency\nmodels achieve higher fidelity and generation stability compared to score-based\ndiffusion models. These advancements offer significant improvements in\ncomputational efficiency and generation accuracy, providing valuable tools for\nHigh Energy Physics (HEP) research.",
        "url": "http://arxiv.org/abs/2508.00250v1",
        "published_date": "2025-08-01T01:41:27+00:00",
        "updated_date": "2025-08-01T01:41:27+00:00",
        "categories": [
            "hep-ph",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Victor D. Martinez",
            "Vidya Manian",
            "Sudhir Malik"
        ],
        "tldr": "This paper explores the application of diffusion models, specifically score-based and consistency models, for generating jet images in high-energy physics, demonstrating improved computational efficiency and generation accuracy compared to latent distribution approaches.",
        "tldr_zh": "该论文探索了扩散模型（特别是基于分数的模型和一致性模型）在产生高能物理中喷注图像的应用，与潜在分布方法相比，展示了更高的计算效率和生成准确性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints",
        "summary": "Articulated objects are an important type of interactable objects in everyday\nenvironments. In this paper, we propose PhysNAP, a novel diffusion model-based\napproach for generating articulated objects that aligns them with partial point\nclouds and improves their physical plausibility. The model represents part\nshapes by signed distance functions (SDFs). We guide the reverse diffusion\nprocess using a point cloud alignment loss computed using the predicted SDFs.\nAdditionally, we impose non-penetration and mobility constraints based on the\npart SDFs for guiding the model to generate more physically plausible objects.\nWe also make our diffusion approach category-aware to further improve point\ncloud alignment if category information is available. We evaluate the\ngenerative ability and constraint consistency of samples generated with PhysNAP\nusing the PartNet-Mobility dataset. We also compare it with an unguided\nbaseline diffusion model and demonstrate that PhysNAP can improve constraint\nconsistency and provides a tradeoff with generative ability.",
        "url": "http://arxiv.org/abs/2508.00558v1",
        "published_date": "2025-08-01T11:56:03+00:00",
        "updated_date": "2025-08-01T11:56:03+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jens U. Kreber",
            "Joerg Stueckler"
        ],
        "tldr": "The paper introduces PhysNAP, a diffusion model-based approach for generating articulated objects from partial point clouds, enhancing physical plausibility through SDF-based constraints.",
        "tldr_zh": "本文介绍PhysNAP，一种基于扩散模型的生成铰接物体的方法，它使用部分点云作为输入，并通过基于SDF的约束来增强物理合理性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5
    }
]