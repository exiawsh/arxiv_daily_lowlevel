[
    {
        "title": "REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework",
        "summary": "Photorealism is an important aspect of modern video games since it can shape\nthe player experience and simultaneously impact the immersion, narrative\nengagement, and visual fidelity. Although recent hardware technological\nbreakthroughs, along with state-of-the-art rendering technologies, have\nsignificantly improved the visual realism of video games, achieving true\nphotorealism in dynamic environments at real-time frame rates still remains a\nmajor challenge due to the tradeoff between visual quality and performance. In\nthis short paper, we present a novel approach for enhancing the photorealism of\nrendered game frames using generative adversarial networks. To this end, we\npropose Real-time photorealism Enhancement in Games via a dual-stage gEnerative\nNetwork framework (REGEN), which employs a robust unpaired image-to-image\ntranslation model to produce semantically consistent photorealistic frames that\ntransform the problem into a simpler paired image-to-image translation task.\nThis enables training with a lightweight method that can achieve real-time\ninference time without compromising visual quality. We demonstrate the\neffectiveness of our framework on Grand Theft Auto V, showing that the approach\nachieves visual results comparable to the ones produced by the robust unpaired\nIm2Im method while improving inference speed by 32.14 times. Our findings also\nindicate that the results outperform the photorealism-enhanced frames produced\nby directly training a lightweight unpaired Im2Im translation method to\ntranslate the video game frames towards the visual characteristics of\nreal-world images. Code, pre-trained models, and demos for this work are\navailable at: https://github.com/stefanos50/REGEN.",
        "url": "http://arxiv.org/abs/2508.17061v1",
        "published_date": "2025-08-23T15:28:05+00:00",
        "updated_date": "2025-08-23T15:28:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Stefanos Pasios",
            "Nikos Nikolaidis"
        ],
        "tldr": "The paper introduces REGEN, a dual-stage generative network framework that enhances photorealism in games in real-time by using a robust unpaired image-to-image translation model to produce semantically consistent photorealistic frames with fast inference speed.",
        "tldr_zh": "该论文介绍了一种双阶段生成网络框架REGEN，它通过使用鲁棒的非配对图像到图像的转换模型来生成语义一致的逼真帧，从而实时增强游戏中的逼真度，并具有快速的推理速度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching",
        "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
        "url": "http://arxiv.org/abs/2508.16984v1",
        "published_date": "2025-08-23T10:35:16+00:00",
        "updated_date": "2025-08-23T10:35:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liang Feng",
            "Shikang Zheng",
            "Jiacheng Liu",
            "Yuqi Lin",
            "Qinming Zhou",
            "Peiliang Cai",
            "Xinyu Wang",
            "Junjie Chen",
            "Chang Zou",
            "Yue Ma",
            "Linfeng Zhang"
        ],
        "tldr": "HiCache is a training-free acceleration method for diffusion models that uses Hermite polynomials to improve feature caching, achieving significant speedups while maintaining or improving quality in various generation tasks.",
        "tldr_zh": "HiCache是一种无需训练的扩散模型加速方法，它利用Hermite多项式来改进特征缓存，在保持或提高各种生成任务质量的同时，实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network",
        "summary": "Contrast-enhanced computed tomography (CT) imaging is essential for\ndiagnosing and monitoring thoracic diseases, including aortic pathologies.\nHowever, contrast agents pose risks such as nephrotoxicity and allergic-like\nreactions. The ability to generate high-fidelity synthetic contrast-enhanced CT\nangiography (CTA) images without contrast administration would be\ntransformative, enhancing patient safety and accessibility while reducing\nhealthcare costs. In this study, we propose the first bridge diffusion-based\nsolution for synthesizing contrast-enhanced CTA images from non-contrast CT\nscans. Our approach builds on the Slice-Consistent Brownian Bridge Diffusion\nModel (SC-BBDM), leveraging its ability to model complex mappings while\nmaintaining consistency across slices. Unlike conventional slice-wise synthesis\nmethods, our framework preserves full 3D anatomical integrity while operating\nin a high-resolution 2D fashion, allowing seamless volumetric interpretation\nunder a low memory budget. To ensure robust spatial alignment, we implement a\ncomprehensive preprocessing pipeline that includes resampling, registration\nusing the Symmetric Normalization method, and a sophisticated dilated\nsegmentation mask to extract the aorta and surrounding structures. We create\ntwo datasets from the Coltea-Lung dataset: one containing only the aorta and\nanother including both the aorta and heart, enabling a detailed analysis of\nanatomical context. We compare our approach against baseline methods on both\ndatasets, demonstrating its effectiveness in preserving vascular structures\nwhile enhancing contrast fidelity.",
        "url": "http://arxiv.org/abs/2508.16897v1",
        "published_date": "2025-08-23T05:02:16+00:00",
        "updated_date": "2025-08-23T05:02:16+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "physics.med-ph"
        ],
        "authors": [
            "Pouya Shiri",
            "Xin Yi",
            "Neel P. Mistry",
            "Samaneh Javadinia",
            "Mohammad Chegini",
            "Seok-Bum Ko",
            "Amirali Baniasadi",
            "Scott J. Adams"
        ],
        "tldr": "This paper introduces a slice-consistent Brownian bridge diffusion model (SC-BBDM) to generate synthetic contrast-enhanced chest CT angiography (CTA) images from non-contrast CT scans, enhancing patient safety and accessibility.",
        "tldr_zh": "本文介绍了一种切片一致的布朗桥扩散模型 (SC-BBDM)，用于从非对比 CT 扫描生成合成的对比增强胸部 CT 血管造影 (CTA) 图像，从而提高患者安全性和可及性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation",
        "summary": "Diffusion-based Handwritten Text Generation (HTG) approaches achieve\nimpressive results on frequent, in-vocabulary words observed at training time\nand on regular styles. However, they are prone to memorizing training samples\nand often struggle with style variability and generation clarity. In\nparticular, standard diffusion models tend to produce artifacts or distortions\nthat negatively affect the readability of the generated text, especially when\nthe style is hard to produce. To tackle these issues, we propose a novel\nsampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an\northogonal projection of a negatively perturbed prompt onto the original\npositive prompt. This approach helps steer the generation away from artifacts\nwhile maintaining the intended content, and encourages more diverse, yet\nplausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which\nrelies on unconditional predictions and produces noise at high guidance scales,\nDOG introduces a more stable, disentangled direction in the latent space. To\ncontrol the strength of the guidance across the denoising process, we apply a\ntriangular schedule: weak at the start and end of denoising, when the process\nis most sensitive, and strongest in the middle steps. Experimental results on\nthe state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both\ncontent clarity and style variability, even for out-of-vocabulary words and\nchallenging writing styles.",
        "url": "http://arxiv.org/abs/2508.17017v1",
        "published_date": "2025-08-23T13:09:19+00:00",
        "updated_date": "2025-08-23T13:09:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Konstantina Nikolaidou",
            "George Retsinas",
            "Giorgos Sfikas",
            "Silvia Cascianelli",
            "Rita Cucchiara",
            "Marcus Liwicki"
        ],
        "tldr": "This paper introduces Dual Orthogonal Guidance (DOG) to improve the clarity and style variability of diffusion-based handwritten text generation, addressing issues like artifacts and poor handling of out-of-vocabulary words and challenging styles.",
        "tldr_zh": "该论文介绍了双正交引导(DOG)，以提高基于扩散的手写文本生成的清晰度和风格可变性，解决了伪影以及对词汇表外单词和具有挑战性的风格处理不佳等问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]