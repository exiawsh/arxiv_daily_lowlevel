[
    {
        "title": "PocketSR: The Super-Resolution Expert in Your Pocket Mobiles",
        "summary": "Real-world image super-resolution (RealSR) aims to enhance the visual quality\nof in-the-wild images, such as those captured by mobile phones. While existing\nmethods leveraging large generative models demonstrate impressive results, the\nhigh computational cost and latency make them impractical for edge deployment.\nIn this paper, we introduce PocketSR, an ultra-lightweight, single-step model\nthat brings generative modeling capabilities to RealSR while maintaining high\nfidelity. To achieve this, we design LiteED, a highly efficient alternative to\nthe original computationally intensive VAE in SD, reducing parameters by 97.5%\nwhile preserving high-quality encoding and decoding. Additionally, we propose\nonline annealing pruning for the U-Net, which progressively shifts generative\npriors from heavy modules to lightweight counterparts, ensuring effective\nknowledge transfer and further optimizing efficiency. To mitigate the loss of\nprior knowledge during pruning, we incorporate a multi-layer feature\ndistillation loss. Through an in-depth analysis of each design component, we\nprovide valuable insights for future research. PocketSR, with a model size of\n146M parameters, processes 4K images in just 0.8 seconds, achieving a\nremarkable speedup over previous methods. Notably, it delivers performance on\npar with state-of-the-art single-step and even multi-step RealSR models, making\nit a highly practical solution for edge-device applications.",
        "url": "http://arxiv.org/abs/2510.03012v1",
        "published_date": "2025-10-03T13:56:18+00:00",
        "updated_date": "2025-10-03T13:56:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoze Sun",
            "Linfeng Jiang",
            "Fan Li",
            "Renjing Pei",
            "Zhixin Wang",
            "Yong Guo",
            "Jiaqi Xu",
            "Haoyu Chen",
            "Jin Han",
            "Fenglong Song",
            "Yujiu Yang",
            "Wenbo Li"
        ],
        "tldr": "PocketSR introduces an ultra-lightweight, single-step model for real-world image super-resolution on edge devices, achieving state-of-the-art performance with significantly reduced computational cost by using LiteED and online annealing pruning.",
        "tldr_zh": "PocketSR 提出了一种超轻量级的单步模型，用于边缘设备上的真实世界图像超分辨率，通过使用 LiteED 和在线退火剪枝，以显著降低的计算成本实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 10,
        "overall_priority_score": 10
    },
    {
        "title": "Product-Quantised Image Representation for High-Quality Image Synthesis",
        "summary": "Product quantisation (PQ) is a classical method for scalable vector encoding,\nyet it has seen limited usage for latent representations in high-fidelity image\ngeneration. In this work, we introduce PQGAN, a quantised image autoencoder\nthat integrates PQ into the well-known vector quantisation (VQ) framework of\nVQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in\nterms of reconstruction performance, including both quantisation methods and\ntheir continuous counterparts. We achieve a PSNR score of 37dB, where prior\nwork achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up\nto 96%. Our key to success is a thorough analysis of the interaction between\ncodebook size, embedding dimensionality, and subspace factorisation, with\nvector and scalar quantisation as special cases. We obtain novel findings, such\nthat the performance of VQ and PQ behaves in opposite ways when scaling the\nembedding dimension. Furthermore, our analysis shows performance trends for PQ\nthat help guide optimal hyperparameter selection. Finally, we demonstrate that\nPQGAN can be seamlessly integrated into pre-trained diffusion models. This\nenables either a significantly faster and more compute-efficient generation, or\na doubling of the output resolution at no additional cost, positioning PQ as a\nstrong extension for discrete latent representation in image synthesis.",
        "url": "http://arxiv.org/abs/2510.03191v1",
        "published_date": "2025-10-03T17:17:38+00:00",
        "updated_date": "2025-10-03T17:17:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Denis Zavadski",
            "Nikita Philip Tatsch",
            "Carsten Rother"
        ],
        "tldr": "This paper introduces PQGAN, a product-quantized image autoencoder that improves upon VQGAN for high-fidelity image generation, offering significant improvements in reconstruction performance and enabling efficient integration with diffusion models for faster or higher-resolution synthesis.",
        "tldr_zh": "本文介绍了一种产品量化的图像自编码器PQGAN，它改进了VQGAN在高保真图像生成方面的性能，并在重建性能方面取得了显著提升，同时能够高效地集成到扩散模型中，以实现更快或更高分辨率的图像合成。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising",
        "summary": "Traditional denoising methods for noise removal have largely relied on\nhandcrafted priors, often perform well in controlled environments but struggle\nto address the complexity and variability of real noise. In contrast, deep\nlearning-based approaches have gained prominence for learning noise\ncharacteristics from large datasets, but these methods frequently require\nextensive labeled data and may not generalize effectively across diverse noise\ntypes and imaging conditions. In this paper, we present an innovative method,\ntermed as Net2Net, that combines the strengths of untrained and pre-trained\nnetworks to tackle the challenges of real-world noise removal. The innovation\nof Net2Net lies in its combination of unsupervised DIP and supervised\npre-trained model DRUNet by regularization by denoising (RED). The untrained\nnetwork adapts to the unique noise characteristics of each input image without\nrequiring labeled data, while the pre-trained network leverages learned\nrepresentations from large-scale datasets to deliver robust denoising\nperformance. This hybrid framework enhances generalization across varying noise\npatterns and improves performance, particularly in scenarios with limited\ntraining data. Extensive experiments on benchmark datasets demonstrate the\nsuperiority of our method for real-world noise removal.",
        "url": "http://arxiv.org/abs/2510.02733v1",
        "published_date": "2025-10-03T05:34:24+00:00",
        "updated_date": "2025-10-03T05:34:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weimin Yuan",
            "Cai Meng"
        ],
        "tldr": "The paper introduces Net2Net, a novel denoising method that combines untrained (DIP) and pre-trained (DRUNet) networks using regularization by denoising (RED) to address real-world noise removal challenges, especially with limited training data.",
        "tldr_zh": "该论文介绍了一种名为Net2Net的新型去噪方法，该方法结合了未经训练的（DIP）和预训练的（DRUNet）网络，使用基于去噪的正则化（RED）来解决实际噪声去除的挑战，尤其是在训练数据有限的情况下。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Mask2IV: Interaction-Centric Video Generation via Mask Trajectories",
        "summary": "Generating interaction-centric videos, such as those depicting humans or\nrobots interacting with objects, is crucial for embodied intelligence, as they\nprovide rich and diverse visual priors for robot learning, manipulation policy\ntraining, and affordance reasoning. However, existing methods often struggle to\nmodel such complex and dynamic interactions. While recent studies show that\nmasks can serve as effective control signals and enhance generation quality,\nobtaining dense and precise mask annotations remains a major challenge for\nreal-world use. To overcome this limitation, we introduce Mask2IV, a novel\nframework specifically designed for interaction-centric video generation. It\nadopts a decoupled two-stage pipeline that first predicts plausible motion\ntrajectories for both actor and object, then generates a video conditioned on\nthese trajectories. This design eliminates the need for dense mask inputs from\nusers while preserving the flexibility to manipulate the interaction process.\nFurthermore, Mask2IV supports versatile and intuitive control, allowing users\nto specify the target object of interaction and guide the motion trajectory\nthrough action descriptions or spatial position cues. To support systematic\ntraining and evaluation, we curate two benchmarks covering diverse action and\nobject categories across both human-object interaction and robotic manipulation\nscenarios. Extensive experiments demonstrate that our method achieves superior\nvisual realism and controllability compared to existing baselines.",
        "url": "http://arxiv.org/abs/2510.03135v1",
        "published_date": "2025-10-03T16:04:33+00:00",
        "updated_date": "2025-10-03T16:04:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Gen Li",
            "Bo Zhao",
            "Jianfei Yang",
            "Laura Sevilla-Lara"
        ],
        "tldr": "The paper introduces Mask2IV, a framework for generating interaction-centric videos by predicting actor and object motion trajectories, thus avoiding the need for dense mask inputs and improving controllability and realism. Two new benchmarks are introduced for evaluation.",
        "tldr_zh": "该论文介绍了Mask2IV，一个通过预测演员和物体运动轨迹来生成以互动为中心的视频的框架，从而避免了对密集掩码输入的需求，并提高了可控性和真实感。同时提出了两个新的基准数据集用于评估。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Dale meets Langevin: A Multiplicative Denoising Diffusion Model",
        "summary": "Gradient descent has proven to be a powerful and effective technique for\noptimization in numerous machine learning applications. Recent advances in\ncomputational neuroscience have shown that learning in standard gradient\ndescent optimization formulation is not consistent with learning in biological\nsystems. This has opened up interesting avenues for building biologically\ninspired learning techniques. One such approach is inspired by Dale's law,\nwhich states that inhibitory and excitatory synapses do not swap roles during\nthe course of learning. The resulting exponential gradient descent optimization\nscheme leads to log-normally distributed synaptic weights. Interestingly, the\ndensity that satisfies the Fokker-Planck equation corresponding to the\nstochastic differential equation (SDE) with geometric Brownian motion (GBM) is\nthe log-normal density. Leveraging this connection, we start with the SDE\ngoverning geometric Brownian motion, and show that discretizing the\ncorresponding reverse-time SDE yields a multiplicative update rule, which\nsurprisingly, coincides with the sampling equivalent of the exponential\ngradient descent update founded on Dale's law. Furthermore, we propose a new\nformalism for multiplicative denoising score-matching, subsuming the loss\nfunction proposed by Hyvaerinen for non-negative data. Indeed, log-normally\ndistributed data is positive and the proposed score-matching formalism turns\nout to be a natural fit. This allows for training of score-based models for\nimage data and results in a novel multiplicative update scheme for sample\ngeneration starting from a log-normal density. Experimental results on MNIST,\nFashion MNIST, and Kuzushiji datasets demonstrate generative capability of the\nnew scheme. To the best of our knowledge, this is the first instance of a\nbiologically inspired generative model employing multiplicative updates,\nfounded on geometric Brownian motion.",
        "url": "http://arxiv.org/abs/2510.02730v1",
        "published_date": "2025-10-03T05:23:33+00:00",
        "updated_date": "2025-10-03T05:23:33+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Nishanth Shetty",
            "Madhava Prasath",
            "Chandra Sekhar Seelamantula"
        ],
        "tldr": "This paper introduces a novel multiplicative denoising diffusion model inspired by Dale's law and geometric Brownian motion, demonstrating generative capabilities on image datasets. It connects biological inspiration with score-matching for positive data distributions.",
        "tldr_zh": "本文提出了一种新型的乘性去噪扩散模型，其灵感来自戴尔定律和几何布朗运动，并在图像数据集上展示了其生成能力。它将生物学灵感与正数据分布的得分匹配联系起来。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]