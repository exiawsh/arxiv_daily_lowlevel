[
    {
        "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
        "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.",
        "url": "http://arxiv.org/abs/2509.10441v1",
        "published_date": "2025-09-12T17:48:57+00:00",
        "updated_date": "2025-09-12T17:48:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Han",
            "Wanghan Xu",
            "Junchao Gong",
            "Xiaoyu Yue",
            "Song Guo",
            "Luping Zhou",
            "Lei Bai"
        ],
        "tldr": "The paper introduces InfGen, a novel approach to arbitrary resolution image generation that significantly reduces the computational cost and generation time compared to existing diffusion models by using a one-step generator to decode images from a fixed latent space.",
        "tldr_zh": "该论文介绍了InfGen，一种新的任意分辨率图像生成方法，通过使用一步生成器从固定潜在空间解码图像，与现有的扩散模型相比，显著降低了计算成本和生成时间。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching",
        "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
        "url": "http://arxiv.org/abs/2509.10312v1",
        "published_date": "2025-09-12T14:53:45+00:00",
        "updated_date": "2025-09-12T14:53:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhixin Zheng",
            "Xinyu Wang",
            "Chang Zou",
            "Shaobo Wang",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces Cluster-Driven Feature Caching (ClusCa) to accelerate diffusion transformers by spatially clustering tokens and computing features for only a representative subset, achieving significant speedups without retraining and even improving performance.",
        "tldr_zh": "该论文介绍了聚类驱动的特征缓存（ClusCa），通过对token进行空间聚类并仅计算代表性子集的特征来加速扩散Transformer，在无需重新训练的情况下实现了显著的加速，甚至提升了性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution",
        "summary": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.",
        "url": "http://arxiv.org/abs/2509.10122v1",
        "published_date": "2025-09-12T10:32:04+00:00",
        "updated_date": "2025-09-12T10:32:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zongliang Wu",
            "Siming Zheng",
            "Peng-Tao Jiang",
            "Xin Yuan"
        ],
        "tldr": "This paper introduces a Realism Controlled One-step Diffusion (RCOD) framework for real-world image super-resolution, enabling flexible control over fidelity-realism trade-offs with improved performance and efficiency compared to existing one-step diffusion methods.",
        "tldr_zh": "本文介绍了一种用于真实世界图像超分辨率的真实感控制单步扩散（RCOD）框架，与现有的单步扩散方法相比，它能够灵活地控制保真度和真实感之间的权衡，并提高了性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey",
        "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.",
        "url": "http://arxiv.org/abs/2509.09971v1",
        "published_date": "2025-09-12T05:16:54+00:00",
        "updated_date": "2025-09-12T05:16:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aupendu Kar",
            "Vishnu Raj",
            "Guan-Ming Su"
        ],
        "tldr": "This survey explores the fusion of event cameras with traditional frame-based cameras for visual media restoration and 3D reconstruction, focusing on deep learning contributions and openly available datasets. It aims to inspire further research in this area.",
        "tldr_zh": "该综述探讨了事件相机与传统帧相机的融合在视觉媒体修复和3D重建中的应用，重点关注深度学习的贡献和公开可用的数据集。旨在激发该领域的进一步研究。",
        "relevance_score": 9,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining",
        "summary": "Diffusion/score-based models have recently emerged as powerful generative\npriors for solving inverse problems, including accelerated MRI reconstruction.\nWhile their flexibility allows decoupling the measurement model from the\nlearned prior, their performance heavily depends on carefully tuned data\nfidelity weights, especially under fast sampling schedules with few denoising\nsteps. Existing approaches often rely on heuristics or fixed weights, which\nfail to generalize across varying measurement conditions and irregular timestep\nschedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling\n(ZADS), a test-time optimization method that adaptively tunes fidelity weights\nacross arbitrary noise schedules without requiring retraining of the diffusion\nprior. ZADS treats the denoising process as a fixed unrolled sampler and\noptimizes fidelity weights in a self-supervised manner using only undersampled\nmeasurements. Experiments on the fastMRI knee dataset demonstrate that ZADS\nconsistently outperforms both traditional compressed sensing and recent\ndiffusion-based methods, showcasing its ability to deliver high-fidelity\nreconstructions across varying noise schedules and acquisition settings.",
        "url": "http://arxiv.org/abs/2509.09880v1",
        "published_date": "2025-09-11T22:22:32+00:00",
        "updated_date": "2025-09-11T22:22:32+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "physics.med-ph"
        ],
        "authors": [
            "Yaşar Utku Alçalar",
            "Junno Yun",
            "Mehmet Akçakaya"
        ],
        "tldr": "This paper introduces Zero-shot Adaptive Diffusion Sampling (ZADS), a test-time optimization method for tuning data fidelity weights in diffusion-based inverse problem solvers without retraining the diffusion prior, demonstrating superior performance in fastMRI reconstruction.",
        "tldr_zh": "该论文介绍了一种零样本自适应扩散采样（ZADS）方法，用于在基于扩散的逆问题求解器中调整数据保真度权重，无需重新训练扩散先验，并在fastMRI重建中表现出卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI",
        "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability.",
        "url": "http://arxiv.org/abs/2509.10257v1",
        "published_date": "2025-09-12T13:59:23+00:00",
        "updated_date": "2025-09-12T13:59:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ema Masterl",
            "Tina Vipotnik Vesnaver",
            "Žiga Špiclin"
        ],
        "tldr": "This paper compares three super-resolution reconstruction (SRR) methods for fetal brain MRI, finding that NeSVoR demonstrates the best reconstruction success and that diagnostic performance for ventriculomegaly is resilient to SRR method choice despite volumetric variability.",
        "tldr_zh": "本文比较了三种用于胎儿脑部MRI的超分辨率重建 (SRR) 方法，发现 NeSVoR 表现出最佳的重建成功率，并且尽管体积存在差异，脑室扩大的诊断性能对 SRR 方法的选择具有弹性。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization",
        "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.",
        "url": "http://arxiv.org/abs/2509.10140v1",
        "published_date": "2025-09-12T11:08:21+00:00",
        "updated_date": "2025-09-12T11:08:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Chang",
            "Jie Qin",
            "Limeng Qiao",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Lin Ma",
            "Xingang Wang"
        ]
    }
]