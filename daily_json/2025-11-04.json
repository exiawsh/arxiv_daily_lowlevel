[
    {
        "title": "Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward",
        "summary": "Reinforcement Learning (RL) has recently been incorporated into diffusion\nmodels, e.g., tasks such as text-to-image. However, directly applying existing\nRL methods to diffusion-based image restoration models is suboptimal, as the\nobjective of restoration fundamentally differs from that of pure generation: it\nplaces greater emphasis on fidelity. In this paper, we investigate how to\neffectively integrate RL into diffusion-based restoration models. First,\nthrough extensive experiments with various reward functions, we find that an\neffective reward can be derived from an Image Quality Assessment (IQA) model,\ninstead of intuitive ground-truth-based supervision, which has already been\noptimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,\nour strategy focuses on using RL for challenging samples that are significantly\ndistant from the ground truth, and our RL approach is innovatively implemented\nusing MLLM-based IQA models to align distributions with high-quality images\ninitially. As the samples approach the ground truth's distribution, RL is\nadaptively combined with SFT for more fine-grained alignment. This dynamic\nprocess is facilitated through an automatic weighting strategy that adjusts\nbased on the relative difficulty of the training samples. Our strategy is\nplug-and-play that can be seamlessly applied to diffusion-based restoration\nmodels, boosting its performance across various restoration tasks. Extensive\nexperiments across multiple benchmarks demonstrate the effectiveness of our\nproposed RL framework.",
        "url": "http://arxiv.org/abs/2511.01645v1",
        "published_date": "2025-11-03T14:57:57+00:00",
        "updated_date": "2025-11-03T14:57:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaogang Xu",
            "Ruihang Chu",
            "Jian Wang",
            "Kun Zhou",
            "Wenjie Shu",
            "Harry Yang",
            "Ser-Nam Lim",
            "Hao Chen",
            "Liang Lin"
        ],
        "tldr": "This paper introduces a difficulty-adaptive reinforcement learning framework with IQA reward to enhance diffusion-based image restoration models, focusing on challenging samples and seamlessly integrating with existing SFT methods.",
        "tldr_zh": "该论文提出了一种基于IQA奖励的难度自适应强化学习框架，以增强基于扩散的图像修复模型，重点关注具有挑战性的样本，并与现有的SFT方法无缝集成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion",
        "summary": "Recent advancements in text-to-image diffusion models have significantly\nimproved the personalization and stylization of generated images. However,\nprevious studies have only assessed content similarity under a single style\nintensity. In our experiments, we observe that increasing style intensity leads\nto a significant loss of content features, resulting in a suboptimal\ncontent-style frontier. To address this, we propose a novel approach to expand\nthe content-style frontier by leveraging Content-Style Subspace Blending and a\nContent-Style Balance loss. Our method improves content similarity across\nvarying style intensities, significantly broadening the content-style frontier.\nExtensive experiments demonstrate that our approach outperforms existing\ntechniques in both qualitative and quantitative evaluations, achieving superior\ncontent-style trade-off with significantly lower Inverted Generational Distance\n(IGD) and Generational Distance (GD) scores compared to current methods.",
        "url": "http://arxiv.org/abs/2511.01355v1",
        "published_date": "2025-11-03T09:03:45+00:00",
        "updated_date": "2025-11-03T09:03:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linhao Huang"
        ],
        "tldr": "This paper introduces a novel approach using Content-Style Subspace Blending and a Content-Style Balance loss to improve content similarity in text-to-image generation across varying style intensities, thereby expanding the content-style frontier.",
        "tldr_zh": "本文提出了一种新方法，使用内容-风格子空间融合和内容-风格平衡损失，以提高文本到图像生成中不同风格强度下的内容相似性，从而扩展内容-风格边界。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers",
        "summary": "Ancient Chinese paintings are a valuable cultural heritage that is damaged by\nirreversible color degradation. Reviving color-degraded paintings is\nextraordinarily difficult due to the complex chemistry mechanism. Progress is\nfurther slowed by the lack of comprehensive, high-quality datasets, which\nhampers the creation of end-to-end digital restoration tools. To revive colors,\nwe propose PRevivor, a prior-guided color transformer that learns from recent\npaintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and\nSong Dynasty). To develop PRevivor, we decompose color restoration into two\nsequential sub-tasks: luminance enhancement and hue correction. For luminance\nenhancement, we employ two variational U-Nets and a multi-scale mapping module\nto translate faded luminance into restored counterparts. For hue correction, we\ndesign a dual-branch color query module guided by localized hue priors\nextracted from faded paintings. Specifically, one branch focuses attention on\nregions guided by masked priors, enforcing localized hue correction, whereas\nthe other branch remains unconstrained to maintain a global reasoning\ncapability. To evaluate PRevivor, we conduct extensive experiments against\nstate-of-the-art colorization methods. The results demonstrate superior\nperformance both quantitatively and qualitatively.",
        "url": "http://arxiv.org/abs/2511.01274v1",
        "published_date": "2025-11-03T06:47:56+00:00",
        "updated_date": "2025-11-03T06:47:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tan Tang",
            "Yanhong Wu",
            "Junming Gao",
            "Yingcai Wu"
        ],
        "tldr": "The paper introduces PRevivor, a prior-guided color transformer for restoring color-degraded ancient Chinese paintings, using luminance enhancement and hue correction sub-tasks.",
        "tldr_zh": "该论文介绍了PRevivor，一种先验引导的颜色转换器，用于恢复颜色退化的中国古画，通过亮度增强和色调校正两个子任务。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution",
        "summary": "Discrete Wavelet Transform (DWT) has been widely explored to enhance the\nperformance of image superresolution (SR). Despite some DWT-based methods\nimproving SR by capturing fine-grained frequency signals, most existing\napproaches neglect the interrelations among multiscale frequency sub-bands,\nresulting in inconsistencies and unnatural artifacts in the reconstructed\nimages. To address this challenge, we propose a Diffusion Transformer model\nbased on image Wavelet spectra for SR (DTWSR). DTWSR incorporates the\nsuperiority of diffusion models and transformers to capture the interrelations\namong multiscale frequency sub-bands, leading to a more consistence and\nrealistic SR image. Specifically, we use a Multi-level Discrete Wavelet\nTransform to decompose images into wavelet spectra. A pyramid tokenization\nmethod is proposed which embeds the spectra into a sequence of tokens for\ntransformer model, facilitating to capture features from both spatial and\nfrequency domain. A dual-decoder is designed elaborately to handle the distinct\nvariances in low-frequency and high-frequency sub-bands, without omitting their\nalignment in image generation. Extensive experiments on multiple benchmark\ndatasets demonstrate the effectiveness of our method, with high performance on\nboth perception quality and fidelity.",
        "url": "http://arxiv.org/abs/2511.01175v2",
        "published_date": "2025-11-03T02:56:58+00:00",
        "updated_date": "2025-11-04T05:16:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peng Du",
            "Hui Li",
            "Han Xu",
            "Paul Barom Jeon",
            "Dongwook Lee",
            "Daehyun Ji",
            "Ran Yang",
            "Feng Zhu"
        ],
        "tldr": "The paper introduces a Diffusion Transformer model based on wavelet spectra (DTWSR) for single image super-resolution, addressing limitations of existing DWT-based methods by capturing interrelations among multiscale frequency sub-bands, resulting in more consistent and realistic SR images.",
        "tldr_zh": "该论文提出了一种基于小波谱的扩散Transformer模型（DTWSR），用于单图像超分辨率，通过捕捉多尺度频率子带之间的相互关系，克服了现有基于DWT方法的局限性，从而产生更一致和真实的SR图像。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment",
        "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.",
        "url": "http://arxiv.org/abs/2511.01775v1",
        "published_date": "2025-11-03T17:28:54+00:00",
        "updated_date": "2025-11-03T17:28:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Zhen Chen",
            "Qing Xu",
            "Jinlin Wu",
            "Biao Yang",
            "Yuhao Zhai",
            "Geng Guo",
            "Jing Zhang",
            "Yinlu Ding",
            "Nassir Navab",
            "Jiebo Luo"
        ],
        "tldr": "This paper introduces SurgVeo, a benchmark for surgical video generation, and the Surgical Plausibility Pyramid (SPP) to evaluate video generation models like Veo-3, finding a 'plausibility gap' where models excel visually but lack causal understanding of surgical procedures.",
        "tldr_zh": "该论文介绍了SurgVeo（一个用于外科手术视频生成的基准）和手术合理性金字塔 (SPP)，用于评估像Veo-3这样的视频生成模型，发现了一个“合理性差距”，即模型在视觉上表现出色，但缺乏对外科手术流程的因果理解。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    },
    {
        "title": "Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond",
        "summary": "Under-display ToF imaging aims to achieve accurate depth sensing through a\nToF camera placed beneath a screen panel. However, transparent OLED (TOLED)\nlayers introduce severe degradations-such as signal attenuation, multi-path\ninterference (MPI), and temporal noise-that significantly compromise depth\nquality. To alleviate this drawback, we propose Learnable Fractional\nReaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the\nexpressive power of neural networks with the interpretability of physical\nmodeling. Specifically, we implement a time-fractional reaction-diffusion\nmodule that enables iterative depth refinement with dynamically generated\ndifferential orders, capturing long-term dependencies. In addition, we\nintroduce an efficient continuous convolution operator via coefficient\nprediction and repeated differentiation to further improve restoration quality.\nExperiments on four benchmark datasets demonstrate the effectiveness of our\napproach. The code is publicly available at https://github.com/wudiqx106/LFRD2.",
        "url": "http://arxiv.org/abs/2511.01704v1",
        "published_date": "2025-11-03T16:12:36+00:00",
        "updated_date": "2025-11-03T16:12:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Qiao",
            "Matteo Poggi",
            "Xing Wei",
            "Pengchao Deng",
            "Yanhui Zhou",
            "Stefano Mattoccia"
        ],
        "tldr": "The paper proposes a novel Learnable Fractional Reaction-Diffusion Dynamics (LFRD2) framework to address depth sensing degradation in under-display Time-of-Flight (ToF) imaging, combining neural networks with physical modeling for improved depth restoration.",
        "tldr_zh": "该论文提出了一种新颖的可学习分数阶反应扩散动力学（LFRD2）框架，旨在解决屏下飞行时间（ToF）成像中的深度传感退化问题，将神经网络与物理建模相结合，以提高深度恢复效果。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Example-Based Feature Painting on Textures",
        "summary": "In this work, we propose a system that covers the complete workflow for\nachieving controlled authoring and editing of textures that present distinctive\nlocal characteristics. These include various effects that change the surface\nappearance of materials, such as stains, tears, holes, abrasions,\ndiscoloration, and more. Such alterations are ubiquitous in nature, and\nincluding them in the synthesis process is crucial for generating realistic\ntextures. We introduce a novel approach for creating textures with such\nblemishes, adopting a learning-based approach that leverages unlabeled\nexamples. Our approach does not require manual annotations by the user;\ninstead, it detects the appearance-altering features through unsupervised\nanomaly detection. The various textural features are then automatically\nclustered into semantically coherent groups, which are used to guide the\nconditional generation of images. Our pipeline as a whole goes from a small\nimage collection to a versatile generative model that enables the user to\ninteractively create and paint features on textures of arbitrary size. Notably,\nthe algorithms we introduce for diffusion-based editing and infinite stationary\ntexture generation are generic and should prove useful in other contexts as\nwell. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html",
        "url": "http://arxiv.org/abs/2511.01513v1",
        "published_date": "2025-11-03T12:26:50+00:00",
        "updated_date": "2025-11-03T12:26:50+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Andrei-Timotei Ardelean",
            "Tim Weyrich"
        ],
        "tldr": "This paper introduces a novel unsupervised learning approach for controlled texture editing, allowing users to interactively create and paint features like stains and abrasions on textures of arbitrary size, leveraging diffusion-based editing.",
        "tldr_zh": "本文介绍了一种新颖的无监督学习方法，用于控制纹理编辑，允许用户交互式地创建和绘制诸如污渍和磨损之类的特征到任意大小的纹理上，并利用基于扩散的编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process",
        "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.",
        "url": "http://arxiv.org/abs/2511.01718v1",
        "published_date": "2025-11-03T16:26:54+00:00",
        "updated_date": "2025-11-03T16:26:54+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jiayi Chen",
            "Wenxuan Song",
            "Pengxiang Ding",
            "Ziyang Zhou",
            "Han Zhao",
            "Feilong Tang",
            "Donglin Wang",
            "Haoang Li"
        ],
        "tldr": "This paper introduces a Unified Diffusion Vision-Language-Action (VLA) model that uses a joint discrete denoising diffusion process to achieve state-of-the-art performance in embodied agent tasks, with faster inference compared to autoregressive methods.",
        "tldr_zh": "本文介绍了一种统一扩散视觉-语言-动作（VLA）模型，该模型使用联合离散去噪扩散过程在具身智能体任务中实现了最先进的性能，并且与自回归方法相比，推理速度更快。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]