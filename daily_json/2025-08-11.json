[
    {
        "title": "Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling",
        "summary": "Unsupervised real-world super-resolution (SR) faces critical challenges due\nto the complex, unknown degradation distributions in practical scenarios.\nExisting methods struggle to generalize from synthetic low-resolution (LR) and\nhigh-resolution (HR) image pairs to real-world data due to a significant domain\ngap. In this paper, we propose an unsupervised real-world SR method based on\nrectified flow to effectively capture and model real-world degradation,\nsynthesizing LR-HR training pairs with realistic degradation. Specifically,\ngiven unpaired LR and HR images, we propose a novel Rectified Flow Degradation\nModule (RFDM) that introduces degradation-transformed LR (DT-LR) images as\nintermediaries. By modeling the degradation trajectory in a continuous and\ninvertible manner, RFDM better captures real-world degradation and enhances the\nrealism of generated LR images. Additionally, we propose a Fourier Prior Guided\nDegradation Module (FGDM) that leverages structural information embedded in\nFourier phase components to ensure more precise modeling of real-world\ndegradation. Finally, the LR images are processed by both FGDM and RFDM,\nproducing final synthetic LR images with real-world degradation. The synthetic\nLR images are paired with the given HR images to train the off-the-shelf SR\nnetworks. Extensive experiments on real-world datasets demonstrate that our\nmethod significantly enhances the performance of existing SR approaches in\nreal-world scenarios.",
        "url": "http://arxiv.org/abs/2508.07214v1",
        "published_date": "2025-08-10T07:27:28+00:00",
        "updated_date": "2025-08-10T07:27:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongyang Zhou",
            "Xiaobin Zhu",
            "Liuling Chen",
            "Junyi He",
            "Jingyan Qin",
            "Xu-Cheng Yin",
            "Zhang xiaoxing"
        ],
        "tldr": "This paper proposes an unsupervised real-world super-resolution method using rectified flow to model complex degradations and generate realistic LR-HR training pairs, significantly enhancing SR performance on real-world datasets.",
        "tldr_zh": "本文提出了一种基于修正流的无监督真实世界超分辨率方法，该方法利用修正流来建模复杂的降质，并生成逼真的LR-HR训练对，从而显著提高真实世界数据集上的超分辨率性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal",
        "summary": "JPEG, as a widely used image compression standard, often introduces severe\nvisual artifacts when achieving high compression ratios. Although existing deep\nlearning-based restoration methods have made considerable progress, they often\nstruggle to recover complex texture details, resulting in over-smoothed\noutputs. To overcome these limitations, we propose SODiff, a novel and\nefficient semantic-oriented one-step diffusion model for JPEG artifacts\nremoval. Our core idea is that effective restoration hinges on providing\nsemantic-oriented guidance to the pre-trained diffusion model, thereby fully\nleveraging its powerful generative prior. To this end, SODiff incorporates a\nsemantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features\nfrom low-quality (LQ) images and projects them into an embedding space\nsemantically aligned with that of the text encoder. Simultaneously, it\npreserves crucial information for faithful reconstruction. Furthermore, we\npropose a quality factor-aware time predictor that implicitly learns the\ncompression quality factor (QF) of the LQ image and adaptively selects the\noptimal denoising start timestep for the diffusion process. Extensive\nexperimental results show that our SODiff outperforms recent leading methods in\nboth visual quality and quantitative metrics. Code is available at:\nhttps://github.com/frakenation/SODiff",
        "url": "http://arxiv.org/abs/2508.07346v1",
        "published_date": "2025-08-10T13:48:07+00:00",
        "updated_date": "2025-08-10T13:48:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tingyu Yang",
            "Jue Gong",
            "Jinpei Guo",
            "Wenbo Li",
            "Yong Guo",
            "Yulun Zhang"
        ],
        "tldr": "The paper introduces SODiff, a semantic-oriented diffusion model for JPEG artifact removal using a semantic-aligned image prompt extractor (SAIPE) and a quality factor-aware time predictor, claiming superior performance over existing methods.",
        "tldr_zh": "该论文介绍了一种语义导向的扩散模型SODiff，用于去除JPEG伪影。该模型使用语义对齐的图像提示提取器 (SAIPE) 和质量因子感知的时间预测器，声称优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation",
        "summary": "Spatial transcriptomics (ST) reveals spatial heterogeneity of gene\nexpression, yet its resolution is limited by current platforms. Recent methods\nenhance resolution via H&E-stained histology, but three major challenges\npersist: (1) isolating expression-relevant features from visually complex H&E\nimages; (2) achieving spatially precise multimodal alignment in diffusion-based\nframeworks; and (3) modeling gene-specific variation across expression\nchannels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST\nGeneration), a high-resolution ST generation framework conditioned on H&E\nimages and low-resolution ST. HaDM-ST includes: (i) a semantic distillation\nnetwork to extract predictive cues from H&E; (ii) a spatial alignment module\nenforcing pixel-wise correspondence with low-resolution ST; and (iii) a\nchannel-aware adversarial learner for fine-grained gene-level modeling.\nExperiments on 200 genes across diverse tissues and species show HaDM-ST\nconsistently outperforms prior methods, enhancing spatial fidelity and\ngene-level coherence in high-resolution ST predictions.",
        "url": "http://arxiv.org/abs/2508.07225v1",
        "published_date": "2025-08-10T08:09:06+00:00",
        "updated_date": "2025-08-10T08:09:06+00:00",
        "categories": [
            "cs.CV",
            "92C40, 68T07",
            "I.2.10; I.4.8"
        ],
        "authors": [
            "Xuepeng Liu",
            "Zheng Jiang",
            "Pinan Zhu",
            "Hanyu Liu",
            "Chao Li"
        ],
        "tldr": "The paper introduces HaDM-ST, a histology-assisted framework to generate high-resolution spatial transcriptomics data from low-resolution ST and H&E images, addressing challenges in feature extraction, alignment, and gene-level modeling.",
        "tldr_zh": "该论文介绍了HaDM-ST，一个利用组织学信息辅助的框架，可以从低分辨率空间转录组学数据和H&E图像生成高分辨率空间转录组学数据，解决了特征提取、对齐和基因水平建模方面的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset",
        "summary": "Image restoration has seen substantial progress in recent years. However,\nexisting methods often neglect depth information, which hurts similarity\nmatching, results in attention distractions in shallow depth-of-field (DoF)\nscenarios, and excessive enhancement of background content in deep DoF\nsettings. To overcome these limitations, we propose a novel Depth-Guided\nNetwork (DGN) for image restoration, together with a novel large-scale\nhigh-resolution dataset. Specifically, the network consists of two interactive\nbranches: a depth estimation branch that provides structural guidance, and an\nimage restoration branch that performs the core restoration task. In addition,\nthe image restoration branch exploits intra-object similarity through\nprogressive window-based self-attention and captures inter-object similarity\nvia sparse non-local attention. Through joint training, depth features\ncontribute to improved restoration quality, while the enhanced visual features\nfrom the restoration branch in turn help refine depth estimation. Notably, we\nalso introduce a new dataset for training and evaluation, consisting of 9,205\nhigh-resolution images from 403 plant species, with diverse depth and texture\nvariations. Extensive experiments show that our method achieves\nstate-of-the-art performance on several standard benchmarks and generalizes\nwell to unseen plant images, demonstrating its effectiveness and robustness.",
        "url": "http://arxiv.org/abs/2508.07211v1",
        "published_date": "2025-08-10T07:17:31+00:00",
        "updated_date": "2025-08-10T07:17:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junyi He",
            "Liuling Chen",
            "Hongyang Zhou",
            "Zhang xiaoxing",
            "Xiaobin Zhu",
            "Shengxiang Yu",
            "Jingyan Qin",
            "Xu-Cheng Yin"
        ],
        "tldr": "This paper introduces a novel Depth-Guided Network (DGN) for image restoration that leverages depth information for improved similarity matching and addresses limitations of existing methods. They also introduce a new high-resolution plant image dataset.",
        "tldr_zh": "该论文介绍了一种新颖的深度引导网络（DGN），用于图像恢复，利用深度信息来改进相似性匹配，并解决了现有方法的局限性。 他们还引入了一个新的高分辨率植物图像数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance",
        "summary": "Murals, as invaluable cultural artifacts, face continuous deterioration from\nenvironmental factors and human activities. Digital restoration of murals faces\nunique challenges due to their complex degradation patterns and the critical\nneed to preserve artistic authenticity. Existing learning-based methods\nstruggle with maintaining consistent mask guidance throughout their networks,\nleading to insufficient focus on damaged regions and compromised restoration\nquality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network\nthat addresses these limitations through comprehensive mask guidance and\nmulti-scale feature extraction. Our framework introduces two key components:\n(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask\nsensitivity across resolution scales through dedicated channel-wise feature\nselection and mask-guided feature fusion; and (2) the Co-Feature Aggregator\n(CFA), operating at both the highest and lowest resolutions to extract\ncomplementary features for capturing fine textures and global structures in\ndegraded regions. Experimental results on benchmark datasets demonstrate that\nCMAMRNet outperforms state-of-the-art methods, effectively preserving both\nstructural integrity and artistic details in restored murals. The code is\navailable\nat~\\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}.",
        "url": "http://arxiv.org/abs/2508.07140v1",
        "published_date": "2025-08-10T02:00:45+00:00",
        "updated_date": "2025-08-10T02:00:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yingtie Lei",
            "Fanghai Yi",
            "Yihang Dong",
            "Weihuang Liu",
            "Xiaofeng Zhang",
            "Zimeng Li",
            "Chi-Man Pun",
            "Xuhang Chen"
        ],
        "tldr": "The paper introduces CMAMRNet, a novel network for mural restoration that uses mask guidance and multi-scale feature extraction to address limitations in existing methods. It outperforms state-of-the-art approaches in preserving structural integrity and artistic details.",
        "tldr_zh": "该论文介绍了CMAMRNet，一种用于壁画修复的新型网络，它使用掩码引导和多尺度特征提取来解决现有方法的局限性。 在保持结构完整性和艺术细节方面，它优于最先进的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays",
        "summary": "Generative image models have achieved remarkable progress in both natural and\nmedical imaging. In the medical context, these techniques offer a potential\nsolution to data scarcity-especially for low-prevalence anomalies that impair\nthe performance of AI-driven diagnostic and segmentation tools. However,\nquestions remain regarding the fidelity and clinical utility of synthetic\nimages, since poor generation quality can undermine model generalizability and\ntrust. In this study, we evaluate the effectiveness of state-of-the-art\ngenerative models-Generative Adversarial Networks (GANs) and Diffusion Models\n(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:\nAtelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged\nCardiac Silhouette (ECS). Using a benchmark composed of real images from the\nMIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a\nreader study with three radiologists of varied experience. Participants were\nasked to distinguish real from synthetic images and assess the consistency\nbetween visual features and the target abnormality. Our results show that while\nDMs generate more visually realistic images overall, GANs can report better\naccuracy for specific conditions, such as absence of ECS. We further identify\nvisual cues radiologists use to detect synthetic images, offering insights into\nthe perceptual gaps in current models. These findings underscore the\ncomplementary strengths of GANs and DMs and point to the need for further\nrefinement to ensure generative models can reliably augment training datasets\nfor AI diagnostic systems.",
        "url": "http://arxiv.org/abs/2508.07128v1",
        "published_date": "2025-08-10T00:32:18+00:00",
        "updated_date": "2025-08-10T00:32:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Gregory Schuit",
            "Denis Parra",
            "Cecilia Besa"
        ],
        "tldr": "This paper evaluates the perceptual realism of GANs and Diffusion Models (DMs) in generating chest X-rays with specific abnormalities, finding that DMs produce more realistic images overall, while GANs may be more accurate for certain conditions. The study identifies perceptual gaps in these models to guide future development for reliable data augmentation.",
        "tldr_zh": "本文评估了GAN和扩散模型（DMs）在生成具有特定异常的胸部X光片时的感知真实性，发现DMs总体上生成更逼真的图像，而GAN在某些情况下可能更准确。该研究发现了这些模型中的感知差距，旨在指导未来的发展，以实现可靠的数据增强。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]