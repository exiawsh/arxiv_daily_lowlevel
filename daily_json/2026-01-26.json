[
    {
        "title": "Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study",
        "summary": "Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.",
        "url": "http://arxiv.org/abs/2601.17723v1",
        "published_date": "2026-01-25T07:09:20+00:00",
        "updated_date": "2026-01-25T07:09:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tayyab Nasir",
            "Daochang Liu",
            "Ajmal Mian"
        ],
        "tldr": "This paper presents an empirical study of INR-based single image super-resolution, comparing existing methods, analyzing the impact of training configurations, and proposing a new loss function for enhanced texture fidelity. It reveals insights about marginal gains of complex methods and the importance of training configurations.",
        "tldr_zh": "本文对基于隐式神经表示的单图像超分辨率技术进行了实证研究，比较了现有方法，分析了训练配置的影响，并提出了一种新的损失函数以增强纹理保真度。 它揭示了复杂方法带来的边际收益以及训练配置的重要性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting",
        "summary": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.",
        "url": "http://arxiv.org/abs/2601.17666v1",
        "published_date": "2026-01-25T03:07:17+00:00",
        "updated_date": "2026-01-25T03:07:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyue Pan",
            "Yuhao Chen",
            "Fengqing Zhu"
        ],
        "tldr": "This paper introduces Prompt Grafting (PG), a training-free method to improve compositional food image generation by addressing object entanglement in text-to-image diffusion models, enabling control over food separation and mixing.",
        "tldr_zh": "本文介绍了一种名为Prompt Grafting (PG)的免训练方法，通过解决文本到图像扩散模型中的对象纠缠问题，改进了组合食物图像的生成，从而能够控制食物的分离和混合。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]