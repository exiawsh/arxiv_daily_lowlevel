[
    {
        "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning",
        "summary": "Enhancing large vision-language models (LVLMs) with visual slow-thinking\nreasoning is crucial for solving complex multimodal tasks. However, since LVLMs\nare mainly trained with vision-language alignment, it is difficult to adopt\non-policy reinforcement learning (RL) to develop the slow thinking ability\nbecause the rollout space is restricted by its initial abilities. Off-policy RL\noffers a way to go beyond the current policy, but directly distilling\ntrajectories from external models may cause visual hallucinations due to\nmismatched visual perception abilities across models. To address these issues,\nthis paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for\nvision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy\nbehavior model by combining on-policy visual understanding from a trainable\nLVLM with off-policy slow-thinking reasoning from a language model, assigns\noutcome-based rewards to reasoning, and propagates visual rewards backward.\nThen LVLM learns slow-thinking reasoning ability from the obtained reasoning\ntrajectories using propagated rewards via off-policy RL algorithms. Extensive\nexperiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the\neffectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in\naverage, reaching state-of-the-art performance among open-source LVLMs on\nmultiple multimodal reasoning benchmarks, and even outperforms some\nclosed-source models (e.g., GPT-4.1) on the challenging MathVision and\nOlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.\nAnalysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy\nRL methods, offering a better policy initialization for further on-policy\ntraining.",
        "url": "http://arxiv.org/abs/2507.16814v1",
        "published_date": "2025-07-22T17:59:34+00:00",
        "updated_date": "2025-07-22T17:59:34+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Junhao Shen",
            "Haiteng Zhao",
            "Yuzhe Gu",
            "Songyang Gao",
            "Kuikun Liu",
            "Haian Huang",
            "Jianfei Gao",
            "Dahua Lin",
            "Wenwei Zhang",
            "Kai Chen"
        ],
        "tldr": "The paper introduces SOPHIA, a semi-off-policy RL method to improve vision-language slow-thinking reasoning in LVLMs, achieving state-of-the-art performance on several benchmarks and even surpassing some closed-source models on challenging tasks.",
        "tldr_zh": "该论文介绍了SOPHIA，一种半离策略强化学习方法，用于提高LVLM中的视觉语言慢思考推理能力，在多个基准测试中实现了最先进的性能，甚至在具有挑战性的任务上超越了一些闭源模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning",
        "summary": "Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce $\\textbf{Zebra-CoT}$, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT.",
        "url": "http://arxiv.org/abs/2507.16746v1",
        "published_date": "2025-07-22T16:35:36+00:00",
        "updated_date": "2025-07-22T16:35:36+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Ang Li",
            "Charles Wang",
            "Kaiyu Yue",
            "Zikui Cai",
            "Ollie Liu",
            "Deqing Fu",
            "Peng Guo",
            "Wang Bill Zhu",
            "Vatsal Sharan",
            "Robin Jia",
            "Willie Neiswanger",
            "Furong Huang",
            "Tom Goldstein",
            "Micah Goldblum"
        ],
        "tldr": "The paper introduces Zebra-CoT, a large-scale dataset for interleaved vision-language reasoning, and demonstrates its effectiveness by fine-tuning VLMs, achieving significant performance gains on various tasks.",
        "tldr_zh": "该论文介绍了Zebra-CoT，一个用于交错视觉语言推理的大规模数据集。通过微调视觉语言模型，在各项任务上取得了显著的性能提升，证明了其有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation",
        "summary": "The application of Vision-language foundation models (VLFMs) to remote\nsensing (RS) imagery has garnered significant attention due to their superior\ncapability in various downstream tasks. A key challenge lies in the scarcity of\nhigh-quality, large-scale, image-text paired training data. Recently, several\nworks introduced extensive image-text datasets for RS and trained their VLFMs.\nHowever, due to the rudimentary methods used for generating captions, the\nquality of datasets is suboptimal, requiring larger volumes of training data,\nwhile only yielding modest performance improvements. In this paper, we propose\na two-stage method named MpGI(Multi-Perspective Generation and Integration) for\ngenerating high-quality text captions for RS images. Firstly, we generate\ndistinct and detailed descriptions from different perspectives using\nRule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs\ngeneration methods. Next, we utilize Large Language Models (LLMs) to integrate\nthese diverse descriptions into comprehensive captions, capturing details from\nmultiple perspectives. Finally, we have created the HQRS-IT-210K dataset,\nincluding about 210,000 RS images and 1.3 million captions. We fine-tuned two\nVLFMs using our dataset: CLIP, a discriminative model, and CoCa, an\nimage-to-text generative model. This process resulted in our proposed HQRS-CLIP\nand RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed\nthe previous SOTA RS CLIP model in various downstream tasks while using only\n4.2\\% of the training data. RS-CoCa outperforms other advanced approaches\nacross benchmark datasets and can generate captions for RS images that rival or\neven exceed manual annotations. Dataset, pre-trained models, and codes will be\nreleased at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.",
        "url": "http://arxiv.org/abs/2507.16716v1",
        "published_date": "2025-07-22T15:54:53+00:00",
        "updated_date": "2025-07-22T15:54:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiguo He",
            "Junjie Zhu",
            "Yiying Li",
            "Xiaoyu Zhang",
            "Chunping Qiu",
            "Jun Wang",
            "Qiangjuan Huang",
            "Ke Yang"
        ],
        "tldr": "This paper introduces a two-stage method, MpGI, for generating high-quality image-text datasets for remote sensing (RS) using MLLMs and LLMs, resulting in the HQRS-IT-210K dataset and improved performance of fine-tuned VLFMs like CLIP and CoCa.",
        "tldr_zh": "该论文提出了一种两阶段方法 MpGI，利用 MLLM 和 LLM 生成高质量的遥感图像文本数据集，得到了 HQRS-IT-210K 数据集，并提高了微调后的 VLFMs（如 CLIP 和 CoCa）的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models",
        "summary": "New era has unlocked exciting possibilities for extending Large Language\nModels (LLMs) to tackle 3D vision-language tasks. However, most existing 3D\nmultimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or\nsegmenting independent objects to perform these tasks, which limits their\nspatial awareness due to insufficient representation of the richness inherent\nin 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D\nMLLM specifically designed to enhance spatial awareness for 3D vision-language\ntasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM\nintegrates an LLM backbone with a progressive spatial awareness scheme that\nprogressively captures spatial information as the perception field expands,\ngenerating location-enriched 3D scene embeddings to serve as visual prompts.\nFurthermore, we introduce two novel tasks: 3D object distance measurement and\n3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate\nthe model's spatial awareness capabilities. Experimental results demonstrate\nthat Spatial 3D-LLM achieves state-of-the-art performance across a wide range\nof 3D vision-language tasks, revealing the improvements stemmed from our\nprogressive spatial awareness scheme of mining more profound spatial\ninformation. Our code is available at\nhttps://github.com/bjshuyuan/Spatial-3D-LLM.",
        "url": "http://arxiv.org/abs/2507.16524v1",
        "published_date": "2025-07-22T12:32:35+00:00",
        "updated_date": "2025-07-22T12:32:35+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaoyan Wang",
            "Zeju Li",
            "Yifan Xu",
            "Jiaxing Qi",
            "Zhifei Yang",
            "Ruifei Ma",
            "Xiangde Liu",
            "Chao Zhang"
        ],
        "tldr": "The paper introduces Spatial 3D-LLM, a 3D vision-language model designed to enhance spatial awareness by using a progressive spatial awareness scheme to enrich 3D scene embeddings, and evaluates it on novel 3D tasks using a newly constructed dataset.",
        "tldr_zh": "该论文介绍了Spatial 3D-LLM，一种旨在通过使用渐进式空间感知方案来丰富3D场景嵌入，从而增强空间感知的3D视觉语言模型，并使用新构建的数据集在新的3D任务上对其进行评估。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning",
        "summary": "Recent advances in multimodal large language models (MLLMs) have shown\nimpressive reasoning capabilities. However, further enhancing existing MLLMs\nnecessitates high-quality vision-language datasets with carefully curated task\ncomplexities, which are both costly and challenging to scale. Although recent\nself-improving models that iteratively refine themselves offer a feasible\nsolution, they still suffer from two core challenges: (i) most existing methods\naugment visual or textual data separately, resulting in discrepancies in data\ncomplexity (e.g., over-simplified diagrams paired with redundant textual\ndescriptions); and (ii) the evolution of data and models is also separated,\nleading to scenarios where models are exposed to tasks with mismatched\ndifficulty levels. To address these issues, we propose C2-Evo, an automatic,\nclosed-loop self-improving framework that jointly evolves both training data\nand model capabilities. Specifically, given a base dataset and a base model,\nC2-Evo enhances them by a cross-modal data evolution loop and a data-model\nevolution loop. The former loop expands the base dataset by generating complex\nmultimodal problems that combine structured textual sub-problems with\niteratively specified geometric diagrams, while the latter loop adaptively\nselects the generated problems based on the performance of the base model, to\nconduct supervised fine-tuning and reinforcement learning alternately.\nConsequently, our method continuously refines its model and training data, and\nconsistently obtains considerable performance gains across multiple\nmathematical reasoning benchmarks. Our code, models, and datasets will be\nreleased.",
        "url": "http://arxiv.org/abs/2507.16518v2",
        "published_date": "2025-07-22T12:27:08+00:00",
        "updated_date": "2025-07-29T07:40:20+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Xiuwei Chen",
            "Wentao Hu",
            "Hanhui Li",
            "Jun Zhou",
            "Zisheng Chen",
            "Meng Cao",
            "Yihan Zeng",
            "Kui Zhang",
            "Yu-Jie Yuan",
            "Jianhua Han",
            "Hang Xu",
            "Xiaodan Liang"
        ],
        "tldr": "The paper introduces C2-Evo, a closed-loop self-improving framework for multimodal large language models that jointly evolves training data and model capabilities to enhance reasoning performance, particularly in mathematical contexts.",
        "tldr_zh": "该论文介绍了C2-Evo，一个闭环自提升框架，用于多模态大型语言模型，它联合进化训练数据和模型能力，以提高推理性能，尤其是在数学领域。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting",
        "summary": "The exponential growth of video content has made personalized video\nhighlighting an essential task, as user preferences are highly variable and\ncomplex. Existing video datasets, however, often lack personalization, relying\non isolated videos or simple text queries that fail to capture the intricacies\nof user behavior. In this work, we introduce HIPPO-Video, a novel dataset for\npersonalized video highlighting, created using an LLM-based user simulator to\ngenerate realistic watch histories reflecting diverse user preferences. The\ndataset includes 2,040 (watch history, saliency score) pairs, covering 20,400\nvideos across 170 semantic categories. To validate our dataset, we propose\nHiPHer, a method that leverages these personalized watch histories to predict\npreference-conditioned segment-wise saliency scores. Through extensive\nexperiments, we demonstrate that our method outperforms existing generic and\nquery-based approaches, showcasing its potential for highly user-centric video\nhighlighting in real-world scenarios.",
        "url": "http://arxiv.org/abs/2507.16873v1",
        "published_date": "2025-07-22T08:24:33+00:00",
        "updated_date": "2025-07-22T08:24:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jeongeun Lee",
            "Youngjae Yu",
            "Dongha Lee"
        ],
        "tldr": "The paper introduces HIPPO-Video, a new dataset for personalized video highlighting generated using an LLM-based user simulator, and a method, HiPHer, that leverages this data to predict preference-conditioned video saliency, outperforming existing approaches.",
        "tldr_zh": "本文介绍HIPPO-Video，一个使用基于LLM的用户模拟器生成的新型个性化视频高亮数据集，以及一种名为HiPHer的方法，该方法利用此数据来预测偏好条件下的视频显著性，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Quality Text, Robust Vision: The Role of Language in Enhancing Visual Robustness of Vision-Language Models",
        "summary": "Defending pre-trained vision-language models (VLMs), such as CLIP, against\nadversarial attacks is crucial, as these models are widely used in diverse\nzero-shot tasks, including image classification. However, existing adversarial\ntraining (AT) methods for robust fine-tuning largely overlook the role of\nlanguage in enhancing visual robustness. Specifically, (1) supervised AT\nmethods rely on short texts (e.g., class labels) to generate adversarial\nperturbations, leading to overfitting to object classes in the training data,\nand (2) unsupervised AT avoids this overfitting but remains suboptimal against\npractical text-guided adversarial attacks due to its lack of semantic guidance.\nTo address these limitations, we propose Quality Text-guided Adversarial\nFine-Tuning (QT-AFT), which leverages high-quality captions during training to\nguide adversarial examples away from diverse semantics present in images. This\nenables the visual encoder to robustly recognize a broader range of image\nfeatures even under adversarial noise, thereby enhancing robustness across\ndiverse downstream tasks. QT-AFT overcomes the key weaknesses of prior methods\n-- overfitting in supervised AT and lack of semantic awareness in unsupervised\nAT -- achieving state-of-the-art zero-shot adversarial robustness and clean\naccuracy, evaluated across 16 zero-shot datasets. Furthermore, our\ncomprehensive study uncovers several key insights into the role of language in\nenhancing vision robustness; for example, describing object properties in\naddition to object names further enhances zero-shot robustness. Our findings\npoint to an urgent direction for future work -- centering high-quality\nlinguistic supervision in robust visual representation learning.",
        "url": "http://arxiv.org/abs/2507.16257v1",
        "published_date": "2025-07-22T06:13:30+00:00",
        "updated_date": "2025-07-22T06:13:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Futa Waseda",
            "Saku Sugawara",
            "Isao Echizen"
        ],
        "tldr": "The paper introduces QT-AFT, a novel adversarial fine-tuning method for VLMs that uses high-quality captions to improve visual robustness against adversarial attacks, achieving state-of-the-art zero-shot performance.",
        "tldr_zh": "本文提出了 QT-AFT，一种新颖的 VLM 对抗性微调方法，它使用高质量的字幕来提高视觉鲁棒性，以对抗对抗性攻击，从而实现了最先进的零样本性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote Sensing",
        "summary": "Natural disasters cause devastating damage to communities and infrastructure\nevery year. Effective disaster response is hampered by the difficulty of\naccessing affected areas during and after events. Remote sensing has allowed us\nto monitor natural disasters in a remote way. More recently there have been\nadvances in computer vision and deep learning that help automate satellite\nimagery analysis, However, they remain limited by their narrow focus on\nspecific disaster types, reliance on manual expert interpretation, and lack of\ndatasets with sufficient temporal granularity or natural language annotations\nfor tracking disaster progression. We present MONITRS, a novel multimodal\ndataset of more than 10,000 FEMA disaster events with temporal satellite\nimagery and natural language annotations from news articles, accompanied by\ngeotagged locations, and question-answer pairs. We demonstrate that fine-tuning\nexisting MLLMs on our dataset yields significant performance improvements for\ndisaster monitoring tasks, establishing a new benchmark for machine\nlearning-assisted disaster response systems. Code can be found at:\nhttps://github.com/ShreelekhaR/MONITRS",
        "url": "http://arxiv.org/abs/2507.16228v1",
        "published_date": "2025-07-22T04:59:09+00:00",
        "updated_date": "2025-07-22T04:59:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shreelekha Revankar",
            "Utkarsh Mall",
            "Cheng Perng Phoo",
            "Kavita Bala",
            "Bharath Hariharan"
        ],
        "tldr": "The paper introduces MONITRS, a new multimodal dataset for disaster monitoring with temporal satellite imagery, news articles, and question-answer pairs, and demonstrates its effectiveness by fine-tuning MLLMs for disaster response tasks.",
        "tldr_zh": "该论文介绍了MONITRS，一个用于灾害监测的新型多模态数据集，包含时序卫星图像、新闻文章和问答对。并通过微调MLLM来展示其在灾害响应任务中的有效性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Advancing Visual Large Language Model for Multi-granular Versatile Perception",
        "summary": "Perception is a fundamental task in the field of computer vision,\nencompassing a diverse set of subtasks that can be systematically categorized\ninto four distinct groups based on two dimensions: prediction type and\ninstruction type. Notably, existing researches often focus solely on a limited\nsubset of these potential combinations, which constrains their applicability\nand versatility across various contexts. In response to this challenge, we\npresent MVP-LM, a Multi-granular and Versatile Perception framework\nincorporating Visual Large Language Model. Our framework is designed to\nintegrate both word-based and sentence-based perception tasks alongside box and\nmask predictions within a single architecture. MVP-LM features an innovative\nmulti-granularity decoder in conjunction with a CoT-inspired dataset\nunification strategy, enabling seamless supervised fine-tuning across a wide\nspectrum of tasks, including but not limited to panoptic segmentation,\ndetection, grounding, and referring expression segmentation. Furthermore, we\nintroduce a query enhancement strategy aimed at harnessing the decoding and\ngenerative capabilities inherent in VLLMs. Extensive experiments conducted\nacross a range of benchmarks in both word-based and sentence-based perception\ntasks substantiate the efficacy of our framework. The code will be available at\nhttps://github.com/xiangwentao666/MVP-LM.",
        "url": "http://arxiv.org/abs/2507.16213v1",
        "published_date": "2025-07-22T04:09:14+00:00",
        "updated_date": "2025-07-22T04:09:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wentao Xiang",
            "Haoxian Tan",
            "Cong Wei",
            "Yujie Zhong",
            "Dengjie Li",
            "Yujiu Yang"
        ],
        "tldr": "The paper introduces MVP-LM, a Visual Large Language Model framework designed for multi-granular and versatile perception tasks, unifying word-based and sentence-based tasks within a single architecture and demonstrating strong performance across various benchmarks.",
        "tldr_zh": "该论文介绍了一种名为MVP-LM的视觉大型语言模型框架，旨在实现多粒度和通用的感知任务，将基于单词和基于句子的任务统一在一个架构中，并在各种基准测试中表现出强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs",
        "summary": "The rapid advancement of Text-guided Image Editing (TIE) enables image\nmodifications through text prompts. However, current TIE models still struggle\nto balance image quality, editing alignment, and consistency with the original\nimage, limiting their practical applications. Existing TIE evaluation\nbenchmarks and metrics have limitations on scale or alignment with human\nperception. To this end, we introduce EBench-18K, the first large-scale image\nEditing Benchmark including 18K edited images with fine-grained human\npreference annotations for evaluating TIE. Specifically, EBench-18K includes\n1,080 source images with corresponding editing prompts across 21 tasks, 18K+\nedited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion\nscores (MOSs) assessed from three evaluation dimensions, and 18K+\nquestion-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs\nto assess edited images, while the evaluation results, in turn, provide\ninsights into assessing the alignment between the LMMs' understanding ability\nand human preferences. Then, we propose LMM4Edit, a LMM-based metric for\nevaluating image Editing models from perceptual quality, editing alignment,\nattribute preservation, and task-specific QA accuracy in an all-in-one manner.\nExtensive experiments show that LMM4Edit achieves outstanding performance and\naligns well with human preference. Zero-shot validation on the other datasets\nalso shows the generalization ability of our model. The dataset and code are\navailable at https://github.com/IntMeGroup/LMM4Edit.",
        "url": "http://arxiv.org/abs/2507.16193v1",
        "published_date": "2025-07-22T03:11:07+00:00",
        "updated_date": "2025-07-22T03:11:07+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Zitong Xu",
            "Huiyu Duan",
            "Bingnan Liu",
            "Guangji Ma",
            "Jiarui Wang",
            "Liu Yang",
            "Shiqi Gao",
            "Xiaoyu Wang",
            "Jia Wang",
            "Xiongkuo Min",
            "Guangtao Zhai",
            "Weisi Lin"
        ],
        "tldr": "This paper introduces EBench-18K, a large-scale benchmark for evaluating text-guided image editing, and LMM4Edit, an LMM-based metric that aligns well with human preference for image editing evaluation.",
        "tldr_zh": "本文介绍了 EBench-18K，一个用于评估文本引导图像编辑的大规模基准，以及 LMM4Edit，一个基于 LMM 的指标，与人类图像编辑评估偏好高度一致。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning",
        "summary": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.",
        "url": "http://arxiv.org/abs/2507.16815v1",
        "published_date": "2025-07-22T17:59:46+00:00",
        "updated_date": "2025-07-22T17:59:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Chi-Pin Huang",
            "Yueh-Hua Wu",
            "Min-Hung Chen",
            "Yu-Chiang Frank Wang",
            "Fu-En Yang"
        ],
        "tldr": "ThinkAct proposes a dual-system framework for vision-language-action reasoning that uses reinforced visual latent planning to improve long-horizon planning and adaptation in embodied AI tasks.",
        "tldr_zh": "ThinkAct 提出了一种视觉-语言-动作推理的双系统框架，该框架利用强化视觉潜在规划来改进具身人工智能任务中的长时程规划和适应性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation",
        "summary": "Desktop accessibility metadata enables AI agents to interpret screens and\nsupports users who depend on tools like screen readers. Yet, many applications\nremain largely inaccessible due to incomplete or missing metadata provided by\ndevelopers - our investigation shows that only 33% of applications on macOS\noffer full accessibility support. While recent work on structured screen\nrepresentation has primarily addressed specific challenges, such as UI element\ndetection or captioning, none has attempted to capture the full complexity of\ndesktop interfaces by replicating their entire hierarchical structure. To\nbridge this gap, we introduce Screen2AX, the first framework to automatically\ncreate real-time, tree-structured accessibility metadata from a single\nscreenshot. Our method uses vision-language and object detection models to\ndetect, describe, and organize UI elements hierarchically, mirroring macOS's\nsystem-level accessibility structure. To tackle the limited availability of\ndata for macOS desktop applications, we compiled and publicly released three\ndatasets encompassing 112 macOS applications, each annotated for UI element\ndetection, grouping, and hierarchical accessibility metadata alongside\ncorresponding screenshots. Screen2AX accurately infers hierarchy trees,\nachieving a 77% F1 score in reconstructing a complete accessibility tree.\nCrucially, these hierarchy trees improve the ability of autonomous agents to\ninterpret and interact with complex desktop interfaces. We introduce\nScreen2AX-Task, a benchmark specifically designed for evaluating autonomous\nagent task execution in macOS desktop environments. Using this benchmark, we\ndemonstrate that Screen2AX delivers a 2.2x performance improvement over native\naccessibility representations and surpasses the state-of-the-art OmniParser V2\nsystem on the ScreenSpot benchmark.",
        "url": "http://arxiv.org/abs/2507.16704v1",
        "published_date": "2025-07-22T15:38:12+00:00",
        "updated_date": "2025-07-22T15:38:12+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Viktor Muryn",
            "Marta Sumyk",
            "Mariya Hirna",
            "Sofiya Garkot",
            "Maksym Shamrai"
        ],
        "tldr": "Screen2AX is a vision-based framework for automatically generating accessibility metadata for macOS applications from screenshots, improving the performance of autonomous agents interacting with desktop interfaces and releasing datasets for macOS accessibility research.",
        "tldr_zh": "Screen2AX是一个基于视觉的框架，可以从屏幕截图自动生成macOS应用程序的可访问性元数据，从而提高自主代理与桌面界面交互的性能，并发布用于macOS可访问性研究的数据集。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Automatic Fine-grained Segmentation-assisted Report Generation",
        "summary": "Reliable end-to-end clinical report generation has been a longstanding goal\nof medical ML research. The end goal for this process is to alleviate\nradiologists' workloads and provide second opinions to clinicians or patients.\nThus, a necessary prerequisite for report generation models is a strong general\nperformance and some type of innate grounding capability, to convince\nclinicians or patients of the veracity of the generated reports. In this paper,\nwe present ASaRG (\\textbf{A}utomatic \\textbf{S}egmentation-\\textbf{a}ssisted\n\\textbf{R}eport \\textbf{G}eneration), an extension of the popular LLaVA\narchitecture that aims to tackle both of these problems. ASaRG proposes to fuse\nintermediate features and fine-grained segmentation maps created by specialist\nradiological models into LLaVA's multi-modal projection layer via simple\nconcatenation. With a small number of added parameters, our approach achieves a\n+0.89\\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA\nbaseline when using only intermediate features, and +2.77\\% performance gain\n($p<0.001$) when adding a combination of intermediate features and fine-grained\nsegmentation maps. Compared with COMG and ORID, two other report generation\nmethods that utilize segmentations, the performance gain amounts to 6.98\\% and\n6.28\\% in F1 score, respectively. ASaRG is not mutually exclusive with other\nchanges made to the LLaVA architecture, potentially allowing our method to be\ncombined with other advances in the field. Finally, the use of an arbitrary\nnumber of segmentations as part of the input demonstrably allows tracing\nelements of the report to the corresponding segmentation maps and verifying the\ngroundedness of assessments. Our code will be made publicly available at a\nlater date.",
        "url": "http://arxiv.org/abs/2507.16623v1",
        "published_date": "2025-07-22T14:16:20+00:00",
        "updated_date": "2025-07-22T14:16:20+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Frederic Jonske",
            "Constantin Seibold",
            "Osman Alperen Koras",
            "Fin Bahnsen",
            "Marie Bauer",
            "Amin Dada",
            "Hamza Kalisch",
            "Anton Schily",
            "Jens Kleesiek"
        ],
        "tldr": "The paper introduces ASaRG, an extension of LLaVA that incorporates fine-grained segmentation maps into the multi-modal projection layer for improved clinical report generation, demonstrating performance gains over LLaVA, COMG, and ORID.",
        "tldr_zh": "该论文介绍了ASaRG，它是LLaVA的扩展，通过将精细分割图整合到多模态投影层中，从而改进了临床报告生成，并且在性能上优于LLaVA、COMG和ORID。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot Action Recognition",
        "summary": "Few-shot action recognition (FSAR) aims to classify human actions in videos\nwith only a small number of labeled samples per category. The scarcity of\ntraining data has driven recent efforts to incorporate additional modalities,\nparticularly text. However, the subtle variations in human posture, motion\ndynamics, and the object interactions that occur during different phases, are\ncritical inherent knowledge of actions that cannot be fully exploited by action\nlabels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a\nnovel framework that goes beyond label semantics by leveraging Large Language\nModels (LLMs) to dissect the essential representational characteristics hidden\nbeneath action labels. Guided by the prior knowledge encoded in LLM, LGA\neffectively captures rich spatiotemporal cues in few-shot scenarios.\nSpecifically, for text, we prompt an off-the-shelf LLM to anatomize labels into\nsequences of atomic action descriptions, focusing on the three core elements of\naction (subject, motion, object). For videos, a Visual Anatomy Module segments\nactions into atomic video phases to capture the sequential structure of\nactions. A fine-grained fusion strategy then integrates textual and visual\nfeatures at the atomic level, resulting in more generalizable prototypes.\nFinally, we introduce a Multimodal Matching mechanism, comprising both\nvideo-video and video-text matching, to ensure robust few-shot classification.\nExperimental results demonstrate that LGA achieves state-of-the-art performance\nacross multipe FSAR benchmarks.",
        "url": "http://arxiv.org/abs/2507.16287v1",
        "published_date": "2025-07-22T07:16:25+00:00",
        "updated_date": "2025-07-22T07:16:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zefeng Qian",
            "Xincheng Yao",
            "Yifei Huang",
            "Chongyang Zhang",
            "Jiangyong Ying",
            "Hong Sun"
        ],
        "tldr": "This paper introduces Language-Guided Action Anatomy (LGA), a framework that uses LLMs to dissect action labels into atomic descriptions and segments videos into atomic phases, then fuses them for improved few-shot action recognition.",
        "tldr_zh": "本文介绍了一种名为语言引导动作解剖 (LGA) 的框架，该框架使用 LLM 将动作标签分解为原子描述，并将视频分割成原子阶段，然后融合它们以改进少样本动作识别。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling",
        "summary": "Recent advancements in unified image generation models, such as OmniGen, have\nenabled the handling of diverse image generation and editing tasks within a\nsingle framework, accepting multimodal, interleaved texts and images in free\nform. This unified architecture eliminates the need for text encoders, greatly\nreducing model complexity and standardizing various image generation and\nediting tasks, making it more user-friendly. However, we found that it suffers\nfrom text instruction neglect, especially when the text instruction contains\nmultiple sub-instructions. To explore this issue, we performed a perturbation\nanalysis on the input to identify critical steps and layers. By examining the\ncross-attention maps of these key steps, we observed significant conflicts\nbetween neglected sub-instructions and the activations of the input image. In\nresponse, we propose Self-Adaptive Attention Scaling (SaaS), a method that\nleverages the consistency of cross-attention between adjacent timesteps to\ndynamically scale the attention activation for each sub-instruction. Our SaaS\nenhances instruction-following fidelity without requiring additional training\nor test-time optimization. Experimental results on instruction-based image\nediting and visual conditional image generation validate the effectiveness of\nour SaaS, showing superior instruction-following fidelity over existing\nmethods. The code is available https://github.com/zhouchao-ops/SaaS.",
        "url": "http://arxiv.org/abs/2507.16240v1",
        "published_date": "2025-07-22T05:25:38+00:00",
        "updated_date": "2025-07-22T05:25:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chao Zhou",
            "Tianyi Wei",
            "Nenghai Yu"
        ],
        "tldr": "This paper introduces Self-Adaptive Attention Scaling (SaaS) to improve the instruction-following fidelity of unified image generation models, particularly in handling multiple sub-instructions, without requiring additional training.",
        "tldr_zh": "本文介绍了一种自适应注意力缩放（SaaS）方法，用于提高统一图像生成模型的指令遵循保真度，尤其是在处理多个子指令时，而无需额外的训练。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]