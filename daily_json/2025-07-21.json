[
    {
        "title": "Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression",
        "summary": "Multimodal Large Language Models (MLLMs) show promise for image-based\nregression tasks, but current approaches face key limitations. Recent methods\nfine-tune MLLMs using preset output vocabularies and generic task-level prompts\n(e.g., \"How would you rate this image?\"), assuming this mimics human rating\nbehavior. Our analysis reveals these approaches provide no benefit over\nimage-only training. Models using preset vocabularies and generic prompts\nperform equivalently to image-only models, failing to leverage semantic\nunderstanding from textual input. We propose Regression via Transformer-Based\nClassification (RvTC), which replaces vocabulary-constrained classification\nwith a flexible bin-based approach. Unlike approaches that address\ndiscretization errors through complex distributional modeling, RvTC eliminates\nmanual vocabulary crafting through straightforward bin increase, achieving\nstate-of-the-art performance on four image assessment datasets using only\nimages. More importantly, we demonstrate that data-specific prompts\ndramatically improve performance. Unlike generic task descriptions, prompts\ncontaining semantic information about specific images enable MLLMs to leverage\ncross-modal understanding. On the AVA dataset, adding challenge titles to\nprompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We\ndemonstrate through empirical evidence from the AVA and AGIQA-3k datasets that\nMLLMs benefit from semantic prompt information surpassing mere statistical\nbiases. This underscores the importance of incorporating meaningful textual\ncontext in multimodal regression tasks.",
        "url": "http://arxiv.org/abs/2507.14997v1",
        "published_date": "2025-07-20T15:05:24+00:00",
        "updated_date": "2025-07-20T15:05:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Roy H. Jennings",
            "Genady Paikin",
            "Roy Shaul",
            "Evgeny Soloveichik"
        ],
        "tldr": "The paper introduces RvTC, a novel approach for image-based regression in MLLMs that utilizes bin-based classification and data-specific prompts to significantly improve performance, demonstrating the importance of semantic context.",
        "tldr_zh": "该论文介绍了一种名为RvTC的新方法，用于多模态大语言模型中的图像回归任务。该方法采用基于bin的分类和数据相关的提示，显著提升了性能，并证明了语义上下文的重要性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hierarchical Cross-modal Prompt Learning for Vision-Language Models",
        "summary": "Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent\ngeneralization abilities. However, adapting these large-scale models to\ndownstream tasks while preserving their generalization capabilities remains\nchallenging. Although prompt learning methods have shown promise, they suffer\nfrom two fundamental bottlenecks that limit generalization: (a) modality\nisolation, and (b) hierarchical semantic decay. To address these limitations,\nwe propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that\nestablishes bidirectional knowledge flow between text and vision modalities,\nenabling them to refine their semantics mutually. HiCroPL routes knowledge\nflows by leveraging the complementary strengths of text and vision. In early\nlayers, text prompts inject relatively clear semantics into visual prompts\nthrough a hierarchical knowledge mapper, enhancing the representation of\nlow-level visual semantics. In later layers, visual prompts encoding specific\ntask-relevant objects flow back to refine text prompts, enabling deeper\nalignment. Crucially, our hierarchical knowledge mapper allows representations\nat multi-scales to be fused, ensuring that deeper representations retain\ntransferable shallow semantics thereby enhancing generalization. We further\nintroduce a lightweight layer-specific knowledge proxy to enable efficient\ncross-modal interactions. Extensive evaluations across four tasks demonstrate\nHiCroPL's superior performance, achieving state-of-the-art results on 11\nbenchmarks with significant improvements. Code is available at:\nhttps://github.com/zzeoZheng/HiCroPL.",
        "url": "http://arxiv.org/abs/2507.14976v1",
        "published_date": "2025-07-20T14:18:04+00:00",
        "updated_date": "2025-07-20T14:18:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Zheng",
            "Shunzhi Yang",
            "Zhuoxin He",
            "Jinfeng Yang",
            "Zhenhua Huang"
        ],
        "tldr": "The paper introduces HiCroPL, a hierarchical cross-modal prompt learning framework for VLMs that improves generalization by establishing bidirectional knowledge flow between text and vision modalities, achieving state-of-the-art results on various benchmarks.",
        "tldr_zh": "该论文提出了HiCroPL，一个用于视觉-语言模型的层级跨模态提示学习框架，通过建立文本和视觉模态之间的双向知识流来提高泛化能力，并在多个基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP",
        "summary": "3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.",
        "url": "http://arxiv.org/abs/2507.14904v1",
        "published_date": "2025-07-20T10:28:06+00:00",
        "updated_date": "2025-07-20T10:28:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fan Li",
            "Zanyi Wang",
            "Zeyi Huang",
            "Guang Dai",
            "Jingdong Wang",
            "Mengmeng Wang"
        ],
        "tldr": "The paper introduces TriCLIP-3D, a parameter-efficient framework for 3D visual grounding that uses a unified 2D pre-trained CLIP model to process RGB images, text, and point clouds, achieving performance improvements with fewer trainable parameters.",
        "tldr_zh": "该论文介绍了 TriCLIP-3D，一个用于 3D 视觉接地的参数高效框架，它使用统一的 2D 预训练 CLIP 模型来处理 RGB 图像、文本和点云，从而以更少的训练参数实现性能改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs",
        "summary": "Universal multimodal retrieval (UMR), which aims to address complex retrieval\ntasks where both queries and candidates span diverse modalities, has been\nsignificantly advanced by the emergence of MLLMs. While state-of-the-art\nMLLM-based methods in the literature predominantly adopt contrastive learning\nprinciples, they often differ in their specific training recipes. Despite their\nsuccess, the mechanisms underlying their retrieval capabilities remain largely\nunexplored, potentially resulting in suboptimal performance and limited\ngeneralization ability. To address these issues, we present a comprehensive\nstudy aimed at uncovering the key factors that drive effective embedding\nlearning for UMR using MLLMs. We begin by implementing a general MLLM-based\nembedding learning pipeline, and systematically analyze the primary\ncontributors to high-performing universal retrieval systems. Based on this, we\nexplore various aspects of the details in embedding generation and training\nstrategies, including progressive transition, hard negative mining and\nre-ranker distillation. Notably, our findings reveal that often-overlooked\nfactors can have a substantial impact on model performance. Building on these\ndiscoveries, we introduce a unified framework termed U-MARVEL\n(\\textbf{U}niversal \\textbf{M}ultimod\\textbf{A}l \\textbf{R}etrie\\textbf{V}al\nvia \\textbf{E}mbedding \\textbf{L}earning), which outperforms state-of-the-art\ncompetitors on the M-BEIR benchmark by a large margin in supervised settings,\nand also exihibits strong zero-shot performance on several tasks such as\ncomposed image retrieval and text-to-video retrieval. These results underscore\nthe generalization potential of our framework across various embedding-based\nretrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL",
        "url": "http://arxiv.org/abs/2507.14902v1",
        "published_date": "2025-07-20T10:27:34+00:00",
        "updated_date": "2025-07-20T10:27:34+00:00",
        "categories": [
            "cs.IR",
            "cs.CV"
        ],
        "authors": [
            "Xiaojie Li",
            "Chu Li",
            "Shi-Zhe Chen",
            "Xi Chen"
        ],
        "tldr": "The paper introduces U-MARVEL, a unified framework for universal multimodal retrieval (UMR) using MLLMs, and identifies key factors for effective embedding learning, outperforming SOTA on M-BEIR and demonstrating strong zero-shot performance.",
        "tldr_zh": "该论文介绍了U-MARVEL，一个使用MLLM的通用多模态检索(UMR)统一框架，并确定了有效嵌入学习的关键因素，在M-BEIR上优于SOTA，并表现出强大的零样本性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering",
        "summary": "Video Question Answering (VideoQA) requires identifying sparse critical\nmoments in long videos and reasoning about their causal relationships to answer\nsemantically complex questions. While recent advances in multimodal learning\nhave improved alignment and fusion, current approaches remain limited by two\nprevalent but fundamentally flawed strategies: (1) task-agnostic sampling\nindiscriminately processes all frames, overwhelming key events with irrelevant\ncontent; and (2) heuristic retrieval captures superficial patterns but misses\ncausal-temporal structures needed for complex reasoning. To address these\nchallenges, we introduce LeAdQA, an innovative approach that bridges these gaps\nthrough synergizing causal-aware query refinement with fine-grained visual\ngrounding. Our method first leverages LLMs to reformulate question-option\npairs, resolving causal ambiguities and sharpening temporal focus. These\nrefined queries subsequently direct a temporal grounding model to precisely\nretrieve the most salient segments, complemented by an adaptive fusion\nmechanism dynamically integrating the evidence to maximize relevance. The\nintegrated visual-textual cues are then processed by an MLLM to generate\naccurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and\nNExT-GQA demonstrate that our method's precise visual grounding substantially\nenhances the understanding of video-question relationships, achieving\nstate-of-the-art (SOTA) performance on complex reasoning tasks while\nmaintaining computational efficiency.",
        "url": "http://arxiv.org/abs/2507.14784v1",
        "published_date": "2025-07-20T01:57:00+00:00",
        "updated_date": "2025-07-20T01:57:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinxin Dong",
            "Baoyun Peng",
            "Haokai Ma",
            "Yufei Wang",
            "Zixuan Dong",
            "Fei Hu",
            "Xiaodong Wang"
        ],
        "tldr": "The paper introduces LeAdQA, a method using LLMs for question refinement and temporal grounding to improve VideoQA, achieving SOTA results on complex reasoning tasks with enhanced visual-textual understanding.",
        "tldr_zh": "该论文介绍了一种名为LeAdQA的方法，该方法利用LLM进行问题提炼和时间定位，以改进视频问答（VideoQA）的效果。通过增强视觉-文本理解，在复杂的推理任务上实现了最先进的（SOTA）结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis",
        "summary": "Non-destructive testing (NDT), particularly X-ray inspection, is vital for\nindustrial quality assurance, yet existing deep-learning-based approaches often\nlack interactivity, interpretability, and the capacity for critical\nself-assessment, limiting their reliability and operator trust. To address\nthese shortcomings, this paper proposes InsightX Agent, a novel LMM-based\nagentic framework designed to deliver reliable, interpretable, and interactive\nX-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent\npositions a Large Multimodal Model (LMM) as a central orchestrator,\ncoordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the\nEvidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect\nregion proposals for multi-scale feature maps and sparsifies them through\nNon-Maximum Suppression (NMS), optimizing detection of small, dense targets in\nX-ray images while maintaining computational efficiency. The EGR tool guides\nthe LMM agent through a chain-of-thought-inspired review process, incorporating\ncontext assessment, individual defect analysis, false positive elimination,\nconfidence recalibration and quality assurance to validate and refine the\nSDMSD's initial proposals. By strategically employing and intelligently using\ntools, InsightX Agent moves beyond passive data processing to active reasoning,\nenhancing diagnostic reliability and providing interpretations that integrate\ndiverse information sources. Experimental evaluations on the GDXray+ dataset\ndemonstrate that InsightX Agent not only achieves a high object detection\nF1-score of 96.35% but also offers significantly improved interpretability and\ntrustworthiness in its analyses, highlighting the transformative potential of\nagentic LLM frameworks for industrial inspection tasks.",
        "url": "http://arxiv.org/abs/2507.14899v1",
        "published_date": "2025-07-20T10:23:22+00:00",
        "updated_date": "2025-07-20T10:23:22+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiale Liu",
            "Huan Wang",
            "Yue Zhang",
            "Xiaoyu Luo",
            "Jiaxiang Hu",
            "Zhiliang Liu",
            "Min Xie"
        ],
        "tldr": "The paper introduces InsightX Agent, an LMM-based agentic framework for X-ray NDT analysis, which leverages a Large Multimodal Model to coordinate a multi-scale defect detector and an evidence-grounded reflection tool, achieving high detection accuracy and improved interpretability.",
        "tldr_zh": "该论文介绍了InsightX Agent，一个基于LMM的智能体框架，用于X射线无损检测分析。该框架利用大型多模态模型协调多尺度缺陷检测器和基于证据的反思工具，实现了高检测精度和改进的可解释性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models",
        "summary": "Large vision-language models (LVLMs) have made significant progress in chart\nunderstanding. However, financial charts, characterized by complex temporal\nstructures and domain-specific terminology, remain notably underexplored. We\nintroduce FinChart-Bench, the first benchmark specifically focused on\nreal-world financial charts. FinChart-Bench comprises 1,200 financial chart\nimages collected from 2015 to 2024, each annotated with True/False (TF),\nMultiple Choice (MC), and Question Answering (QA) questions, totaling 7,016\nquestions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs\non FinChart-Bench. Our evaluation reveals critical insights: (1) the\nperformance gap between open-source and closed-source models is narrowing, (2)\nperformance degradation occurs in upgraded models within families, (3) many\nmodels struggle with instruction following, (4) both advanced models show\nsignificant limitations in spatial reasoning abilities, and (5) current LVLMs\nare not reliable enough to serve as automated evaluators. These findings\nhighlight important limitations in current LVLM capabilities for financial\nchart understanding. The FinChart-Bench dataset is available at\nhttps://huggingface.co/datasets/Tizzzzy/FinChart-Bench.",
        "url": "http://arxiv.org/abs/2507.14823v1",
        "published_date": "2025-07-20T05:00:42+00:00",
        "updated_date": "2025-07-20T05:00:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dong Shu",
            "Haoyang Yuan",
            "Yuchen Wang",
            "Yanguang Liu",
            "Huopu Zhang",
            "Haiyan Zhao",
            "Mengnan Du"
        ],
        "tldr": "The paper introduces FinChart-Bench, a new benchmark dataset for evaluating VLMs on financial chart understanding, and evaluates 25 state-of-the-art VLMs, revealing limitations in their spatial reasoning and instruction following abilities when applied to this domain.",
        "tldr_zh": "该论文介绍了FinChart-Bench，一个新的基准数据集，用于评估VLMs在金融图表理解方面的能力。该论文评估了25个最先进的VLMs，揭示了它们在应用于该领域时，在空间推理和指令遵循方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Light Future: Multimodal Action Frame Prediction via InstructPix2Pix",
        "summary": "Predicting future motion trajectories is a critical capability across domains\nsuch as robotics, autonomous systems, and human activity forecasting, enabling\nsafer and more intelligent decision-making. This paper proposes a novel,\nefficient, and lightweight approach for robot action prediction, offering\nsignificantly reduced computational cost and inference latency compared to\nconventional video prediction models. Importantly, it pioneers the adaptation\nof the InstructPix2Pix model for forecasting future visual frames in robotic\ntasks, extending its utility beyond static image editing. We implement a deep\nlearning-based visual prediction framework that forecasts what a robot will\nobserve 100 frames (10 seconds) into the future, given a current image and a\ntextual instruction. We repurpose and fine-tune the InstructPix2Pix model to\naccept both visual and textual inputs, enabling multimodal future frame\nprediction. Experiments on the RoboTWin dataset (generated based on real-world\nscenarios) demonstrate that our method achieves superior SSIM and PSNR compared\nto state-of-the-art baselines in robot action prediction tasks. Unlike\nconventional video prediction models that require multiple input frames, heavy\ncomputation, and slow inference latency, our approach only needs a single image\nand a text prompt as input. This lightweight design enables faster inference,\nreduced GPU demands, and flexible multimodal control, particularly valuable for\napplications like robotics and sports motion trajectory analytics, where motion\ntrajectory precision is prioritized over visual fidelity.",
        "url": "http://arxiv.org/abs/2507.14809v1",
        "published_date": "2025-07-20T03:57:18+00:00",
        "updated_date": "2025-07-20T03:57:18+00:00",
        "categories": [
            "cs.CV",
            "cs.MM",
            "cs.RO",
            "I.2.10; I.4.8"
        ],
        "authors": [
            "Zesen Zhong",
            "Duomin Zhang",
            "Yijia Li"
        ],
        "tldr": "This paper introduces a lightweight approach for robot action prediction using a modified InstructPix2Pix model, enabling multimodal future frame prediction with significantly reduced computational cost compared to traditional video prediction methods.",
        "tldr_zh": "本文介绍了一种轻量级的机器人动作预测方法，该方法使用改进的InstructPix2Pix模型，能够进行多模态的未来帧预测，并且与传统的视频预测方法相比，显著降低了计算成本。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories",
        "summary": "In intensive care units (ICUs), patients with complex clinical conditions\nrequire vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a\nvital diagnostic tool, providing insights into clinical trajectories, but their\nirregular acquisition limits their utility. Existing tools for CXR\ninterpretation are constrained by cross-sectional analysis, failing to capture\ntemporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal\nframework that integrates temporally sparse CXR imaging and radiology reports\nwith high-frequency clinical data, such as vital signs, laboratory values, and\nrespiratory flow sheets, to predict the trajectory of CXR findings in\ncritically ill patients. CXR-TFT leverages latent embeddings from a vision\nencoder that are temporally aligned with hourly clinical data through\ninterpolation. A transformer model is then trained to predict CXR embeddings at\neach hour, conditioned on previous embeddings and clinical measurements. In a\nretrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy\nin forecasting abnormal CXR findings up to 12 hours before they became\nradiographically evident. This predictive capability in clinical data holds\nsignificant potential for enhancing the management of time-sensitive conditions\nlike acute respiratory distress syndrome, where early intervention is crucial\nand diagnoses are often delayed. By providing distinctive temporal resolution\nin prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights\nthat can directly improve clinical outcomes.",
        "url": "http://arxiv.org/abs/2507.14766v1",
        "published_date": "2025-07-19T22:42:26+00:00",
        "updated_date": "2025-07-19T22:42:26+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Mehak Arora",
            "Ayman Ali",
            "Kaiyuan Wu",
            "Carolyn Davis",
            "Takashi Shimazui",
            "Mahmoud Alwakeel",
            "Victor Moas",
            "Philip Yang",
            "Annette Esper",
            "Rishikesan Kamaleswaran"
        ],
        "tldr": "The paper introduces CXR-TFT, a multi-modal transformer framework that predicts chest X-ray trajectories by integrating CXR images, radiology reports, and clinical data, demonstrating high accuracy in forecasting abnormal CXR findings in ICU patients.",
        "tldr_zh": "该论文介绍了CXR-TFT，一个多模态Transformer框架，通过整合胸部X光片图像、放射学报告和临床数据来预测胸部X光片轨迹，并在ICU患者中展示了预测异常X光片结果的高精度。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 6
    }
]