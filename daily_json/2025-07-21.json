[
    {
        "title": "Grounding Degradations in Natural Language for All-In-One Video Restoration",
        "summary": "In this work, we propose an all-in-one video restoration framework that\ngrounds degradation-aware semantic context of video frames in natural language\nvia foundation models, offering interpretable and flexible guidance. Unlike\nprior art, our method assumes no degradation knowledge in train or test time\nand learns an approximation to the grounded knowledge such that the foundation\nmodel can be safely disentangled during inference adding no extra cost.\nFurther, we call for standardization of benchmarks in all-in-one video\nrestoration, and propose two benchmarks in multi-degradation setting,\nthree-task (3D) and four-task (4D), and two time-varying composite degradation\nbenchmarks; one of the latter being our proposed dataset with varying snow\nintensity, simulating how weather degradations affect videos naturally. We\ncompare our method with prior works and report state-of-the-art performance on\nall benchmarks.",
        "url": "http://arxiv.org/abs/2507.14851v1",
        "published_date": "2025-07-20T07:43:33+00:00",
        "updated_date": "2025-07-20T07:43:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Muhammad Kamran Janjua",
            "Amirhosein Ghasemabadi",
            "Kunlin Zhang",
            "Mohammad Salameh",
            "Chao Gao",
            "Di Niu"
        ],
        "tldr": "The paper introduces an all-in-one video restoration framework using natural language grounding via foundation models, requiring no degradation knowledge during training or testing. It also proposes new benchmarks for multi-degradation video restoration, reporting state-of-the-art performance.",
        "tldr_zh": "该论文提出了一个利用自然语言和基础模型进行视频修复的框架，无需任何训练或测试时的退化信息。同时，该论文还提出了新的多重退化视频修复的基准，并报告了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Paired Image Generation with Diffusion-Guided Diffusion Models",
        "summary": "The segmentation of mass lesions in digital breast tomosynthesis (DBT) images\nis very significant for the early screening of breast cancer. However, the\nhigh-density breast tissue often leads to high concealment of the mass lesions,\nwhich makes manual annotation difficult and time-consuming. As a result, there\nis a lack of annotated data for model training. Diffusion models are commonly\nused for data augmentation, but the existing methods face two challenges.\nFirst, due to the high concealment of lesions, it is difficult for the model to\nlearn the features of the lesion area. This leads to the low generation quality\nof the lesion areas, thus limiting the quality of the generated images. Second,\nexisting methods can only generate images and cannot generate corresponding\nannotations, which restricts the usability of the generated images in\nsupervised training. In this work, we propose a paired image generation method.\nThe method does not require external conditions and can achieve the generation\nof paired images by training an extra diffusion guider for the conditional\ndiffusion model. During the experimental phase, we generated paired DBT slices\nand mass lesion masks. Then, we incorporated them into the supervised training\nprocess of the mass lesion segmentation task. The experimental results show\nthat our method can improve the generation quality without external conditions.\nMoreover, it contributes to alleviating the shortage of annotated data, thus\nenhancing the performance of downstream tasks.",
        "url": "http://arxiv.org/abs/2507.14833v1",
        "published_date": "2025-07-20T06:13:02+00:00",
        "updated_date": "2025-07-20T06:13:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haoxuan Zhang",
            "Wenju Cui",
            "Yuzhu Cao",
            "Tao Tan",
            "Jie Liu",
            "Yunsong Peng",
            "Jian Zheng"
        ],
        "tldr": "This paper introduces a diffusion-guided diffusion model for paired image generation (DBT slices and lesion masks) to address the lack of annotated data for breast cancer mass lesion segmentation, improving generation quality and downstream segmentation performance without external conditions.",
        "tldr_zh": "本文提出了一种扩散引导的扩散模型，用于配对图像生成（DBT切片和病灶掩模），旨在解决乳腺癌肿块病灶分割中缺乏注释数据的问题，从而在没有外部条件的情况下提高生成质量和下游分割性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models",
        "summary": "Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.",
        "url": "http://arxiv.org/abs/2507.14811v3",
        "published_date": "2025-07-20T04:00:53+00:00",
        "updated_date": "2025-07-29T02:42:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiaji Zhang",
            "Ruichao Sun",
            "Hailiang Zhao",
            "Jiaju Wu",
            "Peng Chen",
            "Hao Li",
            "Yuying Liu",
            "Kingsum Chow",
            "Gang Xiong",
            "Shuiguang Deng"
        ],
        "tldr": "The paper introduces SegQuant, a semantics-aware and generalizable post-training quantization framework for diffusion models, aiming to reduce computational costs without retraining, while maintaining performance across different architectures.",
        "tldr_zh": "该论文提出了 SegQuant，一个语义感知和可泛化的扩散模型后训练量化框架，旨在降低计算成本而无需重新训练，同时保持不同架构的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring Scalable Unified Modeling for General Low-Level Vision",
        "summary": "Low-level vision involves a wide spectrum of tasks, including image\nrestoration, enhancement, stylization, and feature extraction, which differ\nsignificantly in both task formulation and output domains. To address the\nchallenge of unified modeling across such diverse tasks, we propose a Visual\ntask Prompt-based Image Processing (VPIP) framework that leverages input-target\nimage pairs as visual prompts to guide the model in performing a variety of\nlow-level vision tasks. The framework comprises an end-to-end image processing\nbackbone, a prompt encoder, and a prompt interaction module, enabling flexible\nintegration with various architectures and effective utilization of\ntask-specific visual representations. Based on this design, we develop a\nunified low-level vision model, GenLV, and evaluate its performance across\nmultiple representative tasks. To explore the scalability of this approach, we\nextend the framework along two dimensions: model capacity and task diversity.\nWe construct a large-scale benchmark consisting of over 100 low-level vision\ntasks and train multiple versions of the model with varying scales.\nExperimental results show that the proposed method achieves considerable\nperformance across a wide range of tasks. Notably, increasing the number of\ntraining tasks enhances generalization, particularly for tasks with limited\ndata, indicating the model's ability to learn transferable representations\nthrough joint training. Further evaluations in zero-shot generalization,\nfew-shot transfer, and task-specific fine-tuning scenarios demonstrate the\nmodel's strong adaptability, confirming the effectiveness, scalability, and\npotential of the proposed framework as a unified foundation for general\nlow-level vision modeling.",
        "url": "http://arxiv.org/abs/2507.14801v1",
        "published_date": "2025-07-20T03:22:52+00:00",
        "updated_date": "2025-07-20T03:22:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Chen",
            "Kaiwen Zhu",
            "Yuandong Pu",
            "Shuo Cao",
            "Xiaohui Li",
            "Wenlong Zhang",
            "Yihao Liu",
            "Yu Qiao",
            "Jiantao Zhou",
            "Chao Dong"
        ],
        "tldr": "The paper introduces VPIP, a visual prompt-based framework for unified low-level vision modeling (GenLV), demonstrating strong performance and scalability across a wide range of tasks, with improved generalization through joint training.",
        "tldr_zh": "本文介绍了一个名为VPIP的视觉提示框架，用于统一的底层视觉建模 (GenLV)，展示了在广泛任务中的强大性能和可扩展性，并通过联合训练提高了泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models",
        "summary": "Diffusion models (DMs) have achieved state-of-the-art generative performance\nbut suffer from high sampling latency due to their sequential denoising nature.\nExisting solver-based acceleration methods often face image quality degradation\nunder a low-latency budget. In this paper, we propose the Ensemble Parallel\nDirection solver (dubbed as \\ours), a novel ODE solver that mitigates\ntruncation errors by incorporating multiple parallel gradient evaluations in\neach ODE step. Importantly, since the additional gradient computations are\nindependent, they can be fully parallelized, preserving low-latency sampling.\n  Our method optimizes a small set of learnable parameters in a distillation\nfashion, ensuring minimal training overhead.\n  In addition, our method can serve as a plugin to improve existing ODE\nsamplers. Extensive experiments on various image synthesis benchmarks\ndemonstrate the effectiveness of our \\ours~in achieving high-quality and\nlow-latency sampling. For example, at the same latency level of 5 NFE, EPD\nachieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26\non LSUN Bedroom, surpassing existing learning-based solvers by a significant\nmargin. Codes are available in https://github.com/BeierZhu/EPD.",
        "url": "http://arxiv.org/abs/2507.14797v1",
        "published_date": "2025-07-20T03:08:06+00:00",
        "updated_date": "2025-07-20T03:08:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Beier Zhu",
            "Ruoyu Wang",
            "Tong Zhao",
            "Hanwang Zhang",
            "Chi Zhang"
        ],
        "tldr": "The paper introduces Ensemble Parallel Direction solver (EPD), a novel ODE solver for diffusion models that leverages parallel gradient evaluations to reduce sampling latency while maintaining high image quality. It outperforms existing methods at low NFEs.",
        "tldr_zh": "该论文提出了一种名为Ensemble Parallel Direction solver (EPD) 的新型ODE求解器，用于扩散模型，它利用并行梯度计算来降低采样延迟，同时保持高质量的图像生成。在低NFE下，其性能优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring",
        "summary": "Underwater image enhancement is vital for marine conservation, particularly\ncoral reef monitoring. However, AI-based enhancement models often face dataset\nbias, high computational costs, and lack of transparency, leading to potential\nmisinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware\nAI framework to address these challenges. EBA-AI leverages CLIP embeddings to\ndetect and mitigate dataset bias, ensuring balanced representation across\nvaried underwater environments. It also integrates adaptive processing to\noptimize energy efficiency, significantly reducing GPU usage while maintaining\ncompetitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100\nshow that while PSNR drops by a controlled 1.0 dB, computational savings enable\nreal-time feasibility for large-scale marine monitoring. Additionally,\nuncertainty estimation and explainability techniques enhance trust in AI-driven\nenvironmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,\nWaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing\nefficiency, fairness, and interpretability in underwater image processing. By\naddressing key limitations of AI-driven enhancement, this work contributes to\nsustainable, bias-aware, and computationally efficient marine conservation\nefforts. For interactive visualizations, animations, source code, and access to\nthe preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/",
        "url": "http://arxiv.org/abs/2507.15036v1",
        "published_date": "2025-07-20T16:37:37+00:00",
        "updated_date": "2025-07-20T16:37:37+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Lyes Saad Saoud",
            "Irfan Hussain"
        ],
        "tldr": "This paper introduces EBA-AI, an ethics-guided and bias-aware AI framework for underwater image enhancement that balances efficiency, fairness, and interpretability, enabling real-time marine monitoring.",
        "tldr_zh": "该论文介绍了EBA-AI，一个伦理引导和偏差感知的AI框架，用于水下图像增强，它平衡了效率、公平性和可解释性，从而实现实时海洋监测。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Systole-Conditioned Generative Cardiac Motion",
        "summary": "Accurate motion estimation in cardiac computed tomography (CT) imaging is\ncritical for assessing cardiac function and surgical planning. Data-driven\nmethods have become the standard approach for dense motion estimation, but they\nrely on vast amounts of labeled data with dense ground-truth (GT) motion\nannotations, which are often unfeasible to obtain. To address this limitation,\nwe present a novel approach that synthesizes realistically looking pairs of\ncardiac CT frames enriched with dense 3D flow field annotations.\n  Our method leverages a conditional Variational Autoencoder (CVAE), which\nincorporates a novel multi-scale feature conditioning mechanism and is trained\nto generate 3D flow fields conditioned on a single CT frame. By applying the\ngenerated flow field to warp the given frame, we create pairs of frames that\nsimulate realistic myocardium deformations across the cardiac cycle. These\npairs serve as fully annotated data samples, providing optical flow GT\nannotations. Our data generation pipeline could enable the training and\nvalidation of more complex and accurate myocardium motion models, allowing for\nsubstantially reducing reliance on manual annotations.\n  Our code, along with animated generated samples and additional material, is\navailable on our project page:\nhttps://shaharzuler.github.io/GenerativeCardiacMotion_Page.",
        "url": "http://arxiv.org/abs/2507.15894v1",
        "published_date": "2025-07-20T14:44:40+00:00",
        "updated_date": "2025-07-20T14:44:40+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shahar Zuler",
            "Gal Lifshitz",
            "Hadar Averbuch-Elor",
            "Dan Raviv"
        ],
        "tldr": "This paper proposes a conditional VAE-based method to generate synthetic cardiac CT image pairs with dense 3D flow field annotations for training and validating myocardium motion models, aiming to reduce the need for manual annotations.",
        "tldr_zh": "本文提出了一种基于条件VAE的方法，用于生成带有密集3D流场注释的合成心脏CT图像对，以训练和验证心肌运动模型，旨在减少对人工标注的需求。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems",
        "summary": "Deep learning models often hallucinate, producing realistic artifacts that\nare not truly present in the sample. This can have dire consequences for\nscientific and medical inverse problems, such as MRI and microscopy denoising,\nwhere accuracy is more important than perceptual quality. Uncertainty\nquantification techniques, such as conformal prediction, can pinpoint outliers\nand provide guarantees for image regression tasks, improving reliability.\nHowever, existing methods utilize a linear constant scaling factor to calibrate\nuncertainty bounds, resulting in larger, less informative bounds. We propose\nQUTCC, a quantile uncertainty training and calibration technique that enables\nnonlinear, non-uniform scaling of quantile predictions to enable tighter\nuncertainty estimates. Using a U-Net architecture with a quantile embedding,\nQUTCC enables the prediction of the full conditional distribution of quantiles\nfor the imaging task. During calibration, QUTCC generates uncertainty bounds by\niteratively querying the network for upper and lower quantiles, progressively\nrefining the bounds to obtain a tighter interval that captures the desired\ncoverage. We evaluate our method on several denoising tasks as well as\ncompressive MRI reconstruction. Our method successfully pinpoints\nhallucinations in image estimates and consistently achieves tighter uncertainty\nintervals than prior methods while maintaining the same statistical coverage.",
        "url": "http://arxiv.org/abs/2507.14760v1",
        "published_date": "2025-07-19T21:44:14+00:00",
        "updated_date": "2025-07-19T21:44:14+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Cassandra Tong Ye",
            "Shamus Li",
            "Tyler King",
            "Kristina Monakhova"
        ],
        "tldr": "The paper introduces QUTCC, a novel quantile uncertainty training and conformal calibration technique for imaging inverse problems, which provides tighter uncertainty bounds for image regression tasks by non-linearly scaling quantile predictions, and effectively pinpointing hallucinations.",
        "tldr_zh": "该论文介绍了QUTCC，一种新颖的分位数不确定性训练和共形校准技术，用于图像反问题。该方法通过非线性缩放分位数预测，为图像回归任务提供更严格的不确定性边界，并能有效指出幻觉。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]