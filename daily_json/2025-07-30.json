[
    {
        "title": "Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT Images",
        "summary": "Foundation models have demonstrated remarkable potential in medical domain.\nHowever, their application to complex cardiovascular diagnostics remains\nunderexplored. In this paper, we present Cardiac-CLIP, a multi-modal foundation\nmodel designed for 3D cardiac CT images. Cardiac-CLIP is developed through a\ntwo-stage pre-training strategy. The first stage employs a 3D masked\nautoencoder (MAE) to perform self-supervised representation learning from\nlarge-scale unlabeled volumetric data, enabling the visual encoder to capture\nrich anatomical and contextual features. In the second stage, contrastive\nlearning is introduced to align visual and textual representations,\nfacilitating cross-modal understanding. To support the pre-training, we collect\n16641 real clinical CT scans, supplemented by 114k publicly available data.\nMeanwhile, we standardize free-text radiology reports into unified templates\nand construct the pathology vectors according to diagnostic attributes, based\non which the soft-label matrix is generated to supervise the contrastive\nlearning process. On the other hand, to comprehensively evaluate the\neffectiveness of Cardiac-CLIP, we collect 6,722 real-clinical data from 12\nindependent institutions, along with the open-source data to construct the\nevaluation dataset. Specifically, Cardiac-CLIP is comprehensively evaluated\nacross multiple tasks, including cardiovascular abnormality classification,\ninformation retrieval and clinical analysis. Experimental results demonstrate\nthat Cardiac-CLIP achieves state-of-the-art performance across various\ndownstream tasks in both internal and external data. Particularly, Cardiac-CLIP\nexhibits great effectiveness in supporting complex clinical tasks such as the\nprospective prediction of acute coronary syndrome, which is notoriously\ndifficult in real-world scenarios.",
        "url": "http://arxiv.org/abs/2507.22024v1",
        "published_date": "2025-07-29T17:20:32+00:00",
        "updated_date": "2025-07-29T17:20:32+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Yutao Hu",
            "Ying Zheng",
            "Shumei Miao",
            "Xiaolei Zhang",
            "Jiahao Xia",
            "Yaolei Qi",
            "Yiyang Zhang",
            "Yuting He",
            "Qian Chen",
            "Jing Ye",
            "Hongyan Qiao",
            "Xiuhua Hu",
            "Lei Xu",
            "Jiayin Zhang",
            "Hui Liu",
            "Minwen Zheng",
            "Yining Wang",
            "Daimin Zhang",
            "Ji Zhang",
            "Wenqi Shao",
            "Yun Liu",
            "Longjiang Zhang",
            "Guanyu Yang"
        ],
        "tldr": "Cardiac-CLIP is a multi-modal foundation model for 3D cardiac CT images, pre-trained with a two-stage approach (MAE and contrastive learning) on a large dataset, demonstrating SOTA performance on various cardiac-related tasks.",
        "tldr_zh": "Cardiac-CLIP是一个用于3D心脏CT图像的多模态基础模型，通过两阶段方法（MAE和对比学习）在大型数据集上进行预训练，并在各种心脏相关任务上表现出SOTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Meta CLIP 2: A Worldwide Scaling Recipe",
        "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.",
        "url": "http://arxiv.org/abs/2507.22062v2",
        "published_date": "2025-07-29T17:59:58+00:00",
        "updated_date": "2025-07-30T22:14:32+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Yung-Sung Chuang",
            "Yang Li",
            "Dong Wang",
            "Ching-Feng Yeh",
            "Kehan Lyu",
            "Ramya Raghavendra",
            "James Glass",
            "Lifei Huang",
            "Jason Weston",
            "Luke Zettlemoyer",
            "Xinlei Chen",
            "Zhuang Liu",
            "Saining Xie",
            "Wen-tau Yih",
            "Shang-Wen Li",
            "Hu Xu"
        ],
        "tldr": "Meta CLIP 2 addresses the challenges of scaling CLIP to worldwide web data by developing a recipe that benefits from both English and non-English data, achieving state-of-the-art results on multilingual benchmarks.",
        "tldr_zh": "Meta CLIP 2 解决了将 CLIP 扩展到全球网络数据的挑战，通过开发一种既能从英语数据中受益也能从非英语数据中受益的配方，在多语言基准测试中取得了最先进的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs",
        "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in visual understanding and multimodal reasoning. However, LVLMs\nfrequently exhibit hallucination phenomena, manifesting as the generated\ntextual responses that demonstrate inconsistencies with the provided visual\ncontent. Existing hallucination mitigation methods are predominantly\ntext-centric, the challenges of visual-semantic alignment significantly limit\ntheir effectiveness, especially when confronted with fine-grained visual\nunderstanding scenarios. To this end, this paper presents ViHallu, a\nVision-Centric Hallucination mitigation framework that enhances visual-semantic\nalignment through Visual Variation Image Generation and Visual Instruction\nConstruction. ViHallu introduces visual variation images with controllable\nvisual alterations while maintaining the overall image structure. These images,\ncombined with carefully constructed visual instructions, enable LVLMs to better\nunderstand fine-grained visual content through fine-tuning, allowing models to\nmore precisely capture the correspondence between visual content and text,\nthereby enhancing visual-semantic alignment. Extensive experiments on multiple\nbenchmarks show that ViHallu effectively enhances models' fine-grained visual\nunderstanding while significantly reducing hallucination tendencies.\nFurthermore, we release ViHallu-Instruction, a visual instruction dataset\nspecifically designed for hallucination mitigation and visual-semantic\nalignment. Code is available at https://github.com/oliviadzy/ViHallu.",
        "url": "http://arxiv.org/abs/2507.22003v2",
        "published_date": "2025-07-29T16:53:27+00:00",
        "updated_date": "2025-07-30T04:41:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyun Dai",
            "Xiaoqiang Li",
            "Shaohua Zhang",
            "Yuanchen Wu",
            "Jide Li"
        ],
        "tldr": "The paper introduces ViHallu, a vision-centric framework that mitigates hallucinations in LVLMs by generating visual variation images and constructing visual instructions to enhance visual-semantic alignment, demonstrating improved performance on multiple benchmarks.",
        "tldr_zh": "该论文介绍了一种名为ViHallu的以视觉为中心的框架，通过生成视觉变异图像和构建视觉指令来增强视觉语义对齐，从而减轻 LVLM 中的幻觉现象，并在多个基准测试中展示了改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning",
        "summary": "Large Language Models (LLMs), enhanced through agent tuning, have\ndemonstrated remarkable capabilities in Chain-of-Thought (CoT) and tool\nutilization, significantly surpassing the performance of standalone models.\nHowever, the multimodal domain still lacks a large-scale, high-quality agent\ntuning dataset to unlock the full potential of multimodal large language\nmodels. To bridge this gap, we introduce MMAT-1M, the first million-scale\nmultimodal agent tuning dataset designed to support CoT, reflection, and\ndynamic tool usage. Our dataset is constructed through a novel four-stage data\nengine: 1) We first curate publicly available multimodal datasets containing\nquestion-answer pairs; 2) Then, leveraging GPT-4o, we generate rationales for\nthe original question-answer pairs and dynamically integrate API calls and\nRetrieval Augmented Generation (RAG) information through a multi-turn paradigm;\n3) Furthermore, we refine the rationales through reflection to ensure logical\nconsistency and accuracy, creating a multi-turn dialogue dataset with both\nRationale and Reflection (RR); 4) Finally, to enhance efficiency, we optionally\ncompress multi-turn dialogues into a One-turn Rationale and Reflection (ORR)\nformat. By fine-tuning open-source multimodal models on the MMAT-1M, we observe\nsignificant performance gains. For instance, the InternVL2.5-8B-RR model\nachieves an average improvement of 2.7% across eight public benchmarks and 8.8%\non the RAG benchmark Dyn-VQA, demonstrating the dataset's effectiveness in\nenhancing multimodal reasoning and tool-based capabilities. The dataset is\npublicly available at https://github.com/VIS-MPU-Agent/MMAT-1M.",
        "url": "http://arxiv.org/abs/2507.21924v1",
        "published_date": "2025-07-29T15:39:14+00:00",
        "updated_date": "2025-07-29T15:39:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianhong Gao",
            "Yannian Fu",
            "Weiqun Wu",
            "Haixiao Yue",
            "Shanshan Liu",
            "Gang Zhang"
        ],
        "tldr": "The paper introduces MMAT-1M, a million-scale multimodal agent tuning dataset for enhancing reasoning and tool-based capabilities of VLMs through CoT, reflection, and dynamic tool usage, demonstrating significant performance gains after fine-tuning.",
        "tldr_zh": "该论文介绍了MMAT-1M，一个百万规模的多模态智能体调优数据集，旨在通过思维链（CoT）、反思和动态工具使用来增强视觉语言模型的推理和基于工具的能力，并在微调后展示了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Distribution-Based Masked Medical Vision-Language Model Using Structured Reports",
        "summary": "Medical image-language pre-training aims to align medical images with\nclinically relevant text to improve model performance on various downstream\ntasks. However, existing models often struggle with the variability and\nambiguity inherent in medical data, limiting their ability to capture nuanced\nclinical information and uncertainty. This work introduces an uncertainty-aware\nmedical image-text pre-training model that enhances generalization capabilities\nin medical image analysis. Building on previous methods and focusing on Chest\nX-Rays, our approach utilizes structured text reports generated by a large\nlanguage model (LLM) to augment image data with clinically relevant context.\nThese reports begin with a definition of the disease, followed by the\n`appearance' section to highlight critical regions of interest, and finally\n`observations' and `verdicts' that ground model predictions in clinical\nsemantics. By modeling both inter- and intra-modal uncertainty, our framework\ncaptures the inherent ambiguity in medical images and text, yielding improved\nrepresentations and performance on downstream tasks. Our model demonstrates\nsignificant advances in medical image-text pre-training, obtaining\nstate-of-the-art performance on multiple downstream tasks.",
        "url": "http://arxiv.org/abs/2507.21794v1",
        "published_date": "2025-07-29T13:31:24+00:00",
        "updated_date": "2025-07-29T13:31:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shreyank N Gowda",
            "Ruichi Zhang",
            "Xiao Gu",
            "Ying Weng",
            "Lu Yang"
        ],
        "tldr": "This paper introduces an uncertainty-aware medical vision-language pre-training model that leverages LLM-generated structured reports to improve performance on downstream medical image analysis tasks, achieving state-of-the-art results.",
        "tldr_zh": "本文介绍了一种不确定性感知的医学视觉语言预训练模型，该模型利用LLM生成的结构化报告来提高下游医学图像分析任务的性能，并取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot Learning",
        "summary": "Vision-language pre-trained models (VLMs) such as CLIP have demonstrated\nremarkable zero-shot generalization, and prompt learning has emerged as an\nefficient alternative to full fine-tuning. However, existing methods often\nstruggle with generalization to novel classes, a phenomenon attributed to\noverfitting on seen classes and forgetting general knowledge. Furthermore,\nrecent approaches that improve generalization often introduce complex\narchitectures or heavy computational overhead. In this paper, we propose a\nMultiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance\nfew-shot generalization while maintaining computational efficiency. Our\napproach leverages an ensemble of parallel learnable context vectors to capture\ndiverse semantic aspects. To enrich these prompts, we introduce a semantic\nguidance mechanism that aligns them with comprehensive class descriptions\nautomatically generated by a Large Language Model (LLM). Furthermore, a\ndiversity regularization loss encourages the prompts to learn complementary and\northogonal features, preventing them from collapsing into redundant\nrepresentations. Extensive experiments on 11 benchmark datasets show that\nMSGCoOp significantly improves performance on base-to-novel generalization,\nachieving an average harmonic mean improvement of 1.10\\% over the strong KgCoOp\nbaseline. Our method also demonstrates enhanced robustness in cross-domain\ngeneralization tasks. Our code is avaliable at:\n\\href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.",
        "url": "http://arxiv.org/abs/2507.21786v1",
        "published_date": "2025-07-29T13:15:09+00:00",
        "updated_date": "2025-07-29T13:15:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaolong Wang",
            "Tongfeng Sun",
            "Mingzheng Du",
            "Yachao Huang"
        ],
        "tldr": "The paper introduces MSGCoOp, a method for few-shot learning that uses multiple semantic-guided prompts optimized with LLM guidance and diversity regularization, achieving improved generalization and robustness with computational efficiency.",
        "tldr_zh": "该论文提出了MSGCoOp，一种用于少样本学习的方法，它使用多个语义引导的提示，通过LLM指导和多样性正则化进行优化，从而在计算效率上实现了更好的泛化和鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards",
        "summary": "Recent advances in large language and vision-language models have enabled\nstrong reasoning capabilities, yet they remain impractical for specialized\ndomains like remote sensing, where annotated data is scarce and expensive. We\npresent the first few-shot reinforcement learning with verifiable reward (RLVR)\nframework for satellite imagery that eliminates the need for caption\nsupervision--relying solely on lightweight, rule-based binary or IoU-based\nrewards. Adapting the \"1-shot RLVR\" paradigm from language models to\nvision-language models, we employ policy-gradient optimization with as few as\none curated example to align model outputs for satellite reasoning tasks.\nComprehensive experiments across multiple remote sensing benchmarks--including\nclassification, visual question answering, and grounding--show that even a\nsingle example yields substantial improvements over the base model. Scaling to\n128 examples matches or exceeds models trained on thousands of annotated\nsamples. While the extreme one-shot setting can induce mild, task-specific\noverfitting, our approach consistently demonstrates robust generalization and\nefficiency across diverse tasks. Further, we find that prompt design and loss\nweighting significantly influence training stability and final accuracy. Our\nmethod enables cost-effective and data-efficient development of\ndomain-specialist vision-language reasoning models, offering a pragmatic recipe\nfor data-scarce fields: start from a compact VLM, curate a handful of\nreward-checkable cases, and train via RLVR.",
        "url": "http://arxiv.org/abs/2507.21745v1",
        "published_date": "2025-07-29T12:23:19+00:00",
        "updated_date": "2025-07-29T12:23:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aybora Koksal",
            "A. Aydin Alatan"
        ],
        "tldr": "The paper introduces a novel few-shot reinforcement learning framework (RLVR) for vision-language reasoning on satellite imagery, using rule-based rewards to overcome data scarcity and achieving strong performance with only a few examples.",
        "tldr_zh": "该论文介绍了一种新的少样本强化学习框架 (RLVR)，用于卫星图像的视觉语言推理，使用基于规则的奖励来克服数据稀缺性，并且仅使用少量示例即可实现强大的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs",
        "summary": "Multimodal large language models (MLLMs) enable vision-language reasoning,\nyet often generate plausible outputs that are factually incorrect or visually\nungrounded, thereby compromising their reliability. Direct preference\noptimization (DPO) is a common strategy for correcting hallucinations by\naligning model outputs with human preferences. Existing DPO strategies\ntypically treat hallucination-related preferences as fixed targets, relying on\nstatic supervision signals during training. This approach tends to overfit to\nsuperficial linguistic cues in preference data, leading to distributional\nrigidity and spurious correlations that impair grounding in causally relevant\nvisual information. To overcome this limitation, we propose TARS, a\ntoken-adaptive preference strategy that reformulates DPO as a min-max\noptimization problem. TARS maximizes token-level distributional shifts under\nsemantic constraints to simulate alignment uncertainty, and simultaneously\nminimizes the expected preference loss under these controlled perturbations.\nThis joint objective preserves causal grounding while mitigating overfitting to\npreference patterns, thereby reducing hallucinations in multimodal reasoning.\nWe evaluate TARS on multiple hallucination benchmarks and find consistently\nstrong performance. Using only 4.8k preference samples and no expert feedback,\nTARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition\nvalue from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on\nseveral key metrics.",
        "url": "http://arxiv.org/abs/2507.21584v2",
        "published_date": "2025-07-29T08:39:19+00:00",
        "updated_date": "2025-07-31T05:23:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kejia Zhang",
            "Keda Tao",
            "Zhiming Luo",
            "Chang Liu",
            "Jiasheng Tang",
            "Huan Wang"
        ],
        "tldr": "This paper introduces TARS, a token-adaptive preference strategy using min-max optimization to reduce hallucinations in MLLMs by mitigating overfitting to superficial preference patterns and preserving causal grounding. It achieves state-of-the-art performance with limited data.",
        "tldr_zh": "本文介绍了一种名为TARS的token自适应偏好策略，该策略采用min-max优化方法来减少多模态大语言模型中的幻觉。通过减轻对表面偏好模式的过度拟合，并保留因果关系，TARS使用有限的数据实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking",
        "summary": "The increasing sophistication of large vision-language models (LVLMs) has\nbeen accompanied by advances in safety alignment mechanisms designed to prevent\nharmful content generation. However, these defenses remain vulnerable to\nsophisticated adversarial attacks. Existing jailbreak methods typically rely on\ndirect and semantically explicit prompts, overlooking subtle vulnerabilities in\nhow LVLMs compose information over multiple reasoning steps. In this paper, we\npropose a novel and effective jailbreak framework inspired by Return-Oriented\nProgramming (ROP) techniques from software security. Our approach decomposes a\nharmful instruction into a sequence of individually benign visual gadgets. A\ncarefully engineered textual prompt directs the sequence of inputs, prompting\nthe model to integrate the benign visual gadgets through its reasoning process\nto produce a coherent and harmful output. This makes the malicious intent\nemergent and difficult to detect from any single component. We validate our\nmethod through extensive experiments on established benchmarks including\nSafeBench and MM-SafetyBench, targeting popular LVLMs. Results show that our\napproach consistently and substantially outperforms existing baselines on\nstate-of-the-art models, achieving near-perfect attack success rates (over 0.90\non SafeBench) and improving ASR by up to 0.39. Our findings reveal a critical\nand underexplored vulnerability that exploits the compositional reasoning\nabilities of LVLMs, highlighting the urgent need for defenses that secure the\nentire reasoning process.",
        "url": "http://arxiv.org/abs/2507.21540v1",
        "published_date": "2025-07-29T07:13:56+00:00",
        "updated_date": "2025-07-29T07:13:56+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Quanchen Zou",
            "Zonghao Ying",
            "Moyang Chen",
            "Wenzhuo Xu",
            "Yisong Xiao",
            "Yakai Li",
            "Deyue Zhang",
            "Dongdong Yang",
            "Zhao Liu",
            "Xiangzheng Zhang"
        ],
        "tldr": "This paper introduces PRISM, a jailbreaking framework for LVLMs that uses a sequence of benign visual gadgets and a carefully engineered prompt to elicit harmful outputs, exploiting compositional reasoning vulnerabilities.",
        "tldr_zh": "本文介绍了一种名为PRISM的LVLM越狱框架，它使用一系列良性的视觉组件和一个精心设计的提示来引出有害的输出，从而利用了组合推理漏洞。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Optimizing Active Learning in Vision-Language Models via Parameter-Efficient Uncertainty Calibration",
        "summary": "Active Learning (AL) has emerged as a powerful approach for minimizing\nlabeling costs by selectively sampling the most informative data for neural\nnetwork model development. Effective AL for large-scale vision-language models\nnecessitates addressing challenges in uncertainty estimation and efficient\nsampling given the vast number of parameters involved. In this work, we\nintroduce a novel parameter-efficient learning methodology that incorporates\nuncertainty calibration loss within the AL framework. We propose a\ndifferentiable loss function that promotes uncertainty calibration for\neffectively selecting fewer and most informative data samples for fine-tuning.\nThrough extensive experiments across several datasets and vision backbones, we\ndemonstrate that our solution can match and exceed the performance of complex\nfeature-based sampling techniques while being computationally very efficient.\nAdditionally, we investigate the efficacy of Prompt learning versus Low-rank\nadaptation (LoRA) in sample selection, providing a detailed comparative\nanalysis of these methods in the context of efficient AL.",
        "url": "http://arxiv.org/abs/2507.21521v1",
        "published_date": "2025-07-29T06:08:28+00:00",
        "updated_date": "2025-07-29T06:08:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Athmanarayanan Lakshmi Narayanan",
            "Amrutha Machireddy",
            "Ranganath Krishnan"
        ],
        "tldr": "This paper introduces a parameter-efficient active learning method for vision-language models using an uncertainty calibration loss, demonstrating improved performance and computational efficiency compared to complex feature-based methods. The paper also compares Prompt learning and LoRA in the context of efficient active learning.",
        "tldr_zh": "本文提出了一种用于视觉-语言模型的参数高效主动学习方法，该方法使用不确定性校准损失，与复杂的基于特征的方法相比，表现出更高的性能和计算效率。该论文还比较了Prompt learning和LoRA在高效主动学习中的应用。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval",
        "summary": "Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D\nobjects of unseen categories beyond the training set. Existing methods\ntypically utilize all modalities (i.e., voxels, point clouds, multi-view\nimages) and train specific backbones before fusion. However, they still\nstruggle to produce generalized representations due to insufficient 3D training\ndata. Being contrastively pre-trained on web-scale image-text pairs, CLIP\ninherently produces generalized representations for a wide range of downstream\ntasks. Building upon it, we present a simple yet effective framework named\nDescribe, Adapt and Combine (DAC) by taking only multi-view images for open-set\n3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large\nlanguage model (MLLM) to learn generalized 3D representations, where the MLLM\nis used for dual purposes. First, it describes the seen category information to\nalign with CLIP's training objective for adaptation during training. Second, it\nprovides external hints about unknown objects complementary to visual cues\nduring inference. To improve the synergy, we introduce an Additive-Bias\nLow-Rank adaptation (AB-LoRA), which alleviates overfitting and further\nenhances the generalization to unseen categories. With only multi-view images,\nDAC significantly surpasses prior arts by an average of +10.01\\% mAP on four\nopen-set 3DOR datasets. Moreover, its generalization is also validated on\nimage-based and cross-dataset setups. Code is available at\nhttps://github.com/wangzhichuan123/DAC.",
        "url": "http://arxiv.org/abs/2507.21489v1",
        "published_date": "2025-07-29T04:11:05+00:00",
        "updated_date": "2025-07-29T04:11:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhichuan Wang",
            "Yang Zhou",
            "Zhe Liu",
            "Rui Yu",
            "Song Bai",
            "Yulong Wang",
            "Xinwei He",
            "Xiang Bai"
        ],
        "tldr": "This paper introduces DAC, a framework leveraging CLIP and MLLMs for open-set 3D object retrieval using multi-view images, achieving significant improvements over existing methods.",
        "tldr_zh": "本文介绍了 DAC，一个利用 CLIP 和 MLLM 的框架，用于使用多视角图像进行开放集 3D 对象检索，与现有方法相比取得了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs",
        "summary": "The computational cost of training multimodal large language models (MLLMs)\nrapidly increases with the number of tokens involved. Existing efficiency\nmethods primarily target inference and rely on token reduction or merging,\noffering limited benefit during training. In this paper, we propose ReGATE\n(Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method\nfor accelerating MLLM training. Specifically, ReGATE adopts a teacher-student\nframework in which the MLLM being trained serves as the student, and a frozen\nreference large language model (LLM) acts as the teacher. The teacher computes\nper-token reference losses, which are combined with an exponential moving\naverage (EMA) of the student's own difficulty scores. This adaptive\ndifficulty-based scoring enables the selective processing of crucial tokens\nwhile bypassing less informative ones in the forward pass, significantly\nreducing computational overhead. Experiments demonstrate that ReGATE, when\napplied to VideoLLaMA2, matches the peak accuracy of standard training on\nMVBench up to 2$\\times$ faster, using only 35% of the tokens. With additional\ntraining, it even surpasses the baseline on several multimodal benchmarks, all\nwhile reducing the total token count by over 41%. Code and models will be\nreleased soon.",
        "url": "http://arxiv.org/abs/2507.21420v1",
        "published_date": "2025-07-29T01:07:09+00:00",
        "updated_date": "2025-07-29T01:07:09+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Chaoyu Li",
            "Yogesh Kulkarni",
            "Pooyan Fazli"
        ],
        "tldr": "ReGATE introduces an adaptive token pruning method for MLLM training, using a teacher-student framework to selectively process crucial tokens and significantly reduce computational cost, achieving faster training and improved accuracy. This paper is of interest because efficiency in VLM training is critical.",
        "tldr_zh": "ReGATE 提出了一种用于 MLLM 训练的自适应 token 剪枝方法，该方法采用师生框架选择性地处理关键 token 并显着降低计算成本，从而实现更快的训练和更高的准确性。本文具有一定的相关性，因为 VLM 训练的效率至关重要。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation",
        "summary": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations. In addition, LLaVA-Reward supports\ndifferent types of preference data for efficient fine-tuning, including paired\npreference data and unpaired data. We train LLaVA-Reward on four evaluation\nperspectives: text-image alignment, fidelity/artifact, safety, and overall\nranking. Empirical results demonstrate that LLaVA-Reward outperforms\nconventional and MLLM-based methods in generating human-aligned scores for\nautomatic evaluations and inference-time scaling in text-to-image generations.",
        "url": "http://arxiv.org/abs/2507.21391v2",
        "published_date": "2025-07-28T23:52:53+00:00",
        "updated_date": "2025-07-30T04:49:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Shijie Zhou",
            "Ruiyi Zhang",
            "Huaisheng Zhu",
            "Branislav Kveton",
            "Yufan Zhou",
            "Jiuxiang Gu",
            "Jian Chen",
            "Changyou Chen"
        ],
        "tldr": "The paper introduces LLaVA-Reward, an efficient reward model using multimodal LLMs for evaluating text-to-image generation, employing a Skip-connection Cross Attention module to enhance text-image correlation and supporting various preference data types for fine-tuning.",
        "tldr_zh": "本文介绍LLaVA-Reward，一种利用多模态LLM评估文本到图像生成的高效奖励模型，该模型采用跳跃连接交叉注意力模块来增强文本-图像相关性，并支持各种偏好数据类型进行微调。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Group Relative Augmentation for Data Efficient Action Detection",
        "summary": "Adapting large Video-Language Models (VLMs) for action detection using only a\nfew examples poses challenges like overfitting and the granularity mismatch\nbetween scene-level pre-training and required person-centric understanding. We\npropose an efficient adaptation strategy combining parameter-efficient tuning\n(LoRA) with a novel learnable internal feature augmentation. Applied within the\nfrozen VLM backbone using FiLM, these augmentations generate diverse feature\nvariations directly relevant to the task. Additionally, we introduce a\ngroup-weighted loss function that dynamically modulates the training\ncontribution of each augmented sample based on its prediction divergence\nrelative to the group average. This promotes robust learning by prioritizing\ninformative yet reasonable augmentations. We demonstrate our method's\neffectiveness on complex multi-label, multi-person action detection datasets\n(AVA, MOMA), achieving strong mAP performance and showcasing significant data\nefficiency for adapting VLMs from limited examples.",
        "url": "http://arxiv.org/abs/2507.21353v1",
        "published_date": "2025-07-28T21:46:05+00:00",
        "updated_date": "2025-07-28T21:46:05+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Deep Anil Patel",
            "Iain Melvin",
            "Zachary Izzo",
            "Martin Renqiang Min"
        ],
        "tldr": "This paper introduces a data-efficient action detection method that adapts large VLMs using LoRA, learnable feature augmentations, and a group-weighted loss to achieve strong performance with limited data.",
        "tldr_zh": "本文提出了一种数据高效的动作检测方法，该方法利用LoRA、可学习特征增强和组加权损失来调整大型VLM，从而在有限数据下实现强大的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Trade-offs in Image Generation: How Do Different Dimensions Interact?",
        "summary": "Model performance in text-to-image (T2I) and image-to-image (I2I) generation\noften depends on multiple aspects, including quality, alignment, diversity, and\nrobustness. However, models' complex trade-offs among these dimensions have\nrarely been explored due to (1) the lack of datasets that allow fine-grained\nquantification of these trade-offs, and (2) the use of a single metric for\nmultiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in\nImage Generation), which spans 10 dimensions (Realism, Originality, Aesthetics,\nContent, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains\n40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we\ndevelop TRIGScore, a VLM-as-judge metric that automatically adapts to various\ndimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I\nand I2I tasks. In addition, we propose the Relation Recognition System to\ngenerate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among\nmodel-specific capabilities. Our experiments demonstrate that DTM consistently\nprovides a comprehensive understanding of the trade-offs between dimensions for\neach type of generative model. Notably, we show that the model's\ndimension-specific weaknesses can be mitigated through fine-tuning on DTM to\nenhance overall performance. Code is available at:\nhttps://github.com/fesvhtr/TRIG",
        "url": "http://arxiv.org/abs/2507.22100v1",
        "published_date": "2025-07-29T17:59:16+00:00",
        "updated_date": "2025-07-29T17:59:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sicheng Zhang",
            "Binzhu Xie",
            "Zhonghao Yan",
            "Yuli Zhang",
            "Donghao Zhou",
            "Xiaofei Chen",
            "Shi Qiu",
            "Jiaqi Liu",
            "Guoyang Xie",
            "Zhichao Lu"
        ],
        "tldr": "The paper introduces TRIG-Bench, a dataset and evaluation metric (TRIGScore) for assessing trade-offs among various dimensions in image generation models, and uses it to analyze 14 models and visualize trade-offs, demonstrating the ability to improve performance through targeted fine-tuning.",
        "tldr_zh": "该论文介绍了TRIG-Bench，一个用于评估图像生成模型中各种维度之间权衡的数据集和评估指标(TRIGScore)，并用它来分析14个模型并可视化权衡，证明了通过有针对性的微调来提高性能的能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding",
        "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.",
        "url": "http://arxiv.org/abs/2507.22025v2",
        "published_date": "2025-07-29T17:22:07+00:00",
        "updated_date": "2025-07-30T12:17:53+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Shuquan Lian",
            "Yuhang Wu",
            "Jia Ma",
            "Zihan Song",
            "Bingqi Chen",
            "Xiawu Zheng",
            "Hui Li"
        ],
        "tldr": "UI-AGILE enhances GUI agents by improving both training (via continuous rewards, \"Simple Thinking\" rewards, and cropping-based resampling) and inference (via decomposed grounding) to achieve state-of-the-art grounding accuracy on GUI benchmarks.",
        "tldr_zh": "UI-AGILE通过改进训练（通过连续奖励、“简单思考”奖励和基于裁剪的重采样）和推理（通过分解式定位）来增强GUI代理，从而在GUI基准测试中实现最先进的定位精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ArtSeek: Deep artwork understanding via multimodal in-context reasoning and late interaction retrieval",
        "summary": "Analyzing digitized artworks presents unique challenges, requiring not only\nvisual interpretation but also a deep understanding of rich artistic,\ncontextual, and historical knowledge. We introduce ArtSeek, a multimodal\nframework for art analysis that combines multimodal large language models with\nretrieval-augmented generation. Unlike prior work, our pipeline relies only on\nimage input, enabling applicability to artworks without links to Wikidata or\nWikipedia-common in most digitized collections. ArtSeek integrates three key\ncomponents: an intelligent multimodal retrieval module based on late\ninteraction retrieval, a contrastive multitask classification network for\npredicting artist, genre, style, media, and tags, and an agentic reasoning\nstrategy enabled through in-context examples for complex visual question\nanswering and artwork explanation via Qwen2.5-VL. Central to this approach is\nWikiFragments, a Wikipedia-scale dataset of image-text fragments curated to\nsupport knowledge-grounded multimodal reasoning. Our framework achieves\nstate-of-the-art results on multiple benchmarks, including a +8.4% F1\nimprovement in style classification over GraphCLIP and a +7.1 BLEU@1 gain in\ncaptioning on ArtPedia. Qualitative analyses show that ArtSeek can interpret\nvisual motifs, infer historical context, and retrieve relevant knowledge, even\nfor obscure works. Though focused on visual arts, our approach generalizes to\nother domains requiring external knowledge, supporting scalable multimodal AI\nresearch. Both the dataset and the source code will be made publicly available\nat https://github.com/cilabuniba/artseek.",
        "url": "http://arxiv.org/abs/2507.21917v1",
        "published_date": "2025-07-29T15:31:58+00:00",
        "updated_date": "2025-07-29T15:31:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nicola Fanelli",
            "Gennaro Vessio",
            "Giovanna Castellano"
        ],
        "tldr": "ArtSeek is a multimodal framework leveraging large language models and retrieval-augmented generation for art analysis, achieving state-of-the-art results by integrating a novel retrieval module, a multitask classification network, and an agentic reasoning strategy, grounded in the new WikiFragments dataset.",
        "tldr_zh": "ArtSeek是一个多模态框架，利用大型语言模型和检索增强生成进行艺术分析。它通过集成新颖的检索模块、多任务分类网络和智能推理策略，并基于新的WikiFragments数据集，实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AU-LLM: Micro-Expression Action Unit Detection via Enhanced LLM-Based Feature Fusion",
        "summary": "The detection of micro-expression Action Units (AUs) is a formidable\nchallenge in affective computing, pivotal for decoding subtle, involuntary\nhuman emotions. While Large Language Models (LLMs) demonstrate profound\nreasoning abilities, their application to the fine-grained, low-intensity\ndomain of micro-expression AU detection remains unexplored. This paper pioneers\nthis direction by introducing \\textbf{AU-LLM}, a novel framework that for the\nfirst time uses LLM to detect AUs in micro-expression datasets with subtle\nintensities and the scarcity of data. We specifically address the critical\nvision-language semantic gap, the \\textbf{Enhanced Fusion Projector (EFP)}. The\nEFP employs a Multi-Layer Perceptron (MLP) to intelligently fuse mid-level\n(local texture) and high-level (global semantics) visual features from a\nspecialized 3D-CNN backbone into a single, information-dense token. This\ncompact representation effectively empowers the LLM to perform nuanced\nreasoning over subtle facial muscle movements.Through extensive evaluations on\nthe benchmark CASME II and SAMM datasets, including stringent\nLeave-One-Subject-Out (LOSO) and cross-domain protocols, AU-LLM establishes a\nnew state-of-the-art, validating the significant potential and robustness of\nLLM-based reasoning for micro-expression analysis. The codes are available at\nhttps://github.com/ZS-liu-JLU/AU-LLMs.",
        "url": "http://arxiv.org/abs/2507.21778v1",
        "published_date": "2025-07-29T13:01:59+00:00",
        "updated_date": "2025-07-29T13:01:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhishu Liu",
            "Kaishen Yuan",
            "Bo Zhao",
            "Yong Xu",
            "Zitong Yu"
        ],
        "tldr": "The paper introduces AU-LLM, a novel framework using LLMs for micro-expression Action Unit detection, enhancing feature fusion with an Enhanced Fusion Projector (EFP) to bridge the vision-language semantic gap and achieve state-of-the-art results on benchmark datasets.",
        "tldr_zh": "该论文介绍了AU-LLM，这是一个使用LLM进行微表情动作单元检测的新框架。该框架使用增强融合投影仪（EFP）来增强特征融合，弥合视觉-语言语义鸿沟，并在基准数据集上实现了最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces",
        "summary": "In the latest advancements in multimodal learning, effectively addressing the\nspatial and semantic losses of visual data after encoding remains a critical\nchallenge. This is because the performance of large multimodal models is\npositively correlated with the coupling between visual encoders and large\nlanguage models. Existing approaches often face issues such as vector gaps or\nsemantic disparities, resulting in information loss during the propagation\nprocess. To address these issues, we propose MAGE (Multimodal Alignment and\nGeneration Enhancement), a novel framework that bridges the semantic spaces of\nvision and text through an innovative alignment mechanism. By introducing the\nIntelligent Alignment Network (IAN), MAGE achieves dimensional and semantic\nalignment. To reduce the gap between synonymous heterogeneous data, we employ a\ntraining strategy that combines cross-entropy and mean squared error,\nsignificantly enhancing the alignment effect. Moreover, to enhance MAGE's\n\"Any-to-Any\" capability, we developed a fine-tuning dataset for multimodal\ntool-calling instructions to expand the model's output capability boundaries.\nFinally, our proposed multimodal large model architecture, MAGE, achieved\nsignificantly better performance compared to similar works across various\nevaluation benchmarks, including MME, MMBench, and SEED. Complete code and\nappendix are available at: https://github.com/GTCOM-NLP/MAGE.",
        "url": "http://arxiv.org/abs/2507.21741v1",
        "published_date": "2025-07-29T12:17:46+00:00",
        "updated_date": "2025-07-29T12:17:46+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Shaojun E",
            "Yuchen Yang",
            "Jiaheng Wu",
            "Yan Zhang",
            "Tiejun Zhao",
            "Ziyan Chen"
        ],
        "tldr": "The paper introduces MAGE, a framework designed to improve multimodal alignment and generation by bridging visual and semantic spaces using an Intelligent Alignment Network and a specialized training strategy. It claims to achieve state-of-the-art performance on various multimodal benchmarks.",
        "tldr_zh": "该论文介绍了一种名为MAGE的框架，旨在通过使用智能对齐网络和专门的训练策略来桥接视觉和语义空间，从而改善多模态对齐和生成。它声称在各种多模态基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "EMIT: Enhancing MLLMs for Industrial Anomaly Detection via Difficulty-Aware GRPO",
        "summary": "Industrial anomaly detection (IAD) plays a crucial role in maintaining the\nsafety and reliability of manufacturing systems. While multimodal large\nlanguage models (MLLMs) show strong vision-language reasoning abilities, their\neffectiveness in IAD remains limited without domain-specific adaptation. In\nthis work, we propose EMIT, a unified framework that enhances MLLMs for IAD via\ndifficulty-aware group relative policy optimization (GRPO). EMIT constructs a\nmulti-task IAD dataset and utilizes GPT-generated object text descriptions to\ncompensate for missing defective images. For few-shot anomaly detection, it\nintegrates a soft prompt and heatmap-guided contrastive embeddings derived from\npatch-level comparisons. To better handle difficult data samples, i.e., cases\nwhere the MLLM struggles to generate correct answers, we propose a\ndifficulty-aware GRPO that extends the original GRPO by incorporating a\nresponse resampling strategy to ensure the inclusion of correct answers in the\nsampled responses, as well as an advantage reweighting mechanism to strengthen\nlearning from such difficult data samples. Extensive experiments on the MMAD\nbenchmark demonstrate that EMIT significantly enhances the IAD performance of\nMLLMs, achieving an average improvement of 7.77\\% over the base model\n(InternVL3-8B) across seven tasks.",
        "url": "http://arxiv.org/abs/2507.21619v1",
        "published_date": "2025-07-29T09:18:22+00:00",
        "updated_date": "2025-07-29T09:18:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wei Guan",
            "Jun Lan",
            "Jian Cao",
            "Hao Tan",
            "Huijia Zhu",
            "Weiqiang Wang"
        ],
        "tldr": "The paper introduces EMIT, a framework that enhances MLLMs for industrial anomaly detection using difficulty-aware group relative policy optimization (GRPO) and achieves significant performance improvements on the MMAD benchmark.",
        "tldr_zh": "该论文介绍了EMIT，一个通过难度感知的分组相对策略优化（GRPO）来增强MLLM在工业异常检测中性能的框架，并在MMAD基准测试中取得了显著的性能提升。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding",
        "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events in videos and\naccurately determine their time intervals. Current VAD methods mainly fall into\ntwo categories: traditional DNN-based approaches that focus on temporal\nlocalization, and LLM-based approaches that emphasize semantic understanding.\nBoth anomaly understanding and grounding are essential for comprehensive video\nanomaly detection and can complement each other. However, no existing model or\ndataset supports both tasks simultaneously. To address this, we introduce VAGU\n(Video Anomaly Grounding and Understanding), the first benchmark to integrate\nboth tasks. Each VAGU instance includes annotations for anomaly category,\nsemantic explanation, precise temporal grounding and Video QA. We also provide\nmultiple-choice Video QA for objective evaluation. Based on this dataset, we\npropose Glance then Scrutinize (GtS), a training-free framework guided by\ntextual prompts. The framework first enables coarse localization of\nhigh-probability anomalous regions, followed by detailed anomaly interpretation\nand temporal boundary refinement. Additionally, we propose the JeAUG metric,\nwhich jointly evaluates semantic interpretability and temporal precision,\novercoming the limitations of traditional metrics. Extensive experiments verify\nthe effectiveness of our benchmark, framework, and evaluation metric.",
        "url": "http://arxiv.org/abs/2507.21507v1",
        "published_date": "2025-07-29T05:17:48+00:00",
        "updated_date": "2025-07-29T05:17:48+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Shibo Gao",
            "Peipei Yang",
            "Yangyang Liu",
            "Yi Chen",
            "Han Zhu",
            "Xuyao Zhang",
            "Linlin Huang"
        ],
        "tldr": "This paper introduces VAGU, a new benchmark dataset for joint video anomaly grounding and understanding, and proposes GtS, a training-free framework leveraging LLMs for this task, along with a new evaluation metric JeAUG.",
        "tldr_zh": "该论文介绍了VAGU，一个新的用于联合视频异常定位和理解的基准数据集，并提出了GtS，一个利用LLMs的免训练框架来完成此任务，同时还提出了一个新的评估指标JeAUG。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Analyzing the Sensitivity of Vision Language Models in Visual Question Answering",
        "summary": "We can think of Visual Question Answering as a (multimodal) conversation\nbetween a human and an AI system. Here, we explore the sensitivity of Vision\nLanguage Models (VLMs) through the lens of cooperative principles of\nconversation proposed by Grice. Specifically, even when Grice's maxims of\nconversation are flouted, humans typically do not have much difficulty in\nunderstanding the conversation even though it requires more cognitive effort.\nHere, we study if VLMs are capable of handling violations to Grice's maxims in\na manner that is similar to humans. Specifically, we add modifiers to\nhuman-crafted questions and analyze the response of VLMs to these modifiers. We\nuse three state-of-the-art VLMs in our study, namely, GPT-4o, Claude-3.5-Sonnet\nand Gemini-1.5-Flash on questions from the VQA v2.0 dataset. Our initial\nresults seem to indicate that the performance of VLMs consistently diminish\nwith the addition of modifiers which indicates our approach as a promising\ndirection to understand the limitations of VLMs.",
        "url": "http://arxiv.org/abs/2507.21335v1",
        "published_date": "2025-07-28T21:01:28+00:00",
        "updated_date": "2025-07-28T21:01:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Monika Shah",
            "Sudarshan Balaji",
            "Somdeb Sarkhel",
            "Sanorita Dey",
            "Deepak Venugopal"
        ],
        "tldr": "This paper analyzes the sensitivity of VLMs to modifications in visual question answering, finding that performance diminishes with added modifiers, suggesting a vulnerability to Grice's maxims violations.",
        "tldr_zh": "本文分析了视觉语言模型(VLMs)在视觉问答中对修改的敏感性，发现性能随着添加修饰符而降低，表明其容易受到格莱斯会话准则违规的影响。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding",
        "summary": "We address the problem of Embodied Reference Understanding, which involves\npredicting the object that a person in the scene is referring to through both\npointing gesture and language. Accurately identifying the referent requires\nmultimodal understanding: integrating textual instructions, visual pointing,\nand scene context. However, existing methods often struggle to effectively\nleverage visual clues for disambiguation. We also observe that, while the\nreferent is often aligned with the head-to-fingertip line, it occasionally\naligns more closely with the wrist-to-fingertip line. Therefore, relying on a\nsingle line assumption can be overly simplistic and may lead to suboptimal\nperformance. To address this, we propose a dual-model framework, where one\nmodel learns from the head-to-fingertip direction and the other from the\nwrist-to-fingertip direction. We further introduce a Gaussian ray heatmap\nrepresentation of these lines and use them as input to provide a strong\nsupervisory signal that encourages the model to better attend to pointing cues.\nTo combine the strengths of both models, we present the CLIP-Aware Pointing\nEnsemble module, which performs a hybrid ensemble based on CLIP features.\nAdditionally, we propose an object center prediction head as an auxiliary task\nto further enhance referent localization. We validate our approach through\nextensive experiments and analysis on the benchmark YouRefIt dataset, achieving\nan improvement of approximately 4 mAP at the 0.25 IoU threshold.",
        "url": "http://arxiv.org/abs/2507.21888v1",
        "published_date": "2025-07-29T15:00:21+00:00",
        "updated_date": "2025-07-29T15:00:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fevziye Irem Eyiokur",
            "Dogucan Yaman",
            "Hazım Kemal Ekenel",
            "Alexander Waibel"
        ],
        "tldr": "This paper introduces CAPE, a CLIP-aware pointing ensemble method for Embodied Reference Understanding, which leverages complementary heatmap cues derived from head-to-fingertip and wrist-to-fingertip lines and a CLIP-based ensemble module to improve referent localization.",
        "tldr_zh": "本文介绍了CAPE，一种CLIP感知的指针集成方法，用于具身引用理解。该方法利用从头到指尖和腕到指尖线获得的互补热图提示，以及基于CLIP的集成模块，以提高指示对象的定位。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]