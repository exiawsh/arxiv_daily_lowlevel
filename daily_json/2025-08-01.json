[
    {
        "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving",
        "summary": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.",
        "url": "http://arxiv.org/abs/2507.23540v1",
        "published_date": "2025-07-31T13:30:47+00:00",
        "updated_date": "2025-07-31T13:30:47+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yi Zhang",
            "Erik Leo Haß",
            "Kuo-Yi Chao",
            "Nenad Petrovic",
            "Yinglei Song",
            "Chengdong Wu",
            "Alois Knoll"
        ],
        "tldr": "The paper introduces a unified Perception-Language-Action (PLA) framework for autonomous driving that integrates multi-sensor fusion with a GPT-4.1 powered Vision-Language-Action (VLA) architecture to enhance adaptability, robustness, and interpretability.",
        "tldr_zh": "该论文介绍了一种用于自动驾驶的统一感知-语言-动作 (PLA) 框架，该框架将多传感器融合与 GPT-4.1 驱动的视觉-语言-动作 (VLA) 架构集成，以增强适应性、鲁棒性和可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing",
        "summary": "In this paper, we propose UniLIP, which extends CLIP to reconstruction,\ngeneration and editing, thereby building a unified tokenizer upon its\nexceptional comprehension capabilities. Previous CLIP-based unified methods\noften require additional diffusion decoders or quantization to support\nreconstruction and generation tasks, leading to inconsistent reconstruction or\ndegradation of original comprehension performance.In contrast, we introduce a\ntwo-stage training scheme and a self-distillation strategy that progressively\nintegrates reconstruction capabilities into CLIP, allowing it to maintain\noriginal comprehension performance while achieving effective image\nreconstruction. Furthermore, we propose a dual-condition architecture to\nconnect the MLLM and diffusion transformer, using both learnable queries and\nthe last layer multimodal hidden states as joint conditions. This method not\nonly enables the utilization of the MLLM's strong reasoning capabilities in\ngeneration tasks, but also maximizes the exploitation of the rich information\nin UniLIP features during editing tasks. In text-to-image generation tasks,\nUniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark\nrespectively, surpassing all previous unified models of similar scale. In image\nediting, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,\nsurpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP\neffectively expand the application scope of CLIP, enabling continuous CLIP\nfeatures to not only serve as the optimal choice for understanding tasks but\nalso achieve highly competitive performance in generation and editing tasks.",
        "url": "http://arxiv.org/abs/2507.23278v1",
        "published_date": "2025-07-31T06:35:03+00:00",
        "updated_date": "2025-07-31T06:35:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Tang",
            "Chenwei Xie",
            "Xiaoyi Bao",
            "Tingyu Weng",
            "Pandeng Li",
            "Yun Zheng",
            "Liwei Wang"
        ],
        "tldr": "UniLIP extends CLIP for unified multimodal understanding, generation, and editing using a two-stage training scheme and a dual-condition architecture, achieving state-of-the-art performance in text-to-image generation and image editing compared to other unified models.",
        "tldr_zh": "UniLIP扩展了CLIP，通过两阶段训练方案和双条件架构，实现了统一的多模态理解、生成和编辑。与其它统一模型相比，在文本到图像生成和图像编辑方面取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding",
        "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the \\textbf{Phi-Ground} model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder $10B$ parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on\nScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\n\\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}",
        "url": "http://arxiv.org/abs/2507.23779v1",
        "published_date": "2025-07-31T17:59:09+00:00",
        "updated_date": "2025-07-31T17:59:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Miaosen Zhang",
            "Ziqiang Xu",
            "Jialiang Zhu",
            "Qi Dai",
            "Kai Qiu",
            "Yifan Yang",
            "Chong Luo",
            "Tianyi Chen",
            "Justin Wagle",
            "Tim Franklin",
            "Baining Guo"
        ],
        "tldr": "The paper introduces Phi-Ground, a new model family for GUI grounding in Computer Use Agents (CUAs), achieving state-of-the-art performance on several benchmarks, especially for models under 10B parameters.",
        "tldr_zh": "该论文介绍了Phi-Ground，一种用于计算机使用代理（CUA）中GUI接地的新的模型系列，在多个基准测试上实现了最先进的性能，特别是对于参数小于10B的模型。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping",
        "summary": "General robotic grasping systems require accurate object affordance\nperception in diverse open-world scenarios following human instructions.\nHowever, current studies suffer from the problem of lacking reasoning-based\nlarge-scale affordance prediction data, leading to considerable concern about\nopen-world effectiveness. To address this limitation, we build a large-scale\ngrasping-oriented affordance segmentation benchmark with human-like\ninstructions, named RAGNet. It contains 273k images, 180 categories, and 26k\nreasoning instructions. The images cover diverse embodied data domains, such as\nwild, robot, ego-centric, and even simulation data. They are carefully\nannotated with an affordance map, while the difficulty of language instructions\nis largely increased by removing their category name and only providing\nfunctional descriptions. Furthermore, we propose a comprehensive\naffordance-based grasping framework, named AffordanceNet, which consists of a\nVLM pre-trained on our massive affordance data and a grasping network that\nconditions an affordance map to grasp the target. Extensive experiments on\naffordance segmentation benchmarks and real-robot manipulation tasks show that\nour model has a powerful open-world generalization ability. Our data and code\nis available at https://github.com/wudongming97/AffordanceNet.",
        "url": "http://arxiv.org/abs/2507.23734v1",
        "published_date": "2025-07-31T17:17:05+00:00",
        "updated_date": "2025-07-31T17:17:05+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Dongming Wu",
            "Yanping Fu",
            "Saike Huang",
            "Yingfei Liu",
            "Fan Jia",
            "Nian Liu",
            "Feng Dai",
            "Tiancai Wang",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Jianbing Shen"
        ],
        "tldr": "The paper introduces RAGNet, a large-scale reasoning-based affordance segmentation benchmark for grasping, and AffordanceNet, a grasping framework utilizing a VLM pre-trained on RAGNet, demonstrating strong open-world generalization.",
        "tldr_zh": "该论文介绍了RAGNet，一个用于抓取的大规模基于推理的能供性分割基准，以及AffordanceNet，一个利用在RAGNet上预训练的VLM的抓取框架，展示了强大的开放世界泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
        "summary": "Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.",
        "url": "http://arxiv.org/abs/2507.23478v1",
        "published_date": "2025-07-31T11:59:06+00:00",
        "updated_date": "2025-07-31T11:59:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ting Huang",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "tldr": "The paper introduces 3D-R1, a foundation model for enhancing reasoning in 3D VLMs by constructing a high-quality synthetic dataset, leveraging RLHF, and employing a dynamic view selection strategy, achieving a 10% improvement across 3D scene benchmarks.",
        "tldr_zh": "该论文介绍了3D-R1，一个通过构建高质量合成数据集、利用RLHF和采用动态视图选择策略来增强3D VLM推理的基础模型，在3D场景基准测试中实现了10%的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models",
        "summary": "Multimodal planning capabilities refer to the ability to predict, reason, and\ndesign steps for task execution with multimodal context, which is essential for\ncomplex reasoning and decision-making across multiple steps. However, current\nbenchmarks face two key challenges: (1) they cannot directly assess multimodal\nreal-world planning capabilities, and (2) they lack constraints or implicit\nconstraints across modalities. To address these issues, we introduce Multimodal\nPlanning with Complex Constraints (MPCC), the first benchmark to systematically\nevaluate MLLMs' ability to handle multimodal constraints in planning. To\naddress the first challenge, MPCC focuses on three real-world tasks: Flight\nPlanning, Calendar Planning, and Meeting Planning. To solve the second\nchallenge, we introduce complex constraints (e.g. budget, temporal, and\nspatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to\nseparate constraint complexity from search space expansion. Experiments on 13\nadvanced MLLMs reveal significant challenges: closed-source models achieve only\n21.3% feasible plans, while open-source models average below 11%. Additionally,\nwe observe that MLLMs are highly sensitive to constraint complexity and that\ntraditional multimodal prompting strategies fail in multi-constraint scenarios.\nOur work formalizes multimodal constraints in planning, provides a rigorous\nevaluation framework, and highlights the need for advancements in\nconstraint-aware reasoning for real-world MLLM applications.",
        "url": "http://arxiv.org/abs/2507.23382v1",
        "published_date": "2025-07-31T09:59:17+00:00",
        "updated_date": "2025-07-31T09:59:17+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "I.2.8; I.2.10"
        ],
        "authors": [
            "Yiyan Ji",
            "Haoran Chen",
            "Qiguang Chen",
            "Chengyue Wu",
            "Libo Qin",
            "Wanxiang Che"
        ],
        "tldr": "The paper introduces MPCC, a new benchmark for evaluating multimodal large language models (MLLMs) in planning tasks with complex, real-world constraints, revealing significant challenges for existing models.",
        "tldr_zh": "该论文介绍了MPCC，一个新的基准测试，用于评估多模态大型语言模型（MLLMs）在具有复杂、真实世界约束的规划任务中的能力，揭示了现有模型面临的重大挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers",
        "summary": "Although large vision-language models (LVLMs) have demonstrated impressive\ncapabilities in multi-modal understanding and reasoning, their practical\napplications are still limited by massive model parameters and high\ncomputational costs. Recent efforts from natural language processing (NLP) have\nshown the effectiveness of layer pruning, offering a plausible training-free\ncompression solution. However, due to the modality divergence between vision\nand language, it is unclear whether these NLP techniques are still effective in\nLVLMs. In this paper, we empirically prove that directly applying these layer\npruning methods to LVLMs is ineffective. Through extensive experiments, we find\nthat non-essential vision-language (VL) tokens and inter-layer feature gaps\npose critical challenges to pruning layers in LVLMs. Based on these insights,\nwe propose a novel framework Short-LVLM (SVL) that can utilize important VL\ntokens and mitigate the layer-wise feature gaps. Notably, Short-LVLM not only\nachieves a superior trade-off between performance and efficiency but also\nexhibits several potential advantages, i.e., training-free, model-agnostic, and\nhighly compatible. The code for this work is publicly available at\nhttps://github.com/ASGO-MM/Short-LVLM.",
        "url": "http://arxiv.org/abs/2507.23362v1",
        "published_date": "2025-07-31T09:17:53+00:00",
        "updated_date": "2025-07-31T09:17:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ji Ma",
            "Wei Suo",
            "Peng Wang",
            "Yanning Zhang"
        ],
        "tldr": "The paper introduces Short-LVLM, a training-free framework for compressing and accelerating large vision-language models by pruning redundant layers, addressing challenges of applying NLP layer pruning techniques directly to LVLMs due to modality divergence.",
        "tldr_zh": "该论文介绍了Short-LVLM，一个无需训练的框架，通过剪枝冗余层来压缩和加速大型视觉语言模型，解决了由于模态差异而直接将NLP层剪枝技术应用于LVLM的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval",
        "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.",
        "url": "http://arxiv.org/abs/2507.23284v1",
        "published_date": "2025-07-31T06:57:28+00:00",
        "updated_date": "2025-07-31T06:57:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dohwan Ko",
            "Ji Soo Lee",
            "Minhyuk Choi",
            "Zihang Meng",
            "Hyunwoo J. Kim"
        ],
        "tldr": "The paper introduces BLiM, a novel framework for Text-Video Retrieval using MLLMs that addresses candidate prior bias by employing bidirectional likelihood estimation and Candidate Prior Normalization (CPN), achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了一种名为BLiM 的新型文本-视频检索框架，该框架使用多模态大型语言模型（MLLM），通过双向似然估计和候选先验归一化（CPN）来解决候选先验偏差问题，并取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model",
        "summary": "Fine-grained image classification, the task of distinguishing between\nvisually similar subcategories within a broader category (e.g., bird species,\ncar models, flower types), is a challenging computer vision problem.\nTraditional approaches rely heavily on fixed vocabularies and closed-set\nclassification paradigms, limiting their scalability and adaptability in\nreal-world settings where novel classes frequently emerge. Recent research has\ndemonstrated that combining large language models (LLMs) with vision-language\nmodels (VLMs) makes open-set recognition possible without the need for\npredefined class labels. However, the existing methods are often limited in\nharnessing the power of LLMs at the classification phase, and also rely heavily\non the guessed class names provided by an LLM without thorough analysis and\nrefinement. To address these bottlenecks, we propose our training-free method,\nEnriched-FineR (or E-FineR for short), which demonstrates state-of-the-art\nresults in fine-grained visual recognition while also offering greater\ninterpretability, highlighting its strong potential in real-world scenarios and\nnew domains where expert annotations are difficult to obtain. Additionally, we\ndemonstrate the application of our proposed approach to zero-shot and few-shot\nclassification, where it demonstrated performance on par with the existing SOTA\nwhile being training-free and not requiring human interventions. Overall, our\nvocabulary-free framework supports the shift in image classification from rigid\nlabel prediction to flexible, language-driven understanding, enabling scalable\nand generalizable systems for real-world applications. Well-documented code is\navailable on https://github.com/demidovd98/e-finer.",
        "url": "http://arxiv.org/abs/2507.23070v1",
        "published_date": "2025-07-30T20:06:01+00:00",
        "updated_date": "2025-07-30T20:06:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dmitry Demidov",
            "Zaigham Zaheer",
            "Omkar Thawakar",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "tldr": "This paper introduces E-FineR, a training-free, vocabulary-free method for fine-grained visual recognition that leverages LLMs and VLMs for open-set classification, achieving state-of-the-art results with improved interpretability and applicability to zero-shot and few-shot scenarios.",
        "tldr_zh": "本文介绍了一种名为 E-FineR 的免训练、无词汇的细粒度视觉识别方法，该方法利用 LLM 和 VLM 进行开放集分类，实现了最先进的结果，并提高了可解释性，适用于零样本和少样本场景。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints",
        "summary": "Autonomous cars need geometric accuracy and semantic understanding to\nnavigate complex environments, yet most stacks handle them separately. We\npresent XYZ-Drive, a single vision-language model that reads a front-camera\nframe, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs\nsteering and speed. A lightweight goal-centered cross-attention layer lets\nwaypoint tokens highlight relevant image and map patches, supporting both\naction and textual explanations, before the fused tokens enter a partially\nfine-tuned LLaMA-3.2 11B model.\n  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and\n0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and\nhalving collisions, all while significantly improving efficiency by using only\na single branch. Sixteen ablations explain the gains. Removing any modality\n(vision, waypoint, map) drops success by up to 11%, confirming their\ncomplementary roles and rich connections. Replacing goal-centered attention\nwith simple concatenation cuts 3% in performance, showing query-based fusion\ninjects map knowledge more effectively. Keeping the transformer frozen loses\n5%, showing the importance of fine-tuning when applying VLMs for specific tasks\nsuch as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs\nlane edges and raises crash rate.\n  Overall, these results demonstrate that early, token-level fusion of intent\nand map layout enables accurate, transparent, real-time driving.",
        "url": "http://arxiv.org/abs/2507.23064v1",
        "published_date": "2025-07-30T19:51:23+00:00",
        "updated_date": "2025-07-30T19:51:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO",
            "I.4.8; I.2.10; I.2.6; C.3.3; I.4.9"
        ],
        "authors": [
            "Santosh Patapati",
            "Trisanth Srinivasan",
            "Murari Ambati"
        ],
        "tldr": "The paper introduces XYZ-Drive, a vision-language model that fuses camera, HD-Map, and waypoint data using goal-centered cross-attention for autonomous driving, achieving state-of-the-art results on the MD-NEX benchmark with improved efficiency and interpretability.",
        "tldr_zh": "该论文介绍了XYZ-Drive，一种视觉语言模型，它利用目标中心交叉注意力融合摄像头、高清地图和航点数据用于自动驾驶，在MD-NEX基准测试中取得了最先进的结果，并提高了效率和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving",
        "summary": "Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.",
        "url": "http://arxiv.org/abs/2507.23042v1",
        "published_date": "2025-07-30T19:12:42+00:00",
        "updated_date": "2025-07-30T19:12:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.RO",
            "I.2.6; I.2.9; I.2.10; C.3.3"
        ],
        "authors": [
            "Santosh Patapati",
            "Trisanth Srinivasan"
        ],
        "tldr": "The paper introduces NovaDrive, a single-branch vision-language architecture for autonomous driving that achieves state-of-the-art performance on the MD-NEX Outdoor benchmark through a novel cross-attention mechanism and smoothness loss, fine-tuning LLaMA-3.2 vision-language backbone.",
        "tldr_zh": "该论文介绍了 NovaDrive，一种用于自动驾驶的单分支视觉语言架构，通过新颖的交叉注意力机制和平滑损失函数，在 MD-NEX Outdoor 基准测试上实现了最先进的性能，并对 LLaMA-3.2 视觉语言骨干网络进行了微调。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language Translation",
        "summary": "Sign Language Translation (SLT) is a challenging task that requires bridging\nthe modality gap between visual and linguistic information while capturing\nsubtle variations in hand shapes and movements. To address these challenges, we\nintroduce \\textbf{BeyondGloss}, a novel gloss-free SLT framework that leverages\nthe spatio-temporal reasoning capabilities of Video Large Language Models\n(VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail,\nwe propose a novel approach to generate fine-grained, temporally-aware textual\ndescriptions of hand motion. A contrastive alignment module aligns these\ndescriptions with video features during pre-training, encouraging the model to\nfocus on hand-centric temporal dynamics and distinguish signs more effectively.\nTo further enrich hand-specific representations, we distill fine-grained\nfeatures from HaMeR. Additionally, we apply a contrastive loss between sign\nvideo representations and target language embeddings to reduce the modality gap\nin pre-training. \\textbf{BeyondGloss} achieves state-of-the-art performance on\nthe Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of the\nproposed framework. We will release the code upon acceptance of the paper.",
        "url": "http://arxiv.org/abs/2507.23575v1",
        "published_date": "2025-07-31T14:06:07+00:00",
        "updated_date": "2025-07-31T14:06:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sobhan Asasi",
            "Mohamed Ilyas Lakhal",
            "Ozge Mercanoglu Sincan",
            "Richard Bowden"
        ],
        "tldr": "The paper introduces BeyondGloss, a gloss-free sign language translation framework using VideoLLMs with a novel approach for fine-grained hand motion description and contrastive alignment, achieving state-of-the-art results on benchmark datasets.",
        "tldr_zh": "该论文介绍了一种名为BeyondGloss的无gloss手语翻译框架，该框架使用VideoLLM并采用了一种新的精细手部动作描述和对比对齐方法，在基准数据集上实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "ART: Adaptive Relation Tuning for Generalized Relation Prediction",
        "summary": "Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes.",
        "url": "http://arxiv.org/abs/2507.23543v1",
        "published_date": "2025-07-31T13:34:06+00:00",
        "updated_date": "2025-07-31T13:34:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Gopika Sudhakaran",
            "Hikaru Shindo",
            "Patrick Schramowski",
            "Simone Schaub-Meyer",
            "Kristian Kersting",
            "Stefan Roth"
        ],
        "tldr": "The paper introduces ART, an Adaptive Relation Tuning framework that uses instruction tuning and adaptive sampling to improve the generalization of vision-language models for visual relation detection, particularly in handling novel and complex relations.",
        "tldr_zh": "该论文介绍了ART，一种自适应关系调整框架，该框架使用指令调整和自适应采样来提高视觉语言模型在视觉关系检测中的泛化能力，尤其是在处理新的和复杂的关系方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "AGA: An adaptive group alignment framework for structured medical cross-modal representation learning",
        "summary": "Learning medical visual representations from paired images and reports is a\npromising direction in representation learning. However, current\nvision-language pretraining methods in the medical domain often simplify\nclinical reports into single entities or fragmented tokens, ignoring their\ninherent structure. In addition, contrastive learning frameworks typically\ndepend on large quantities of hard negative samples, which is impractical for\nsmall-scale medical datasets. To tackle these challenges, we propose Adaptive\nGrouped Alignment (AGA), a new framework that captures structured semantics\nfrom paired medical images and reports. AGA introduces a bidirectional grouping\nmechanism based on a sparse similarity matrix. For each image-report pair, we\ncompute fine-grained similarities between text tokens and image patches. Each\ntoken selects its top-matching patches to form a visual group, and each patch\nselects its most related tokens to form a language group. To enable adaptive\ngrouping, we design two threshold gating modules, called Language Grouped\nThreshold Gate and Vision Grouped Threshold Gate, which learn grouping\nthresholds dynamically. Group representations are computed as weighted averages\nbased on similarity scores. To align each token with its group representation,\nwe introduce an Instance Aware Group Alignment loss that operates within each\nimage-text pair, removing the need for external negatives. Finally, a\nBidirectional Cross-modal Grouped Alignment module is applied to enhance\nfine-grained alignment between visual and linguistic group representations.\nExtensive experiments on public and private datasets show that our method\nachieves strong performance on image-text retrieval and classification tasks\nunder both fine-tuning and zero-shot settings.",
        "url": "http://arxiv.org/abs/2507.23402v1",
        "published_date": "2025-07-31T10:14:49+00:00",
        "updated_date": "2025-07-31T10:14:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Wei Li",
            "Xun Gong",
            "Jiao Li",
            "Xiaobin Sun"
        ],
        "tldr": "The paper introduces Adaptive Grouped Alignment (AGA), a new framework for structured medical cross-modal representation learning that captures structured semantics from paired medical images and reports using bidirectional grouping and instance-aware alignment, achieving strong performance on retrieval and classification tasks without relying on hard negative samples.",
        "tldr_zh": "该论文提出了一种自适应分组对齐（AGA）框架，用于结构化医学跨模态表示学习。AGA 使用双向分组和实例感知对齐来捕捉配对的医学图像和报告中的结构化语义，无需依赖困难负样本，并在检索和分类任务中表现出色。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Multi-Prompt Progressive Alignment for Multi-Source Unsupervised Domain Adaptation",
        "summary": "Large Vision-Language Models like CLIP have become a powerful foundation for\nUnsupervised Domain Adaptation due to their strong zero-shot generalization.\nState-of-the-art methods typically leverage CLIP to generate pseudo-labels for\nthe target domain, then fine-tune the model to learn domain-invariant features.\nHowever, these methods attempt to align source and target domains using all\npseudo-labeled data simultaneously. This one-shot alignment struggles with\nnoisy, hard-to-classify samples, leading to error propagation and suboptimal\nfeature learning. The problem is even more amplified in the multi-source\nscenario, where diverse domain gaps and varying noise levels across multiple\nsource domains further destabilize the alignment process. To address this\nissue, in this work, we propose a progressive alignment strategy for adapting\nCLIP to unlabeled downstream task. Our method begins by training the model on a\nhigh-confidence subset of target samples, allowing it to first learn a\nwell-aligned representation from the most reliable data. As training\nprogresses, it gradually incorporates more challenging samples, guiding the\nmodel to refine its understanding without being overwhelmed by initial label\nnoise. This progressive approach effectively mitigates confirmation bias and\npromotes a more robust convergence, allowing for the learning of genuinely\ndomain-invariant features. We name our approach MP^2A and test it on three\npopular UDA benchmarks, namely ImageCLEF, Office-Home, and the most challenging\nDomainNet. Experiments showcase that MP^2A achieves state-of-the-art\nperformance when compared with most recent CLIP-based MS-UDA approaches,\ndemonstrating the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2507.23373v1",
        "published_date": "2025-07-31T09:42:42+00:00",
        "updated_date": "2025-07-31T09:42:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Chen",
            "Zexiao Wang",
            "Haidong Cao",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "tldr": "The paper introduces a Multi-Prompt Progressive Alignment (MP^2A) strategy for multi-source unsupervised domain adaptation using CLIP, progressively aligning source and target domains to mitigate noise and improve performance. It achieves state-of-the-art results on common UDA benchmarks.",
        "tldr_zh": "本文提出了一种多提示渐进对齐（MP^2A）策略，用于使用CLIP的多源无监督领域自适应，通过渐进对齐源域和目标域来减轻噪声并提高性能。在常见的UDA基准测试中取得了最先进的成果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries",
        "summary": "Emotional understanding and generation are often treated as separate tasks,\nyet they are inherently complementary and can mutually enhance each other. In\nthis paper, we propose the UniEmo, a unified framework that seamlessly\nintegrates these two tasks. The key challenge lies in the abstract nature of\nemotions, necessitating the extraction of visual representations beneficial for\nboth tasks. To address this, we propose a hierarchical emotional understanding\nchain with learnable expert queries that progressively extracts multi-scale\nemotional features, thereby serving as a foundational step for unification.\nSimultaneously, we fuse these expert queries and emotional representations to\nguide the diffusion model in generating emotion-evoking images. To enhance the\ndiversity and fidelity of the generated emotional images, we further introduce\nthe emotional correlation coefficient and emotional condition loss into the\nfusion process. This step facilitates fusion and alignment for emotional\ngeneration guided by the understanding. In turn, we demonstrate that joint\ntraining allows the generation component to provide implicit feedback to the\nunderstanding part. Furthermore, we propose a novel data filtering algorithm to\nselect high-quality and diverse emotional images generated by the well-trained\nmodel, which explicitly feedback into the understanding part. Together, these\ngeneration-driven dual feedback processes enhance the model's understanding\ncapacity. Extensive experiments show that UniEmo significantly outperforms\nstate-of-the-art methods in both emotional understanding and generation tasks.\nThe code for the proposed method is available at\nhttps://github.com/JiuTian-VL/UniEmo.",
        "url": "http://arxiv.org/abs/2507.23372v1",
        "published_date": "2025-07-31T09:39:27+00:00",
        "updated_date": "2025-07-31T09:39:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yijie Zhu",
            "Lingsen Zhang",
            "Zitong Yu",
            "Rui Shao",
            "Tao Tan",
            "Liqiang Nie"
        ],
        "tldr": "The paper introduces UniEmo, a unified framework for emotional understanding and generation using learnable expert queries and a diffusion model, enhanced by dual feedback processes for improved performance in both tasks.",
        "tldr_zh": "该论文介绍了UniEmo，一个统一的情感理解和生成框架，它使用可学习的专家查询和扩散模型，并通过双重反馈过程来提高两项任务的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning",
        "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential\nin complex scene understanding and action reasoning, leading to their\nincreasing adoption in end-to-end autonomous driving systems. However, the long\nvisual tokens of VLA models greatly increase computational costs. Current\nvisual token pruning methods in Vision-Language Models (VLM) rely on either\nvisual token similarity or visual-text attention, but both have shown poor\nperformance in autonomous driving scenarios. Given that human drivers\nconcentrate on relevant foreground areas while driving, we assert that\nretaining visual tokens containing this foreground information is essential for\neffective decision-making. Inspired by this, we propose FastDriveVLA, a novel\nreconstruction-based vision token pruning framework designed specifically for\nautonomous driving. FastDriveVLA includes a plug-and-play visual token pruner\ncalled ReconPruner, which prioritizes foreground information through MAE-style\npixel reconstruction. A novel adversarial foreground-background reconstruction\nstrategy is designed to train ReconPruner for the visual encoder of VLA models.\nOnce trained, ReconPruner can be seamlessly applied to different VLA models\nwith the same visual encoder without retraining. To train ReconPruner, we also\nintroduce a large-scale dataset called nuScenes-FG, consisting of 241K\nimage-mask pairs with annotated foreground regions. Our approach achieves\nstate-of-the-art results on the nuScenes closed-loop planning benchmark across\ndifferent pruning ratios.",
        "url": "http://arxiv.org/abs/2507.23318v1",
        "published_date": "2025-07-31T07:55:56+00:00",
        "updated_date": "2025-07-31T07:55:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiajun Cao",
            "Qizhe Zhang",
            "Peidong Jia",
            "Xuhui Zhao",
            "Bo Lan",
            "Xiaoan Zhang",
            "Xiaobao Wei",
            "Sixiang Chen",
            "Zhuo Li",
            "Yang Wang",
            "Liyun Li",
            "Xianming Liu",
            "Ming Lu",
            "Shanghang Zhang"
        ],
        "tldr": "The paper introduces FastDriveVLA, a reconstruction-based visual token pruning framework for VLA models in autonomous driving, achieving state-of-the-art results by prioritizing foreground information through a plug-and-play pruner and a new foreground-annotated dataset.",
        "tldr_zh": "该论文介绍了FastDriveVLA，一个基于重建的视觉token剪枝框架，用于自动驾驶中的VLA模型。通过一个即插即用的剪枝器和一个新的前景标注数据集，该框架优先考虑前景信息，从而实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Toward Safe, Trustworthy and Realistic Augmented Reality User Experience",
        "summary": "As augmented reality (AR) becomes increasingly integrated into everyday life,\nensuring the safety and trustworthiness of its virtual content is critical. Our\nresearch addresses the risks of task-detrimental AR content, particularly that\nwhich obstructs critical information or subtly manipulates user perception. We\ndeveloped two systems, ViDDAR and VIM-Sense, to detect such attacks using\nvision-language models (VLMs) and multimodal reasoning modules. Building on\nthis foundation, we propose three future directions: automated, perceptually\naligned quality assessment of virtual content; detection of multimodal attacks;\nand adaptation of VLMs for efficient and user-centered deployment on AR\ndevices. Overall, our work aims to establish a scalable, human-aligned\nframework for safeguarding AR experiences and seeks feedback on perceptual\nmodeling, multimodal AR content implementation, and lightweight model\nadaptation.",
        "url": "http://arxiv.org/abs/2507.23226v1",
        "published_date": "2025-07-31T03:42:52+00:00",
        "updated_date": "2025-07-31T03:42:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanming Xiu"
        ],
        "tldr": "This paper focuses on detecting task-detrimental AR content using vision-language models and proposes future directions for improving AR safety and trustworthiness, including automated quality assessment and multimodal attack detection.",
        "tldr_zh": "该论文侧重于利用视觉语言模型检测有害的增强现实内容，并提出了改进增强现实安全性和可信度的未来方向，包括自动质量评估和多模态攻击检测。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks",
        "summary": "This paper addresses the challenge of generating adversarial image using a\ndiffusion model to deceive multimodal large language models (MLLMs) into\ngenerating the targeted responses, while avoiding significant distortion of the\nclean image. To address the above challenges, we propose an adversarial-guided\ndiffusion (AGD) approach for adversarial attack MLLMs. We introduce\nadversarial-guided noise to ensure attack efficacy. A key observation in our\ndesign is that, unlike most traditional adversarial attacks which embed\nhigh-frequency perturbations directly into the clean image, AGD injects target\nsemantics into the noise component of the reverse diffusion. Since the added\nnoise in a diffusion model spans the entire frequency spectrum, the adversarial\nsignal embedded within it also inherits this full-spectrum property.\nImportantly, during reverse diffusion, the adversarial image is formed as a\nlinear combination of the clean image and the noise. Thus, when applying\ndefenses such as a simple low-pass filtering, which act independently on each\ncomponent, the adversarial image within the noise component is less likely to\nbe suppressed, as it is not confined to the high-frequency band. This makes AGD\ninherently robust to variety defenses. Extensive experiments demonstrate that\nour AGD outperforms state-of-the-art methods in attack performance as well as\nin model robustness to some defenses.",
        "url": "http://arxiv.org/abs/2507.23202v1",
        "published_date": "2025-07-31T02:57:20+00:00",
        "updated_date": "2025-07-31T02:57:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengwei Xia",
            "Fan Ma",
            "Ruijie Quan",
            "Kun Zhan",
            "Yi Yang"
        ],
        "tldr": "This paper introduces an adversarial-guided diffusion (AGD) approach to generate adversarial images that can effectively attack multimodal large language models (MLLMs) while being robust to common defenses like low-pass filtering.",
        "tldr_zh": "本文提出了一种对抗引导扩散（AGD）方法，用于生成对抗图像，能够有效地攻击多模态大型语言模型（MLLM），同时对低通滤波等常见防御具有鲁棒性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Details Matter for Indoor Open-vocabulary 3D Instance Segmentation",
        "summary": "Unlike closed-vocabulary 3D instance segmentation that is often trained\nend-to-end, open-vocabulary 3D instance segmentation (OV-3DIS) often leverages\nvision-language models (VLMs) to generate 3D instance proposals and classify\nthem. While various concepts have been proposed from existing research, we\nobserve that these individual concepts are not mutually exclusive but\ncomplementary. In this paper, we propose a new state-of-the-art solution for\nOV-3DIS by carefully designing a recipe to combine the concepts together and\nrefining them to address key challenges. Our solution follows the two-stage\nscheme: 3D proposal generation and instance classification. We employ robust 3D\ntracking-based proposal aggregation to generate 3D proposals and remove\noverlapped or partial proposals by iterative merging/removal. For the\nclassification stage, we replace the standard CLIP model with Alpha-CLIP, which\nincorporates object masks as an alpha channel to reduce background noise and\nobtain object-centric representation. Additionally, we introduce the\nstandardized maximum similarity (SMS) score to normalize text-to-proposal\nsimilarity, effectively filtering out false positives and boosting precision.\nOur framework achieves state-of-the-art performance on ScanNet200 and S3DIS\nacross all AP and AR metrics, even surpassing an end-to-end closed-vocabulary\nmethod.",
        "url": "http://arxiv.org/abs/2507.23134v1",
        "published_date": "2025-07-30T22:26:56+00:00",
        "updated_date": "2025-07-30T22:26:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sanghun Jung",
            "Jingjing Zheng",
            "Ke Zhang",
            "Nan Qiao",
            "Albert Y. C. Chen",
            "Lu Xia",
            "Chi Liu",
            "Yuyin Sun",
            "Xiao Zeng",
            "Hsiang-Wei Huang",
            "Byron Boots",
            "Min Sun",
            "Cheng-Hao Kuo"
        ],
        "tldr": "This paper proposes a new state-of-the-art method for open-vocabulary 3D instance segmentation (OV-3DIS) using robust 3D tracking and Alpha-CLIP with a standardized maximum similarity score to improve performance on ScanNet200 and S3DIS datasets.",
        "tldr_zh": "本文提出了一种新的最先进的开放词汇三维实例分割（OV-3DIS）方法，该方法使用鲁棒的三维跟踪和 Alpha-CLIP，并采用标准化最大相似度得分来提高 ScanNet200 和 S3DIS 数据集的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods",
        "summary": "This paper investigates the inverse capabilities and broader utility of\nmultimodal latent spaces within task-specific AI (Artificial Intelligence)\nmodels. While these models excel at their designed forward tasks (e.g.,\ntext-to-image generation, audio-to-text transcription), their potential for\ninverse mappings remains largely unexplored. We propose an optimization-based\nframework to infer input characteristics from desired outputs, applying it\nbidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio\n(Whisper-Large-V3, Chatterbox-TTS) modalities.\n  Our central hypothesis posits that while optimization can guide models\ntowards inverse tasks, their multimodal latent spaces will not consistently\nsupport semantically meaningful and perceptually coherent inverse mappings.\nExperimental results consistently validate this hypothesis. We demonstrate that\nwhile optimization can force models to produce outputs that align textually\nwith targets (e.g., a text-to-image model generating an image that an image\ncaptioning model describes correctly, or an ASR model transcribing optimized\naudio accurately), the perceptual quality of these inversions is chaotic and\nincoherent. Furthermore, when attempting to infer the original semantic input\nfrom generative models, the reconstructed latent space embeddings frequently\nlack semantic interpretability, aligning with nonsensical vocabulary tokens.\n  These findings highlight a critical limitation. multimodal latent spaces,\nprimarily optimized for specific forward tasks, do not inherently possess the\nstructure required for robust and interpretable inverse mappings. Our work\nunderscores the need for further research into developing truly semantically\nrich and invertible multimodal latent spaces.",
        "url": "http://arxiv.org/abs/2507.23010v1",
        "published_date": "2025-07-30T18:19:11+00:00",
        "updated_date": "2025-07-30T18:19:11+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Siwoo Park"
        ],
        "tldr": "This paper investigates the limitations of using optimization to invert multimodal latent spaces in tasks like text-to-image and text-to-audio, finding that while optimization can achieve textual alignment, the resulting inversions often lack perceptual coherence and semantic interpretability. It highlights the need for developing semantically richer and invertible multimodal latent spaces.",
        "tldr_zh": "本文研究了使用优化方法反转多模态潜在空间（如文本到图像和文本到音频）的局限性，发现优化可以实现文本对齐，但反转结果往往缺乏感知一致性和语义可解释性。 这强调了开发语义更丰富且可逆的多模态潜在空间的必要性。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision",
        "summary": "Traffic sign recognition, as a core component of autonomous driving\nperception systems, directly influences vehicle environmental awareness and\ndriving safety. Current technologies face two significant challenges: first,\nthe traffic sign dataset exhibits a pronounced long-tail distribution,\nresulting in a substantial decline in recognition performance of traditional\nconvolutional networks when processing low-frequency and out-of-distribution\nclasses; second, traffic signs in real-world scenarios are predominantly small\ntargets with significant scale variations, making it difficult to extract\nmulti-scale features.To overcome these issues, we propose a novel two-stage\nframework combining open-vocabulary detection and cross-modal learning. For\ntraffic sign detection, our NanoVerse YOLO model integrates a reparameterizable\nvision-language path aggregation network (RepVL-PAN) and an SPD-Conv module to\nspecifically enhance feature extraction for small, multi-scale targets. For\ntraffic sign classification, we designed a Traffic Sign Recognition Multimodal\nContrastive Learning model (TSR-MCL). By contrasting visual features from a\nVision Transformer with semantic features from a rule-based BERT, TSR-MCL\nlearns robust, frequency-independent representations, effectively mitigating\nclass confusion caused by data imbalance. On the TT100K dataset, our method\nachieves a state-of-the-art 78.4% mAP in the long-tail detection task for\nall-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,\nsignificantly outperforming mainstream algorithms and demonstrating superior\naccuracy and generalization in complex, open-world scenarios.",
        "url": "http://arxiv.org/abs/2507.23331v1",
        "published_date": "2025-07-31T08:23:30+00:00",
        "updated_date": "2025-07-31T08:23:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiang Lu",
            "Waikit Xiu",
            "Xiying Li",
            "Shenyu Hu",
            "Shengbo Sun"
        ],
        "tldr": "This paper presents a two-stage framework for traffic sign recognition, combining a novel YOLO-based detector with a contrastive learning approach that fuses visual and semantic information to address long-tail distributions and small object detection challenges.",
        "tldr_zh": "本文提出了一种两阶段的交通标志识别框架，结合了一种新颖的基于YOLO的检测器和对比学习方法，该方法融合了视觉和语义信息，以解决长尾分布和小物体检测的挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6
    }
]