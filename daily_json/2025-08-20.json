[
    {
        "title": "Learning to See Through Flare",
        "summary": "Machine vision systems are susceptible to laser flare, where unwanted intense\nlaser illumination blinds and distorts its perception of the environment\nthrough oversaturation or permanent damage to sensor pixels. We introduce\nNeuSee, the first computational imaging framework for high-fidelity sensor\nprotection across the full visible spectrum. It jointly learns a neural\nrepresentation of a diffractive optical element (DOE) and a frequency-space\nMamba-GAN network for image restoration. NeuSee system is adversarially trained\nend-to-end on 100K unique images to suppress the peak laser irradiance as high\nas $10^6$ times the sensor saturation threshold $I_{\\textrm{sat}}$, the point\nat which camera sensors may experience damage without the DOE. Our system\nleverages heterogeneous data and model parallelism for distributed computing,\nintegrating hyperspectral information and multiple neural networks for\nrealistic simulation and image restoration. NeuSee takes into account\nopen-world scenes with dynamically varying laser wavelengths, intensities, and\npositions, as well as lens flare effects, unknown ambient lighting conditions,\nand sensor noises. It outperforms other learned DOEs, achieving full-spectrum\nimaging and laser suppression for the first time, with a 10.1\\% improvement in\nrestored image quality.",
        "url": "http://arxiv.org/abs/2508.13907v1",
        "published_date": "2025-08-19T15:06:59+00:00",
        "updated_date": "2025-08-19T15:06:59+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Xiaopeng Peng",
            "Heath Gemar",
            "Erin Fleet",
            "Kyle Novak",
            "Abbie Watnik",
            "Grover Swartzlander"
        ],
        "tldr": "The paper introduces NeuSee, a computational imaging framework with a learned diffractive optical element (DOE) and Mamba-GAN for suppressing laser flare and restoring image quality in machine vision systems, achieving significant improvements over existing methods.",
        "tldr_zh": "本文介绍了一种计算成像框架NeuSee，它使用学习到的衍射光学元件（DOE）和Mamba-GAN来抑制激光闪光并恢复机器视觉系统中的图像质量，与现有方法相比取得了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation",
        "summary": "State-of-the-art text-to-image models produce visually impressive results but\noften struggle with precise alignment to text prompts, leading to missing\ncritical elements or unintended blending of distinct concepts. We propose a\nnovel approach that learns a high-success-rate distribution conditioned on a\ntarget prompt, ensuring that generated images faithfully reflect the\ncorresponding prompts. Our method explicitly models the signal component during\nthe denoising process, offering fine-grained control that mitigates\nover-optimization and out-of-distribution artifacts. Moreover, our framework is\ntraining-free and seamlessly integrates with both existing diffusion and flow\nmatching architectures. It also supports additional conditioning modalities --\nsuch as bounding boxes -- for enhanced spatial alignment. Extensive experiments\ndemonstrate that our approach outperforms current state-of-the-art methods. The\ncode is available at https://github.com/grimalPaul/gsn-factory.",
        "url": "http://arxiv.org/abs/2508.13866v1",
        "published_date": "2025-08-19T14:31:15+00:00",
        "updated_date": "2025-08-19T14:31:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Paul Grimal",
            "Michaël Soumm",
            "Hervé Le Borgne",
            "Olivier Ferret",
            "Akihiro Sugimoto"
        ],
        "tldr": "The paper introduces SAGA, a training-free method that improves text-to-image generation by learning signal-aligned distributions, leading to better prompt alignment and spatial control.",
        "tldr_zh": "该论文介绍了SAGA，一种无需训练的方法，通过学习信号对齐的分布来改进文本到图像的生成，从而实现更好的提示对齐和空间控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images",
        "summary": "Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis\nand treatment. However, its reliance on contrast agents introduces safety\nconcerns, contraindications, increased cost, and workflow complexity. To this\nend, we present pre-contrast conditioned denoising diffusion probabilistic\nmodels to synthesize DCE-MRI, introducing, evaluating, and comparing a total of\n22 generative model variants in both single-breast and full breast settings.\nTowards enhancing lesion fidelity, we introduce both tumor-aware loss functions\nand explicit tumor segmentation mask conditioning. Using a public multicenter\ndataset and comparing to respective pre-contrast baselines, we observe that\nsubtraction image-based models consistently outperform post-contrast-based\nmodels across five complementary evaluation metrics. Apart from assessing the\nentire image, we also separately evaluate the region of interest, where both\ntumor-aware losses and segmentation mask inputs improve evaluation metrics. The\nlatter notably enhance qualitative results capturing contrast uptake, albeit\nassuming access to tumor localization inputs that are not guaranteed to be\navailable in screening settings. A reader study involving 2 radiologists and 4\nMRI technologists confirms the high realism of the synthetic images, indicating\nan emerging clinical potential of generative contrast-enhancement. We share our\ncodebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.",
        "url": "http://arxiv.org/abs/2508.13776v1",
        "published_date": "2025-08-19T12:24:55+00:00",
        "updated_date": "2025-08-19T12:24:55+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Sebastian Ibarra",
            "Javier del Riego",
            "Alessandro Catanese",
            "Julian Cuba",
            "Julian Cardona",
            "Nataly Leon",
            "Jonathan Infante",
            "Karim Lekadir",
            "Oliver Diaz",
            "Richard Osuala"
        ],
        "tldr": "This paper explores conditional diffusion models for synthesizing contrast-enhanced breast MRI from pre-contrast images, aiming to reduce reliance on contrast agents and improve lesion fidelity using tumor-aware losses and segmentation mask conditioning. The results show promising realism and clinical potential.",
        "tldr_zh": "本文探索了使用条件扩散模型，通过预对比度图像合成对比度增强乳腺MRI的方法，旨在减少对造影剂的依赖，并通过肿瘤感知损失和分割掩码条件来提高病灶的保真度。 结果表明该方法具有良好的真实感和临床潜力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffIER: Optimizing Diffusion Models with Iterative Error Reduction",
        "summary": "Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality samples and enhancing performance across diverse domains through\nClassifier-Free Guidance (CFG). However, the quality of generated samples is\nhighly sensitive to the selection of the guidance weight. In this work, we\nidentify a critical ``training-inference gap'' and we argue that it is the\npresence of this gap that undermines the performance of conditional generation\nand renders outputs highly sensitive to the guidance weight. We quantify this\ngap by measuring the accumulated error during the inference stage and establish\na correlation between the selection of guidance weight and minimizing this gap.\nFurthermore, to mitigate this gap, we propose DiffIER, an optimization-based\nmethod for high-quality generation. We demonstrate that the accumulated error\ncan be effectively reduced by an iterative error minimization at each step\nduring inference. By introducing this novel plug-and-play optimization\nframework, we enable the optimization of errors at every single inference step\nand enhance generation quality. Empirical results demonstrate that our proposed\nmethod outperforms baseline approaches in conditional generation tasks.\nFurthermore, the method achieves consistent success in text-to-image\ngeneration, image super-resolution, and text-to-speech generation, underscoring\nits versatility and potential for broad applications in future research.",
        "url": "http://arxiv.org/abs/2508.13628v1",
        "published_date": "2025-08-19T08:41:49+00:00",
        "updated_date": "2025-08-19T08:41:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ao Chen",
            "Lihe Ding",
            "Tianfan Xue"
        ],
        "tldr": "The paper introduces DiffIER, a method to optimize diffusion models by iteratively reducing the 'training-inference gap' during inference, leading to improved sample quality across various conditional generation tasks.",
        "tldr_zh": "该论文介绍了 DiffIER，一种通过在推理过程中迭代减少“训练-推理差距”来优化扩散模型的方法，从而提高了各种条件生成任务的样本质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bridging Clear and Adverse Driving Conditions",
        "summary": "Autonomous Driving (AD) systems exhibit markedly degraded performance under\nadverse environmental conditions, such as low illumination and precipitation.\nThe underrepresentation of adverse conditions in AD datasets makes it\nchallenging to address this deficiency. To circumvent the prohibitive cost of\nacquiring and annotating adverse weather data, we propose a novel Domain\nAdaptation (DA) pipeline that transforms clear-weather images into fog, rain,\nsnow, and nighttime images. Here, we systematically develop and evaluate\nseveral novel data-generation pipelines, including simulation-only, GAN-based,\nand hybrid diffusion-GAN approaches, to synthesize photorealistic adverse\nimages from labelled clear images. We leverage an existing DA GAN, extend it to\nsupport auxiliary inputs, and develop a novel training recipe that leverages\nboth simulated and real images. The simulated images facilitate exact\nsupervision by providing perfectly matched image pairs, while the real images\nhelp bridge the simulation-to-real (sim2real) gap. We further introduce a\nmethod to mitigate hallucinations and artifacts in Stable-Diffusion\nImage-to-Image (img2img) outputs by blending them adaptively with their\nprogenitor images. We finetune downstream models on our synthetic data and\nevaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We\nachieve 1.85 percent overall improvement in semantic segmentation, and 4.62\npercent on nighttime, demonstrating the efficacy of our hybrid method for\nrobust AD perception under challenging conditions.",
        "url": "http://arxiv.org/abs/2508.13592v1",
        "published_date": "2025-08-19T07:58:05+00:00",
        "updated_date": "2025-08-19T07:58:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yoel Shapiro",
            "Yahia Showgan",
            "Koustav Mullick"
        ],
        "tldr": "This paper proposes a domain adaptation pipeline using GANs and diffusion models to generate synthetic adverse weather images for training autonomous driving systems, showing improved semantic segmentation performance.",
        "tldr_zh": "本文提出了一种域适应管道，使用 GAN 和扩散模型生成合成的恶劣天气图像来训练自动驾驶系统，结果表明语义分割性能有所提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FLAIR: Frequency- and Locality-Aware Implicit Neural Representations",
        "summary": "Implicit Neural Representations (INRs) leverage neural networks to map\ncoordinates to corresponding signals, enabling continuous and compact\nrepresentations. This paradigm has driven significant advances in various\nvision tasks. However, existing INRs lack frequency selectivity, spatial\nlocalization, and sparse representations, leading to an over-reliance on\nredundant signal components. Consequently, they exhibit spectral bias, tending\nto learn low-frequency components early while struggling to capture fine\nhigh-frequency details. To address these issues, we propose FLAIR (Frequency-\nand Locality-Aware Implicit Neural Representations), which incorporates two key\ninnovations. The first is RC-GAUSS, a novel activation designed for explicit\nfrequency selection and spatial localization under the constraints of the\ntime-frequency uncertainty principle (TFUP). The second is\nWavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet\ntransform (DWT) to compute energy scores and explicitly guide frequency\ninformation to the network. Our method consistently outperforms existing INRs\nin 2D image representation and restoration, as well as 3D reconstruction.",
        "url": "http://arxiv.org/abs/2508.13544v1",
        "published_date": "2025-08-19T06:06:04+00:00",
        "updated_date": "2025-08-19T06:06:04+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sukhun Ko",
            "Dahyeon Kye",
            "Kyle Min",
            "Chanho Eom",
            "Jihyong Oh"
        ],
        "tldr": "The paper introduces FLAIR, a novel Implicit Neural Representation that uses RC-GAUSS activations and Wavelet-Energy-Guided Encoding to improve frequency selectivity, spatial localization, and representation sparsity, outperforming existing INRs in image representation, restoration, and 3D reconstruction.",
        "tldr_zh": "该论文介绍了FLAIR，一种新颖的隐式神经表示，它使用RC-GAUSS激活和Wavelet-Energy-Guided编码来提高频率选择性、空间定位和表示稀疏性，在图像表示、修复和3D重建方面优于现有的INR。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "2D Gaussians Meet Visual Tokenizer",
        "summary": "The image tokenizer is a critical component in AR image generation, as it\ndetermines how rich and structured visual content is encoded into compact\nrepresentations. Existing quantization-based tokenizers such as VQ-GAN\nprimarily focus on appearance features like texture and color, often neglecting\ngeometric structures due to their patch-based design. In this work, we explored\nhow to incorporate more visual information into the tokenizer and proposed a\nnew framework named Visual Gaussian Quantization (VGQ), a novel tokenizer\nparadigm that explicitly enhances structural modeling by integrating 2D\nGaussians into traditional visual codebook quantization frameworks. Our\napproach addresses the inherent limitations of naive quantization methods such\nas VQ-GAN, which struggle to model structured visual information due to their\npatch-based design and emphasis on texture and color. In contrast, VGQ encodes\nimage latents as 2D Gaussian distributions, effectively capturing geometric and\nspatial structures by directly modeling structure-related parameters such as\nposition, rotation and scale. We further demonstrate that increasing the\ndensity of 2D Gaussians within the tokens leads to significant gains in\nreconstruction fidelity, providing a flexible trade-off between token\nefficiency and visual richness. On the ImageNet 256x256 benchmark, VGQ achieves\nstrong reconstruction quality with an rFID score of 1.00. Furthermore, by\nincreasing the density of 2D Gaussians within the tokens, VGQ gains a\nsignificant boost in reconstruction capability and achieves a state-of-the-art\nreconstruction rFID score of 0.556 and a PSNR of 24.93, substantially\noutperforming existing methods. Codes will be released soon.",
        "url": "http://arxiv.org/abs/2508.13515v1",
        "published_date": "2025-08-19T05:04:10+00:00",
        "updated_date": "2025-08-19T05:04:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiang Shi",
            "Xiaoyang Guo",
            "Wei Yin",
            "Mingkai Jia",
            "Qian Zhang",
            "Xiaolin Hu",
            "Wenyu Liu",
            "Xinggang Wan"
        ],
        "tldr": "This paper introduces Visual Gaussian Quantization (VGQ), a novel image tokenizer that uses 2D Gaussians to encode geometric and spatial structures, achieving state-of-the-art reconstruction quality on ImageNet.",
        "tldr_zh": "该论文介绍了视觉高斯量化 (VGQ)，一种新型图像分词器，它使用 2D 高斯函数来编码几何和空间结构，并在 ImageNet 上实现了最先进的重建质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MINR: Efficient Implicit Neural Representations for Multi-Image Encoding",
        "summary": "Implicit Neural Representations (INRs) aim to parameterize discrete signals\nthrough implicit continuous functions. However, formulating each image with a\nseparate neural network~(typically, a Multi-Layer Perceptron (MLP)) leads to\ncomputational and storage inefficiencies when encoding multi-images. To address\nthis issue, we propose MINR, sharing specific layers to encode multi-image\nefficiently. We first compare the layer-wise weight distributions for several\ntrained INRs and find that corresponding intermediate layers follow highly\nsimilar distribution patterns. Motivated by this, we share these intermediate\nlayers across multiple images while preserving the input and output layers as\ninput-specific. In addition, we design an extra novel projection layer for each\nimage to capture its unique features. Experimental results on image\nreconstruction and super-resolution tasks demonstrate that MINR can save up to\n60\\% parameters while maintaining comparable performance. Particularly, MINR\nscales effectively to handle 100 images, maintaining an average peak\nsignal-to-noise ratio (PSNR) of 34 dB. Further analysis of various backbones\nproves the robustness of the proposed MINR.",
        "url": "http://arxiv.org/abs/2508.13471v1",
        "published_date": "2025-08-19T03:05:14+00:00",
        "updated_date": "2025-08-19T03:05:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenyong Zhou",
            "Taiqiang Wu",
            "Zhengwu Liu",
            "Yuxin Cheng",
            "Chen Zhang",
            "Ngai Wong"
        ],
        "tldr": "The paper introduces MINR, an efficient implicit neural representation method for encoding multiple images by sharing intermediate layers of MLPs, achieving significant parameter savings with comparable performance on image reconstruction and super-resolution tasks.",
        "tldr_zh": "该论文介绍了MINR，一种高效的隐式神经表示方法，通过共享MLP的中间层来编码多个图像，在图像重建和超分辨率任务中实现了显著的参数节省，同时保持了相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DIME-Net: A Dual-Illumination Adaptive Enhancement Network Based on Retinex and Mixture-of-Experts",
        "summary": "Image degradation caused by complex lighting conditions such as low-light and\nbacklit scenarios is commonly encountered in real-world environments,\nsignificantly affecting image quality and downstream vision tasks. Most\nexisting methods focus on a single type of illumination degradation and lack\nthe ability to handle diverse lighting conditions in a unified manner. To\naddress this issue, we propose a dual-illumination enhancement framework called\nDIME-Net. The core of our method is a Mixture-of-Experts illumination estimator\nmodule, where a sparse gating mechanism adaptively selects suitable S-curve\nexpert networks based on the illumination characteristics of the input image.\nBy integrating Retinex theory, this module effectively performs enhancement\ntailored to both low-light and backlit images. To further correct\nillumination-induced artifacts and color distortions, we design a damage\nrestoration module equipped with Illumination-Aware Cross Attention and\nSequential-State Global Attention mechanisms. In addition, we construct a\nhybrid illumination dataset, MixBL, by integrating existing datasets, allowing\nour model to achieve robust illumination adaptability through a single training\nprocess. Experimental results show that DIME-Net achieves competitive\nperformance on both synthetic and real-world low-light and backlit datasets\nwithout any retraining. These results demonstrate its generalization ability\nand potential for practical multimedia applications under diverse and complex\nillumination conditions.",
        "url": "http://arxiv.org/abs/2508.13921v1",
        "published_date": "2025-08-19T15:17:47+00:00",
        "updated_date": "2025-08-19T15:17:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziang Wang",
            "Xiaoqin Wang",
            "Dingyi Wang",
            "Qiang Li",
            "Shushan Qiao"
        ],
        "tldr": "DIME-Net introduces a dual-illumination enhancement network using a Mixture-of-Experts to handle both low-light and backlit image degradation, demonstrating good generalization on various datasets.",
        "tldr_zh": "DIME-Net 提出了一种双重光照增强网络，该网络使用混合专家模型来处理低光和背光图像的降级，并在各种数据集上表现出良好的泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Is-NeRF: In-scattering Neural Radiance Field for Blurred Images",
        "summary": "Neural Radiance Fields (NeRF) has gained significant attention for its\nprominent implicit 3D representation and realistic novel view synthesis\ncapabilities. Available works unexceptionally employ straight-line volume\nrendering, which struggles to handle sophisticated lightpath scenarios and\nintroduces geometric ambiguities during training, particularly evident when\nprocessing motion-blurred images. To address these challenges, this work\nproposes a novel deblur neural radiance field, Is-NeRF, featuring explicit\nlightpath modeling in real-world environments. By unifying six common light\npropagation phenomena through an in-scattering representation, we establish a\nnew scattering-aware volume rendering pipeline adaptable to complex lightpaths.\nAdditionally, we introduce an adaptive learning strategy that enables\nautonomous determining of scattering directions and sampling intervals to\ncapture finer object details. The proposed network jointly optimizes NeRF\nparameters, scattering parameters, and camera motions to recover fine-grained\nscene representations from blurry images. Comprehensive evaluations demonstrate\nthat it effectively handles complex real-world scenarios, outperforming\nstate-of-the-art approaches in generating high-fidelity images with accurate\ngeometric details.",
        "url": "http://arxiv.org/abs/2508.13808v1",
        "published_date": "2025-08-19T13:13:02+00:00",
        "updated_date": "2025-08-19T13:13:02+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Nan Luo",
            "Chenglin Ye",
            "Jiaxu Li",
            "Gang Liu",
            "Bo Wan",
            "Di Wang",
            "Lupeng Liu",
            "Jun Xiao"
        ],
        "tldr": "The paper proposes Is-NeRF, a novel neural radiance field that explicitly models light scattering to deblur images, outperforming state-of-the-art methods by jointly optimizing NeRF parameters, scattering parameters, and camera motions.",
        "tldr_zh": "该论文提出了Is-NeRF，一种新型神经辐射场，通过显式建模光散射来对图像进行去模糊处理，并通过联合优化NeRF参数、散射参数和相机运动，优于现有技术。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis",
        "summary": "Gait recognition is a valuable biometric task that enables the identification\nof individuals from a distance based on their walking patterns. However, it\nremains limited by the lack of large-scale labeled datasets and the difficulty\nof collecting diverse gait samples for each individual while preserving\nprivacy. To address these challenges, we propose GaitCrafter, a diffusion-based\nframework for synthesizing realistic gait sequences in the silhouette domain.\nUnlike prior works that rely on simulated environments or alternative\ngenerative models, GaitCrafter trains a video diffusion model from scratch,\nexclusively on gait silhouette data. Our approach enables the generation of\ntemporally consistent and identity-preserving gait sequences. Moreover, the\ngeneration process is controllable-allowing conditioning on various covariates\nsuch as clothing, carried objects, and view angle. We show that incorporating\nsynthetic samples generated by GaitCrafter into the gait recognition pipeline\nleads to improved performance, especially under challenging conditions.\nAdditionally, we introduce a mechanism to generate novel identities-synthetic\nindividuals not present in the original dataset-by interpolating identity\nembeddings. These novel identities exhibit unique, consistent gait patterns and\nare useful for training models while maintaining privacy of real subjects.\nOverall, our work takes an important step toward leveraging diffusion models\nfor high-quality, controllable, and privacy-aware gait data generation.",
        "url": "http://arxiv.org/abs/2508.13300v1",
        "published_date": "2025-08-18T18:32:42+00:00",
        "updated_date": "2025-08-18T18:32:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sirshapan Mitra",
            "Yogesh S. Rawat"
        ],
        "tldr": "GaitCrafter uses a diffusion model to generate realistic and controllable gait sequences for biometric identification, improving performance in gait recognition and enabling privacy-preserving data augmentation.",
        "tldr_zh": "GaitCrafter使用扩散模型生成逼真且可控的步态序列，用于生物特征识别，提高了步态识别的性能，并实现了保护隐私的数据增强。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]