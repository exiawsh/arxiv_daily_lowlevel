[
    {
        "title": "A Modality-agnostic Multi-task Foundation Model for Human Brain Imaging",
        "summary": "Recent learning-based approaches have made astonishing advances in calibrated\nmedical imaging like computerized tomography (CT), yet they struggle to\ngeneralize in uncalibrated modalities -- notably magnetic resonance (MR)\nimaging, where performance is highly sensitive to the differences in MR\ncontrast, resolution, and orientation. This prevents broad applicability to\ndiverse real-world clinical protocols. Here we introduce BrainFM, a\nmodality-agnostic, multi-task vision foundation model for human brain imaging.\nWith the proposed \"mild-to-severe\" intra-subject generation and \"real-synth\"\nmix-up training strategy, BrainFM is resilient to the appearance of acquired\nimages (e.g., modality, contrast, deformation, resolution, artifacts), and can\nbe directly applied to five fundamental brain imaging tasks, including image\nsynthesis for CT and T1w/T2w/FLAIR MRI, anatomy segmentation, scalp-to-cortical\ndistance, bias field estimation, and registration. We evaluate the efficacy of\nBrainFM on eleven public datasets, and demonstrate its robustness and\neffectiveness across all tasks and input modalities. Code is available at\nhttps://github.com/jhuldr/BrainFM.",
        "url": "http://arxiv.org/abs/2509.00549v1",
        "published_date": "2025-08-30T16:15:32+00:00",
        "updated_date": "2025-08-30T16:15:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peirong Liu",
            "Oula Puonti",
            "Xiaoling Hu",
            "Karthik Gopinath",
            "Annabel Sorby-Adams",
            "Daniel C. Alexander",
            "W. Taylor Kimberly",
            "Juan E. Iglesias"
        ],
        "tldr": "BrainFM is a modality-agnostic foundation model for human brain imaging that is robust to variations in MR imaging and can be applied to multiple brain imaging tasks, including image synthesis.",
        "tldr_zh": "BrainFM是一种模态无关的脑部成像基础模型，对磁共振成像的各种变化具有鲁棒性，可应用于多种脑部成像任务，包括图像合成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Visually Grounded Narratives: Reducing Cognitive Burden in Researcher-Participant Interaction",
        "summary": "Narrative inquiry has been one of the prominent application domains for the\nanalysis of human experience, aiming to know more about the complexity of human\nsociety. However, researchers are often required to transform various forms of\ndata into coherent hand-drafted narratives in storied form throughout narrative\nanalysis, which brings an immense burden of data analysis. Participants, too,\nare expected to engage in member checking and presentation of these narrative\nproducts, which involves reviewing and responding to large volumes of\ndocuments. Given the dual burden and the need for more efficient and\nparticipant-friendly approaches to narrative making and representation, we made\na first attempt: (i) a new paradigm is proposed, NAME, as the initial attempt\nto push the field of narrative inquiry. Name is able to transfer research\ndocuments into coherent story images, alleviating the cognitive burden of\ninterpreting extensive text-based materials during member checking for both\nresearchers and participants. (ii) We develop an actor location and shape\nmodule to facilitate plausible image generation. (iii) We have designed a set\nof robust evaluation metrics comprising three key dimensions to objectively\nmeasure the perceptual quality and narrative consistency of generated\ncharacters. Our approach consistently demonstrates state-of-the-art performance\nacross different data partitioning schemes. Remarkably, while the baseline\nrelies on the full 100% of the available data, our method requires only 0.96%\nyet still reduces the FID score from 195 to 152. Under identical data volumes,\nour method delivers substantial improvements: for the 70:30 split, the FID\nscore decreases from 175 to 152, and for the 95:5 split, it is nearly halved\nfrom 96 to 49. Furthermore, the proposed model achieves a score of 3.62 on the\nnewly introduced metric, surpassing the baseline score of 2.66.",
        "url": "http://arxiv.org/abs/2509.00381v1",
        "published_date": "2025-08-30T06:38:13+00:00",
        "updated_date": "2025-08-30T06:38:13+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Runtong Wu",
            "Jiayao Song",
            "Fei Teng",
            "Xianhao Ren",
            "Yuyan Gao",
            "Kailun Yang"
        ],
        "tldr": "This paper introduces a novel paradigm (NAME) that uses visually grounded narratives to transform research documents into story images, aiming to reduce the cognitive burden on researchers and participants during narrative analysis. It presents a method for generating plausible images and evaluation metrics to assess their quality.",
        "tldr_zh": "本文介绍了一种新的范例 (NAME)，它使用视觉叙事将研究文档转换为故事图像，旨在减轻叙事分析过程中研究人员和参与者的认知负担。它提出了一种生成合理图像的方法和评估指标来评估其质量。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]