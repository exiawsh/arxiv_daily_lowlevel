<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv CS.CV Papers (Visual Language Model) - August 05, 2025</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/framer-motion/10.16.4/framer-motion.dev.js"></script>
    <!-- Example using Font Awesome (replace with your preferred icon library if needed) -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');

        :root {
            /* New Palette: Light, Clean, Futuristic with Teal/Aqua accents */
            --bg-color: #f8fafc; /* Tailwind slate-50 (Very Light Gray) */
            --card-bg-color: #ffffff; /* White */
            --text-color: #1e293b; /* Tailwind slate-800 (Dark Gray-Blue) */
            --text-muted-color: #64748b; /* Tailwind slate-500 (Medium Gray-Blue) */
            --header-color: #0f172a; /* Tailwind slate-900 (Very Dark Blue) */
            --highlight-primary: #14b8a6; /* Tailwind teal-500 */
            --highlight-secondary: #67e8f9; /* Tailwind cyan-300 */
            --border-color: #e2e8f0; /* Tailwind slate-200 (Light Gray) */
            --shadow-color: rgba(15, 23, 42, 0.08); /* Subtle shadow based on slate-900 */
        }

        body {
            background-color: var(--bg-color);
            color: var(--text-color);
            font-family: 'Inter', sans-serif;
            overflow-x: hidden; /* Prevent horizontal scroll */
            line-height: 1.6;
        }

        .bento-grid {
            display: grid;
            gap: 1.5rem; /* Tailwind gap-6 */
            grid-template-columns: 1fr; /* Force single column */
            padding-bottom: 4rem; /* Add padding at the bottom */
        }

        .bento-item {
            /* Apply semi-transparent white background and blur */
            background-color: rgba(255, 255, 255, 0.7); /* White with 70% opacity */
            backdrop-filter: blur(10px); /* Apply blur effect */
            -webkit-backdrop-filter: blur(10px); /* Safari prefix */
            border-radius: 1rem; /* Slightly larger radius */
            padding: 1.75rem; /* Slightly more padding */
            border: 1px solid rgba(226, 232, 240, 0.5); /* Lighter border with transparency */
            box-shadow: 0 4px 12px var(--shadow-color);
            transition: transform 0.3s ease-out, box-shadow 0.3s ease-out, background-color 0.3s ease-out;
            overflow: hidden; /* Ensure content doesn't overflow */
            position: relative; /* For potential pseudo-elements */
        }

        /* Removed ::before pseudo-element for a cleaner look */


        .bento-item:hover {
            transform: translateY(-6px);
            box-shadow: 0 10px 20px var(--shadow-color), 0 4px 8px rgba(15, 23, 42, 0.06); /* Adjusted hover shadow */
        }

        .paper-title {
            font-size: 1.125rem; /* Tailwind text-lg */
            font-weight: 600; /* Tailwind font-semibold */
            color: var(--highlight-primary); /* Use new primary highlight */
            margin-bottom: 0.75rem; /* Tailwind mb-3 */
            line-height: 1.4;
        }

        .paper-summary {
            font-size: 0.875rem; /* Tailwind text-sm */
            color: var(--text-muted-color);
            margin-bottom: 1.25rem; /* Tailwind mb-5 */
            line-height: 1.6;
        }

        .paper-link {
            display: inline-flex; /* Use flex for icon alignment */
            align-items: center;
            font-size: 0.875rem; /* Tailwind text-sm */
            font-weight: 600;
            color: var(--highlight-primary);
            text-decoration: none;
            padding: 0.5rem 1rem; /* Add padding */
            border-radius: 0.5rem; /* Slightly rounder */
            background-color: rgba(20, 184, 166, 0.08); /* Subtle teal background */
            border: 1px solid rgba(20, 184, 166, 0.2);
            transition: background-color 0.3s ease, color 0.3s ease, transform 0.2s ease;
        }

        .paper-link i {
            margin-right: 0.5rem; /* Tailwind mr-2 */
            transition: transform 0.3s ease;
        }

        .paper-link:hover {
            background-color: rgba(20, 184, 166, 0.15);
            color: #0d9488; /* Darker teal on hover */
            transform: translateY(-1px);
        }
        .paper-link:hover i {
             transform: translateX(2px);
        }

        .paper-authors {
            font-size: 0.75rem; /* Tailwind text-xs */
            color: var(--text-muted-color);
            margin-top: 1rem; /* Tailwind mt-4 */
            font-style: italic;
        }

        .header {
            text-align: center;
            margin-bottom: 3rem; /* Tailwind mb-12 */
            padding-top: 3rem; /* Tailwind pt-12 */
        }

        .header h1 {
            font-size: 2.5rem; /* Tailwind text-4xl or 5xl */
            font-weight: 700; /* Tailwind font-bold */
            color: var(--header-color);
            letter-spacing: -0.025em; /* Tailwind tracking-tight */
            margin-bottom: 0.5rem;
            /* Optional: Add a subtle text gradient */
            /* background: linear-gradient(90deg, var(--highlight-primary), var(--highlight-secondary)); */
            /* -webkit-background-clip: text; */
            /* -webkit-text-fill-color: transparent; */
        }

        .header p {
            font-size: 1.125rem; /* Tailwind text-lg */
            color: var(--text-muted-color);
            margin-top: 0.5rem; /* Tailwind mt-2 */
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
        }

        .footer {
            text-align: center;
            color: var(--text-muted-color);
            font-size: 0.875rem; /* Tailwind text-sm */
            padding-top: 2rem;
            padding-bottom: 2rem; /* Tailwind py-8 */
            border-top: 1px solid var(--border-color);
            margin-top: 4rem;
        }

        /* Simple line graphic element (optional) */
        .line-graphic {
            height: 1px; /* Thinner line */
            background: linear-gradient(90deg, rgba(20, 184, 166, 0), var(--highlight-primary), rgba(20, 184, 166, 0));
            opacity: 0.6;
            margin: 1.5rem 0; /* Adjust margin */
        }

        /* Framer Motion requires the script, styles enhance appearance */
        [data-motion-element] {
             /* Base styles for elements animated by Framer Motion */
        }

        .paper-tldr {
            font-size: 0.95rem; /* Slightly bigger than summary */
            color: #475569; /* Changed to Tailwind slate-600 (slightly darker than summary) */
            margin-top: 0.75rem; /* Tailwind mt-3 */
            margin-bottom: 0.75rem; /* Tailwind mb-2 */
            /* font-style: italic; */
            font-weight: bold;
        }

        .paper-rating {
            margin-top: 1rem; /* Tailwind mt-4 */
            margin-bottom: 1rem; /* Tailwind mb-4 */
            color: #f59e0b; /* Tailwind amber-500 */
        }

        .paper-rating i {
            margin-right: 0.125rem; /* Tailwind mr-0.5 */
        }

        /* Apply consistent star color to sub-ratings */
        .paper-sub-ratings .rating-item i {
            color: #f59e0b; /* Match overall rating star color (amber-500) */
            margin-right: 0.125rem; /* Consistent spacing */
        }

    </style>
</head>
<body class="container mx-auto px-4 antialiased">

    <motion.div
        initial="{ opacity: 0, y: -30 }"
        animate="{ opacity: 1, y: 0 }"
        transition="{ duration: 0.6, ease: 'easeOut' }"
        class="header"
        data-motion-element
    >
        <h1>Low-level/Image generation Daily Papers</h1>
        <p>Daily papers related to Image Restoration, Image Super-resolution, Image Generation from cs.CV</p>
        
            <p>August 05, 2025</p>
        
        <div class="line-graphic mt-4 mb-8 mx-auto w-1/4"></div> <!-- Added line graphic -->
    </motion.div>

    <div class="bento-grid" id="paper-grid">
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.0, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention</h2>
            
            <p class="paper-summary">While large-scale text-to-image diffusion models enable the generation of
high-quality, diverse images from text prompts, these prompts struggle to
capture intricate details, such as textures, preventing the user intent from
being reflected. This limitation has led to efforts to generate images
conditioned on user-provided images, referred to as image prompts. Recent work
modifies the self-attention mechanism to impose image conditions in generated
images by replacing or concatenating the keys and values from the image prompt.
This enables the self-attention layer to work like a cross-attention layer,
generally used to incorporate text prompts. In this paper, we identify two
common issues in existing methods of modifying self-attention to generate
images that reflect the details of image prompts. First, existing approaches
neglect the importance of image prompts in classifier-free guidance.
Specifically, current methods use image prompts as both desired and undesired
conditions in classifier-free guidance, causing conflicting signals. To resolve
this, we propose conflict-free guidance by using image prompts only as desired
conditions, ensuring that the generated image faithfully reflects the image
prompt. In addition, we observe that the two most common self-attention
modifications involve a trade-off between the realism of the generated image
and alignment with the image prompt. Specifically, selecting more keys and
values from the image prompt improves alignment, while selecting more from the
generated image enhances realism. To balance both, we propose an new
self-attention modification method, Stratified Attention to jointly use keys
and values from both images rather than selecting between them. Through
extensive experiments across three image generation tasks, we show that the
proposed method outperforms existing image-prompting models in faithfully
reflecting the image prompt.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: This paper introduces two improvements to image-prompted image generation: conflict-free guidance in classifier-free settings and stratified attention to balance realism and prompt alignment, outperforming existing methods.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 本文提出了图像提示图像生成方面的两项改进：无分类器指导中的无冲突指导以及平衡真实感和提示对齐的分层注意力，性能优于现有方法。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(9/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02004v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Kyungmin Jo, Jooyeol Yun, Jaegul Choo</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.05, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">From Pixels to Pathology: Restoration Diffusion for Diagnostic-Consistent Virtual IHC</h2>
            
            <p class="paper-summary">Hematoxylin and eosin (H&E) staining is the clinical standard for assessing
tissue morphology, but it lacks molecular-level diagnostic information. In
contrast, immunohistochemistry (IHC) provides crucial insights into biomarker
expression, such as HER2 status for breast cancer grading, but remains costly
and time-consuming, limiting its use in time-sensitive clinical workflows. To
address this gap, virtual staining from H&E to IHC has emerged as a promising
alternative, yet faces two core challenges: (1) Lack of fair evaluation of
synthetic images against misaligned IHC ground truths, and (2) preserving
structural integrity and biological variability during translation. To this
end, we present an end-to-end framework encompassing both generation and
evaluation in this work. We introduce Star-Diff, a structure-aware staining
restoration diffusion model that reformulates virtual staining as an image
restoration task. By combining residual and noise-based generation pathways,
Star-Diff maintains tissue structure while modeling realistic biomarker
variability. To evaluate the diagnostic consistency of the generated IHC
patches, we propose the Semantic Fidelity Score (SFS), a
clinical-grading-task-driven metric that quantifies class-wise semantic
degradation based on biomarker classification accuracy. Unlike pixel-level
metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment
and classifier uncertainty. Experiments on the BCI dataset demonstrate that
Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity
and diagnostic relevance. With rapid inference and strong clinical alignment,it
presents a practical solution for applications such as intraoperative virtual
IHC synthesis.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces Star-Diff, a structure-aware diffusion model for generating IHC images from H&E stains, along with a new evaluation metric (SFS) that focuses on diagnostic consistency.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了Star-Diff，一种结构感知的扩散模型，用于从H&E染色生成IHC图像，并提出了一种新的评估指标(SFS)，专注于诊断一致性。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02528v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Jingsong Liu, Xiaofeng Deng, Han Li, Azar Kazemi, Christian Grashei, Gesa Wilkens, Xin You, Tanja Groll, Nassir Navab, Carolin Mogler, Peter J. Schüffler</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.1, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Qwen-Image Technical Report</h2>
            
            <p class="paper-summary">We present Qwen-Image, an image generation foundation model in the Qwen
series that achieves significant advances in complex text rendering and precise
image editing. To address the challenges of complex text rendering, we design a
comprehensive data pipeline that includes large-scale data collection,
filtering, annotation, synthesis, and balancing. Moreover, we adopt a
progressive training strategy that starts with non-text-to-text rendering,
evolves from simple to complex textual inputs, and gradually scales up to
paragraph-level descriptions. This curriculum learning approach substantially
enhances the model's native text rendering capabilities. As a result,
Qwen-Image not only performs exceptionally well in alphabetic languages such as
English, but also achieves remarkable progress on more challenging logographic
languages like Chinese. To enhance image editing consistency, we introduce an
improved multi-task training paradigm that incorporates not only traditional
text-to-image (T2I) and text-image-to-image (TI2I) tasks but also
image-to-image (I2I) reconstruction, effectively aligning the latent
representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed
the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and
reconstructive representations, respectively. This dual-encoding mechanism
enables the editing module to strike a balance between preserving semantic
consistency and maintaining visual fidelity. Qwen-Image achieves
state-of-the-art performance, demonstrating its strong capabilities in both
image generation and editing across multiple benchmarks.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: Qwen-Image is a new image generation model focusing on complex text rendering (including logographic languages like Chinese) and precise image editing, achieving state-of-the-art results through a comprehensive data pipeline and multi-task training.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: Qwen-Image 是一种新的图像生成模型，专注于复杂文本渲染（包括中文等表意文字）和精确的图像编辑，通过全面的数据管道和多任务训练实现最先进的结果。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02324v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.15000000000000002, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models</h2>
            
            <p class="paper-summary">Recent breakthroughs in text-to-image diffusion models have significantly
enhanced both the visual fidelity and semantic controllability of generated
images. However, fine-grained control over aesthetic attributes remains
challenging, especially when users require continuous and intensity-specific
adjustments. Existing approaches often rely on vague textual prompts, which are
inherently ambiguous in expressing both the aesthetic semantics and the desired
intensity, or depend on costly human preference data for alignment, limiting
their scalability and practicality. To address these limitations, we propose
AttriCtrl, a plug-and-play framework for precise and continuous control of
aesthetic attributes. Specifically, we quantify abstract aesthetics by
leveraging semantic similarity from pre-trained vision-language models, and
employ a lightweight value encoder that maps scalar intensities in $[0,1]$ to
learnable embeddings within diffusion-based generation. This design enables
intuitive and customizable aesthetic manipulation, with minimal training
overhead and seamless integration into existing generation pipelines. Extensive
experiments demonstrate that AttriCtrl achieves accurate control over
individual attributes as well as flexible multi-attribute composition.
Moreover, it is fully compatible with popular open-source controllable
generation frameworks, showcasing strong integration capability and practical
utility across diverse generation scenarios.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces AttriCtrl, a plug-and-play framework for fine-grained and continuous control of aesthetic attributes in text-to-image diffusion models using semantic similarity and a lightweight value encoder.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了AttriCtrl，一个即插即用的框架，通过语义相似性和轻量级数值编码器，实现对文本到图像扩散模型中审美属性的细粒度和连续控制。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02151v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Die Chen, Zhongjie Duan, Zhiwen Li, Cen Chen, Daoyuan Chen, Yaliang Li, Yinda Chen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.2, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation</h2>
            
            <p class="paper-summary">Despite recent advances in photorealistic image generation through
large-scale models like FLUX and Stable Diffusion v3, the practical deployment
of these architectures remains constrained by their inherent intractability to
parameter fine-tuning. While low-rank adaptation (LoRA) have demonstrated
efficacy in enabling model customization with minimal parameter overhead, the
effective utilization of distributed open-source LoRA modules faces three
critical challenges: sparse metadata annotation, the requirement for zero-shot
adaptation capabilities, and suboptimal fusion strategies for multi-LoRA fusion
strategies. To address these limitations, we introduce a novel framework that
enables semantic-driven LoRA retrieval and dynamic aggregation through two key
components: (1) weight encoding-base LoRA retriever that establishes a shared
semantic space between LoRA parameter matrices and text prompts, eliminating
dependence on original training data, and (2) fine-grained gated fusion
mechanism that computes context-specific fusion weights across network layers
and diffusion timesteps to optimally integrate multiple LoRA modules during
generation. Our approach achieves significant improvement in image generation
perfermance, thereby facilitating scalable and data-efficient enhancement of
foundational models. This work establishes a critical bridge between the
fragmented landscape of community-developed LoRAs and practical deployment
requirements, enabling collaborative model evolution through standardized
adapter integration.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: This paper introduces a novel framework for automatically retrieving and fusing LoRA modules for text-to-image generation, addressing challenges in utilizing open-source LoRAs through semantic retrieval and fine-grained gated fusion.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 本文提出了一种新的框架，用于自动检索和融合LoRA模块，以用于文本到图像的生成，通过语义检索和细粒度的门控融合，解决了利用开源LoRA模块的挑战。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(8/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02107v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Zhiwen Li, Zhongjie Duan, Die Chen, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.25, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots</h2>
            
            <p class="paper-summary">Panoramic cameras, capturing comprehensive 360-degree environmental data, are
suitable for quadruped robots in surrounding perception and interaction with
complex environments. However, the scarcity of high-quality panoramic training
data-caused by inherent kinematic constraints and complex sensor calibration
challenges-fundamentally limits the development of robust perception systems
tailored to these embodied platforms. To address this issue, we propose
QuaDreamer-the first panoramic data generation engine specifically designed for
quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of
quadruped robots to generate highly controllable, realistic panoramic videos,
providing a data source for downstream tasks. Specifically, to effectively
capture the unique vertical vibration characteristics exhibited during
quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts
controllable vertical signals through frequency-domain feature filtering and
provides high-quality prompts. To facilitate high-quality panoramic video
generation under jitter signal control, we propose a Scene-Object Controller
(SOC) that effectively manages object motion and boosts background jitter
control through the attention mechanism. To address panoramic distortions in
wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream
architecture that synergizes frequency-texture refinement for local detail
enhancement with spatial-structure correction for global geometric consistency.
We further demonstrate that the generated video sequences can serve as training
data for the quadruped robot's panoramic visual perception model, enhancing the
performance of multi-object tracking in 360-degree scenes. The source code and
model weights will be publicly available at
https://github.com/losehu/QuaDreamer.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces QuaDreamer, a panoramic video generation engine designed for quadruped robots, addressing the scarcity of training data by generating controllable, realistic panoramic videos with specific modules for vertical jitter, scene-object control, and panoramic enhancement. It shows performance gain on downstream tasks.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了QuaDreamer，一个专为四足机器人设计的全景视频生成引擎。它通过生成可控的、逼真的全景视频来解决训练数据稀缺的问题，并具有用于垂直抖动、场景对象控制和全景增强的特定模块。实验结果表明在下游任务上的性能有所提升。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(7/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02512v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Sheng Wu, Fei Teng, Hao Shi, Qi Jiang, Kai Luo, Kaiwei Wang, Kailun Yang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.30000000000000004, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">Forecasting When to Forecast: Accelerating Diffusion Models with Confidence-Gated Taylor</h2>
            
            <p class="paper-summary">Diffusion Transformers (DiTs) have demonstrated remarkable performance in
visual generation tasks. However, their low inference speed limits their
deployment in low-resource applications. Recent training-free approaches
exploit the redundancy of features across timesteps by caching and reusing past
representations to accelerate inference. Building on this idea, TaylorSeer
instead uses cached features to predict future ones via Taylor expansion.
However, its module-level prediction across all transformer blocks (e.g.,
attention or feedforward modules) requires storing fine-grained intermediate
features, leading to notable memory and computation overhead. Moreover, it
adopts a fixed caching schedule without considering the varying accuracy of
predictions across timesteps, which can lead to degraded outputs when
prediction fails. To address these limitations, we propose a novel approach to
better leverage Taylor-based acceleration. First, we shift the Taylor
prediction target from the module level to the last block level, significantly
reducing the number of cached features. Furthermore, observing strong
sequential dependencies among Transformer blocks, we propose to use the error
between the Taylor-estimated and actual outputs of the first block as an
indicator of prediction reliability. If the error is small, we trust the Taylor
prediction for the last block; otherwise, we fall back to full computation,
thereby enabling a dynamic caching mechanism. Empirical results show that our
method achieves a better balance between speed and quality, achieving a 3.17x
acceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible
quality drop. The Project Page is
\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: This paper introduces a confidence-gated Taylor expansion method to accelerate diffusion transformer inference by selectively applying Taylor prediction based on the predicted reliability, reducing memory overhead and improving the speed-quality trade-off.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 本文提出了一种基于置信度门控的泰勒展开方法，通过根据预测可靠性选择性地应用泰勒预测，来加速扩散Transformer的推理，从而减少内存开销并提高速度-质量的平衡。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(7/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02240v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Xiaoliu Guan, Lielin Jiang, Hanqi Chen, Xu Zhang, Jiaxing Yan, Guanzhong Wang, Yi Liu, Zetao Zhang, Yu Wu</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.35000000000000003, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">After the Party: Navigating the Mapping From Color to Ambient Lighting</h2>
            
            <p class="paper-summary">Illumination in practical scenarios is inherently complex, involving colored
light sources, occlusions, and diverse material interactions that produce
intricate reflectance and shading effects. However, existing methods often
oversimplify this challenge by assuming a single light source or uniform,
white-balanced lighting, leaving many of these complexities unaddressed.In this
paper, we introduce CL3AN, the first large-scale, high-resolution dataset of
its kind designed to facilitate the restoration of images captured under
multiple Colored Light sources to their Ambient-Normalized counterparts.
Through benchmarking, we find that leading approaches often produce artifacts,
such as illumination inconsistencies, texture leakage, and color distortion,
primarily due to their limited ability to precisely disentangle illumination
from reflectance. Motivated by this insight, we achieve such a desired
decomposition through a novel learning framework that leverages explicit
chromaticity and luminance components guidance, drawing inspiration from the
principles of the Retinex model. Extensive evaluations on existing benchmarks
and our dataset demonstrate the effectiveness of our approach, showcasing
enhanced robustness under non-homogeneous color lighting and material-specific
reflectance variations, all while maintaining a highly competitive
computational cost. The benchmark, codes, and models are available at
www.github.com/fvasluianu97/RLN2.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: This paper introduces CL3AN, a new large-scale dataset for restoring images captured under colored light sources to ambient-normalized counterparts, and proposes a novel learning framework for improved illumination-reflectance disentanglement.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 本文介绍了CL3AN，一个用于将彩色光源下拍摄的图像恢复到环境归一化对应物的大规模新数据集，并提出了一种用于改进光照-反射解耦的新学习框架。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(7/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02168v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Florin-Alexandru Vasluianu, Tim Seizinger, Zongwei Wu, Radu Timofte</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.4, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">DreamPainter: Image Background Inpainting for E-commerce Scenarios</h2>
            
            <p class="paper-summary">Although diffusion-based image genenation has been widely explored and
applied, background generation tasks in e-commerce scenarios still face
significant challenges. The first challenge is to ensure that the generated
products are consistent with the given product inputs while maintaining a
reasonable spatial arrangement, harmonious shadows, and reflections between
foreground products and backgrounds. Existing inpainting methods fail to
address this due to the lack of domain-specific data. The second challenge
involves the limitation of relying solely on text prompts for image control, as
effective integrating visual information to achieve precise control in
inpainting tasks remains underexplored. To address these challenges, we
introduce DreamEcom-400K, a high-quality e-commerce dataset containing accurate
product instance masks, background reference images, text prompts, and
aesthetically pleasing product images. Based on this dataset, we propose
DreamPainter, a novel framework that not only utilizes text prompts for control
but also flexibly incorporates reference image information as an additional
control signal. Extensive experiments demonstrate that our approach
significantly outperforms state-of-the-art methods, maintaining high product
consistency while effectively integrating both text prompt and reference image
information.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces DreamPainter, a diffusion-based image inpainting framework for e-commerce scenarios, along with a new dataset DreamEcom-400K, addressing challenges in product consistency and incorporating visual information for precise control.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了DreamPainter，一个用于电子商务场景的基于扩散的图像修复框架，以及一个新的数据集DreamEcom-400K，解决了产品一致性以及结合视觉信息以实现精确控制的挑战。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(7/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02155v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Sijie Zhao, Jing Cheng, Yaoyao Wu, Hao Xu, Shaohui Jiao</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.45, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">DeflareMamba: Hierarchical Vision Mamba for Contextually Consistent Lens Flare Removal</h2>
            
            <p class="paper-summary">Lens flare removal remains an information confusion challenge in the
underlying image background and the optical flares, due to the complex optical
interactions between light sources and camera lens. While recent solutions have
shown promise in decoupling the flare corruption from image, they often fail to
maintain contextual consistency, leading to incomplete and inconsistent flare
removal. To eliminate this limitation, we propose DeflareMamba, which leverages
the efficient sequence modeling capabilities of state space models while
maintains the ability to capture local-global dependencies. Particularly, we
design a hierarchical framework that establishes long-range pixel correlations
through varied stride sampling patterns, and utilize local-enhanced state space
models that simultaneously preserves local details. To the best of our
knowledge, this is the first work that introduces state space models to the
flare removal task. Extensive experiments demonstrate that our method
effectively removes various types of flare artifacts, including scattering and
reflective flares, while maintaining the natural appearance of non-flare
regions. Further downstream applications demonstrate the capacity of our method
to improve visual object recognition and cross-modal semantic understanding.
Code is available at https://github.com/BNU-ERC-ITEA/DeflareMamba.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: The paper introduces DeflareMamba, a hierarchical vision Mamba architecture for lens flare removal that leverages state space models to maintain contextual consistency and capture local-global dependencies, outperforming existing methods.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 该论文介绍了DeflareMamba，一种用于镜头光晕去除的分层视觉Mamba架构，利用状态空间模型来保持上下文一致性并捕获局部-全局依赖关系，优于现有方法。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i>
                    <span class="text-xs text-gray-500 ml-1">(9/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star-half-alt"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(7/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02113v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Yihang Huang, Yuanfei Huang, Junhui Lin, Hua Huang</p>
            
        </motion.div>
        
        <motion.div
            initial="{ opacity: 0, y: 50, scale: 0.9 }"
            whileInView="{ opacity: 1, y: 0, scale: 1 }"
            viewport="{ once: true, amount: 0.2 }" /* Trigger when 20% is visible */
            transition="{ duration: 0.5, delay: 0.5, ease: 'easeOut' }"  
            class="bento-item"
            data-motion-element
        >
            <h2 class="paper-title">An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception</h2>
            
            <p class="paper-summary">Event cameras offer significant advantages, including a wide dynamic range,
high temporal resolution, and immunity to motion blur, making them highly
promising for addressing challenging visual conditions. Extracting and
utilizing effective information from asynchronous event streams is essential
for the onboard implementation of event cameras. In this paper, we propose a
streamlined event-based intensity reconstruction scheme, event-based single
integration (ESI), to address such implementation challenges. This method
guarantees the portability of conventional frame-based vision methods to
event-based scenarios and maintains the intrinsic advantages of event cameras.
The ESI approach reconstructs intensity images by performing a single
integration of the event streams combined with an enhanced decay algorithm.
Such a method enables real-time intensity reconstruction at a high frame rate,
typically 100 FPS. Furthermore, the relatively low computation load of ESI fits
onboard implementation suitably, such as in UAV-based visual tracking
scenarios. Extensive experiments have been conducted to evaluate the
performance comparison of ESI and state-of-the-art algorithms. Compared to
state-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency
improvements, superior reconstruction quality, and a high frame rate. As a
result, ESI enhances UAV onboard perception significantly under visual
adversary surroundings. In-flight tests, ESI demonstrates effective performance
for UAV onboard visual tracking under extremely low illumination
conditions(2-10lux), whereas other comparative algorithms fail due to
insufficient frame rate, poor image quality, or limited real-time performance.</p>
            
            <p class="paper-tldr"><strong>TLDR</strong>: This paper presents ESI, a fast, event-based intensity reconstruction scheme suitable for real-time UAV perception, demonstrating improved runtime, reconstruction quality, and frame rate compared to state-of-the-art methods, especially in low-light conditions.</p>
            
            
            <p class="paper-tldr"><strong>TLDR</strong>: 本文提出了一种快速的基于事件的强度重建方案ESI，适用于无人机的实时感知。与现有技术相比，该方案在运行时、重建质量和帧率方面都有所提高，尤其是在低光照条件下。</p>
            

            
            
            <div class="paper-sub-ratings" style="display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 5px; font-size: 0.8em;">
                
                <div class="rating-item">
                    <span class="rating-label">Relevance:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i><i class="far fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(3/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Novelty:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star-half-alt"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(7/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Clarity:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(8/10)</span>
                </div>
                
                
                <div class="rating-item">
                    <span class="rating-label">Potential Impact:</span>
                    
                    <i class="fas fa-star"></i><i class="fas fa-star"></i><i class="fas fa-star"></i><i class="far fa-star"></i><i class="far fa-star"></i>
                    <span class="text-xs text-gray-500 ml-1">(6/10)</span>
                </div>
                
            </div>
            
            

            
            <div class="paper-rating">
                <span class="rating-label" style="color: #000; font-weight: bold;">Overall:</span>
                
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="fas fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                    
                        <i class="far fa-star"></i>
                    
                
                <span class="text-xs text-gray-500 ml-1">(4/10)</span>
            </div>
            

            <a href="http://arxiv.org/abs/2508.02238v1" target="_blank" class="paper-link">
                <i class="fas fa-file-pdf mr-1"></i> Read Paper (PDF)
            </a>
            
            <p class="paper-authors">Authors: Xin Dong, Yiwei Zhang, Yangjie Cui, Jinwu Xiang, Daochun Li, Zhan Tu</p>
            
        </motion.div>
        
    </div>

    <footer class="footer">
        Generated on 2025-08-07 03:01:14 UTC. Powered by <a href="https://github.com/onion-liu" target="_blank">onion-liu</a>.
    </footer>

</body>
</html>